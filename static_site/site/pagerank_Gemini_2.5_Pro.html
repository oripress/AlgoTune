<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>AlgoTuner Log – pagerank – Gemini 2.5 Pro</title>
    <link rel="icon" type="image/png" href="assets/AlgoTunerMascot.png">
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7XSBWH5NQF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-7XSBWH5NQF');
    </script>
    
    <!-- Prism.js for syntax highlighting - loaded after styles.css to ensure proper precedence -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <link rel="stylesheet" href="styles.css">"
    
    <!-- Basic Styling & Layout -->
    <style>
    /* Basic Styling & Layout */
    :root {
        --primary-color: #2196F3;
        --primary-light: #E3F2FD;
        --text-color: #333;
        --border-color: #eaeaea;
        --content-bg: #ffffff;
        --error-border: #dc3545;
        --code-bg: #f6f8fa;
        --code-border: #d0d7de;
        --code-text: #24292e;
        
        /* Glass-morphism variables for light mode */
        --glass-bg: rgba(255, 255, 255, 0.12);
        --glass-border: rgba(255, 255, 255, 0.05);
        --glass-header-bg: rgba(0, 0, 0, 0.03);
        --glass-header-border: rgba(255, 255, 255, 0.08);
        --glass-btn-bg: rgba(255, 255, 255, 0.1);
        --glass-btn-border: rgba(255, 255, 255, 0.2);
        --glass-btn-hover: rgba(255, 255, 255, 0.2);
        --glass-expand-bg: linear-gradient(to top, rgba(255, 255, 255, 0.95), rgba(255, 255, 255, 0.8));
        --glass-text: rgba(0, 0, 0, 0.8);
        --glass-text-secondary: rgba(0, 0, 0, 0.6);
    }
    
    /* Dark mode support */
    @media (prefers-color-scheme: dark) {
        :root {
            --glass-bg: rgba(0, 0, 0, 0.15);
            --glass-border: rgba(255, 255, 255, 0.08);
            --glass-header-bg: rgba(255, 255, 255, 0.05);
            --glass-header-border: rgba(255, 255, 255, 0.12);
            --glass-btn-bg: rgba(255, 255, 255, 0.08);
            --glass-btn-border: rgba(255, 255, 255, 0.15);
            --glass-btn-hover: rgba(255, 255, 255, 0.15);
            --glass-expand-bg: linear-gradient(to top, rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.7));
            --glass-text: rgba(255, 255, 255, 0.9);
            --glass-text-secondary: rgba(255, 255, 255, 0.7);
        }
    }
    
    body {
        margin: 0;
        padding: 0;
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        color: var(--text-color);
        line-height: 1.5;
        background: var(--content-bg);
        display: flex;
        min-height: 100vh;
    }
    
    /* Sidebar - Desktop only, hidden by default on mobile */
    .sidebar {
        /* Slightly narrower sidebar to give more room to main content */
        width: 180px;
        background: #f8f9fa;
        border-right: 1px solid var(--border-color);
        position: fixed;
        left: 0;
        top: 0;
        height: 100vh;
        overflow-y: auto;
        z-index: 1000;
        display: none; /* Hidden by default */
    }
    
    /* Show sidebar only on large screens (1025px and up) */
    @media (min-width: 1025px) {
        .sidebar {
            display: block;
        }
    }
    
    /* --------------------------- */
    /* Sidebar header & back link  */
    /* --------------------------- */
    .sidebar-header {
        padding: 1.25rem;
        display: flex;
        justify-content: center;  /* Centre the back link horizontally */
    }

    .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.4rem;               /* Space between arrow and label */
        padding: 0.45rem 0.9rem;
        background: #2196F3;       /* Primary blue */
        border-radius: 8px;
        color: #ffffff;            /* White text */
        font-weight: 600;
        font-size: 0.9rem;
        text-decoration: none;
        transition: background 0.2s ease, box-shadow 0.2s ease;
    }

    .back-link:hover {
        background: #1976D2;       /* Darker blue on hover */
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.15);
        text-decoration: none; 
        color: #ffffff;
    }
    
    .sidebar-content {
        padding: 1rem;
    }
    
    .sidebar-content h3 {
        margin: 0 0 1rem 0;
        font-size: 1rem;
        color: var(--text-color);
        text-align: left;
        /* Allow long task names with underscores to wrap onto multiple lines */
        white-space: normal;
        word-wrap: break-word;
        overflow-wrap: anywhere;
        line-height: 1.3;
    }
    
    .sidebar-task {
        margin-bottom: 1.5rem;
    }
    
    .task-name {
        font-weight: 600;
        font-size: 0.85rem;
        color: var(--primary-color);
        margin-bottom: 0.5rem;
        padding: 0.25rem 0;
        border-bottom: 1px solid #e0e0e0;
        /* Allow very long task names to wrap instead of overflowing */
        white-space: normal;
        word-wrap: break-word;
        overflow-wrap: anywhere;
        line-height: 1.3;
    }
    
    .sidebar-run {
        margin-bottom: 8px;
    }
    
    /* Make sidebar run links occupy full width */
    .sidebar-run a {
        display: block;
        width: 100%;
        text-decoration: none;
    }
    
    .sidebar-run a:hover, .sidebar-run a:focus, .sidebar-run a:visited {
        text-decoration: none;
    }
    
    /* Ensure the coloured badge stretches the whole column */
    .run-score {
        width: 100%;
    }
    
    /* Thicker highlight for the currently selected run on desktop */
    @media (min-width: 769px) {
        .sidebar-run.current a {
            border-left: 5px solid #2196F3 !important;
        }
    }
    
    .main-content {
        flex: 1;
        margin-left: 180px;
        padding: 0;
        max-width: calc(100vw - 180px);
    }
    
    .container {
        /* Allow the main conversation area to take up the full width that is
           available once the fixed sidebar is accounted for. */
        max-width: 100%;
        margin: 0 auto;
        padding: 0 15px;
    }
    
    h1 {
        color: var(--primary-color);
        border-bottom: 2px solid var(--primary-light);
        padding-bottom: 0.5rem;
    }
    
    .info-section {
        background: var(--primary-light);
        padding: 1rem;
        border-radius: 6px;
        margin-bottom: 1.5rem;
        display: flex;
        flex-wrap: nowrap;
        gap: 1rem;
        overflow-x: auto;
        white-space: nowrap;
    }
    
    .info-item {
        display: flex;
        flex-direction: column;
        flex-shrink: 0;
        min-width: 140px;
    }
    
    .info-label {
        font-weight: 600;
        color: var(--primary-color);
        font-size: 0.85rem;
        margin-bottom: 0.25rem;
        white-space: nowrap;
    }
    
    .info-value {
        font-size: 0.9rem;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }
    
    .task-info-line, .model-info-line {
        font-size: 1.3rem;
        word-wrap: break-word;
        overflow-wrap: break-word;
        text-align: left !important;
        display: block;
    }
    
    .task-name-display {
        font-weight: 600;
        font-size: clamp(0.9rem, 4vw, 1.3rem);
        word-wrap: break-word;
        overflow-wrap: break-word;
        line-height: 1.2;
    }
    
    .plots-container {
        display: flex;
        gap: 1rem;
        margin: 1.5rem 0;
    }
    
    .plot-section {
        margin: 0;
    }
    
    .plot-half {
        flex: 1;
        width: 50%;
    }
    
    .plot-section h3 {
        margin-bottom: 0.8rem;
        color: var(--text-color);
    }
    
    .plot-container {
        text-align: center;
        background: #ffffff;
        border: 1px solid var(--border-color);
        border-radius: 6px;
        padding: 1rem;
        margin-bottom: 1rem;
    }
    
    .plot-container img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 0 auto;
    }
    
    .no-plot {
        color: #666;
        font-style: italic;
        padding: 2rem;
        text-align: center;
    }
    
    .conversation-section {
        margin: 1.5rem 0;
    }
    
    .message {
        margin-bottom: 1rem;
        padding: 1rem;
        border-radius: 18px;
        /* Give the chat bubbles more breathing room. 90 % looks good on both
           desktop and tablet while still leaving a small margin on the side. */
        max-width: 90%;
        position: relative;
    }
    
    .message.system {
        background: #e5e5ea;
        color: #000;
        margin-left: auto;
        margin-right: 0;
        border-radius: 18px 18px 4px 18px;
    }
    
    .message.assistant {
        background: #007aff;
        color: white;
        margin-left: 0;
        margin-right: auto;
        border-radius: 18px 18px 18px 4px;
    }
    
    .message-header {
        font-weight: 600;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .command-badge {
        background: rgba(0, 0, 0, 0.2);
        color: inherit;
        padding: 0.2rem 0.5rem;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: 500;
        opacity: 0.8;
    }
    
    .message.system .command-badge {
        background: rgba(0, 0, 0, 0.15);
        color: #000;
    }
    
    .message.assistant .command-badge {
        background: rgba(255, 255, 255, 0.25);
        color: white;
    }
    
    /* Premium Glass-Morphism Code Block Container */
    .code-block {
        position: relative;
        margin: clamp(1.5rem, 2vw, 2rem) 0;
        border-radius: clamp(12px, 3vw, 20px);
        padding: 0;
        overflow: hidden;
        max-width: 100%;
        box-sizing: border-box;
        
        /* Glass-morphism backdrop effects */
        background: var(--code-bg);
        backdrop-filter: blur(16px);
        -webkit-backdrop-filter: blur(16px);
        
        /* Multi-layer shadows for depth */
        box-shadow: 
            0 8px 32px rgba(0, 0, 0, 0.12),
            0 2px 8px rgba(0, 0, 0, 0.08),
            inset 0 1px 0 rgba(255, 255, 255, 0.15),
            0 0 0 1px var(--code-border);
        
        /* Smooth animations */
        transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    }
    
    .code-block:hover {
        transform: translateY(-2px);
        box-shadow: 
            0 12px 40px rgba(0, 0, 0, 0.16),
            0 4px 12px rgba(0, 0, 0, 0.12),
            inset 0 1px 0 rgba(255, 255, 255, 0.2),
            0 0 0 1px rgba(255, 255, 255, 0.08);
    }
    
    /* Code Block Header with Copy Button */
    .code-block-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: clamp(0.75rem, 2vw, 1rem) clamp(1rem, 3vw, 1.5rem);
        background: var(--glass-header-bg);
        border-bottom: 1px solid var(--glass-header-border);
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
    }
    
    .code-language-tag {
        font-size: clamp(0.7rem, 1.5vw, 0.75rem);
        font-weight: 600;
        color: var(--glass-text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', system-ui, sans-serif;
    }
    
    .code-copy-btn {
        padding: clamp(0.4rem, 1vw, 0.5rem) clamp(0.6rem, 1.5vw, 0.8rem);
        background: var(--glass-btn-bg);
        border: 1px solid var(--glass-btn-border);
        border-radius: clamp(6px, 1.5vw, 8px);
        color: var(--glass-text-secondary);
        font-size: clamp(0.7rem, 1.5vw, 0.75rem);
        font-weight: 500;
        cursor: pointer;
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
        transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
        touch-action: manipulation;
        user-select: none;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', system-ui, sans-serif;
    }
    
    .code-copy-btn:hover {
        background: var(--glass-btn-hover);
        border-color: var(--glass-btn-border);
        transform: scale(1.02);
    }
    
    .code-copy-btn:active {
        transform: scale(0.98);
    }
    
    .code-copy-btn.copied {
        background: rgba(16, 185, 129, 0.15);
        border-color: rgba(16, 185, 129, 0.3);
        color: #059669;
    }
    
    /* Code Content Container */
    .code-content {
        position: relative;
        overflow: hidden;
    }
    
    /* Code Block Content (pre/code tags) */
    .code-block pre, .code-block code {
        margin: 0;
        padding: 0;
        background: none !important;
        font-family: 'SF Mono', 'Fira Code', 'Menlo', 'Consolas', monospace;
        font-size: clamp(0.8rem, 2vw, 0.85rem);
        line-height: 1.6;
        color: var(--code-text);
        text-shadow: none;
    }
    
    .code-block pre {
        padding: clamp(1rem, 3vw, 1.5rem);
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
    }
    
    /* Expand/Collapse functionality for long code blocks */
    .code-expandable .code-content {
        max-height: 400px; /* Default collapsed height */
        transition: max-height 0.5s cubic-bezier(0.4, 0, 0.2, 1);
    }
    
    .code-expandable .code-content.expanded {
        max-height: 2000px; /* Expanded height */
    }
    
    .code-expand-overlay {
        position: absolute;
        bottom: 0;
        left: 0;
        right: 0;
        height: 100px;
        background: var(--glass-expand-bg);
        display: flex;
        justify-content: center;
        align-items: flex-end;
        padding-bottom: 1rem;
        pointer-events: none;
        opacity: 1;
        transition: opacity 0.3s;
    }
    
    .code-expandable .code-content.expanded + .code-expand-overlay {
        opacity: 0;
    }
    
    .code-expand-btn {
        padding: 0.5rem 1rem;
        background: var(--glass-btn-bg);
        border: 1px solid var(--glass-btn-border);
        border-radius: 8px;
        color: var(--glass-text-secondary);
        font-size: 0.8rem;
        font-weight: 500;
        cursor: pointer;
        pointer-events: all;
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
        transition: all 0.2s;
    }
    
    .code-expand-btn:hover {
        background: var(--glass-btn-hover);
    }
    
    .code-expand-btn::after {
        content: 'Show More';
    }
    
    .code-expandable .code-content.expanded + .code-expand-overlay .code-expand-btn::after {
        content: 'Show Less';
    }
    
    /* Collapsible Sections */
    .collapsible-sections {
        margin: 2rem 0;
    }
    
    .collapsible-section {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 1rem;
        overflow: hidden;
    }
    
    .collapsible-section summary {
        padding: 1rem;
        font-weight: 600;
        cursor: pointer;
        background: #f8f9fa;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }
    
    .collapsible-section summary::after {
        content: '▼';
        font-size: 0.8rem;
        transition: transform 0.2s;
    }
    
    .collapsible-section[open] summary::after {
        transform: rotate(180deg);
    }
    
    .collapsible-section .section-content {
        padding: 1rem;
        background: white;
    }
    
    /* Special styling for plot sections to avoid double frames */
    .plot-section-content {
        background: transparent !important;
        padding: 0 !important;
    }
    
    .collapsible-section pre {
        background: var(--code-bg);
        padding: 1rem;
        border-radius: 6px;
        overflow-x: auto;
    }
    
    .best-code, .reference-code {
        max-height: 500px;
        overflow-y: auto;
    }
    
    /* Desktop-specific adjustments for collapsible sections */
    @media (min-width: 769px) {
        .collapsible-section {
            margin-bottom: 0.5rem;  /* Reduced from 1rem */
        }
        
        .collapsible-section summary {
            padding: 0.75rem 1rem;  /* Reduced vertical padding */
            font-size: 0.95rem;  /* Slightly smaller font */
        }
        
        .collapsible-section .section-content {
            padding: 0.75rem 1rem;  /* Reduced padding */
        }
        
        .collapsible-section pre {
            font-size: 0.85rem;  /* Smaller font for code blocks */
            line-height: 1.4;
            padding: 0.75rem;
        }
        
        /* Larger font size for reference and best code on desktop */
        .best-code {
            font-size: 1rem !important;  /* Increase from default */
            line-height: 1.5;
        }
        
        .reference-code {
            font-size: 1rem !important;  /* Increase from default */
            line-height: 1.5;
        }
        
        .collapsible-sections {
            margin: 1.5rem 0;  /* Reduced from 2rem */
        }
    }
    
    /* Floating back button - hidden by default */
    .mobile-back-button {
        display: none;
        position: fixed;
        top: 1rem;
        left: 1rem;
        z-index: 9999;  /* ensure it stays above all content */
        background: #2196F3;
        color: white;
        text-decoration: none;
        border-radius: 8px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
    }
    
    .mobile-back-button:hover {
        background: #1976D2;
        transform: scale(1.05);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    }
    
    .mobile-back-button:active {
        transform: scale(0.95);
    }
    
    .mobile-back-button svg {
        width: 24px;
        height: 24px;
    }
    
    /* Responsive adjustments for mobile */
    @media (max-width: 768px) {
        /* Show floating back button on mobile */
        .mobile-back-button {
            display: flex;
        }
        
        .main-content {
            margin-left: 0;
            padding: 60px 10px 0 10px;  /* Added top padding to account for floating button */
            max-width: 100vw;
        }
        
        .container {
            padding: 0 5px;
        }
        
        .plots-container {
            flex-direction: column;
        }
        
        .plot-half {
            width: 100%;
        }
        
        /* Keep plots within container on mobile */
        .plot-container {
            overflow: hidden;
            padding: 0.5rem;
        }
        
        .plot-container img {
            max-width: 100%;
            height: auto;
        }
        
        .message {
            max-width: 100%;
        }
        
        .header-section {
            margin-bottom: 0.5rem;
            text-align: left !important;
        }
        
        /* Mobile trajectory page adjustments */
        .task-info-line, .model-info-line {
            display: flex;
            flex-direction: column;
            gap: 4px;
            margin-bottom: 6px;
            text-align: left !important;
            align-items: flex-start !important;
        }
        
        .task-info-line span:first-child,
        .model-info-line span:first-child {
            font-size: 0.9rem;
            font-weight: 500;
        }
        
        .task-name-display,
        .model-name-display {
            font-size: 1.1rem !important;
            font-weight: 600;
        }
        
        .info-value {
            font-size: 1.1rem !important;
            font-weight: 600;
        }
        
        .header-section {
            margin-bottom: 0.5rem !important;
        }
        
        .header-section > div:first-child {
            margin-bottom: 8px !important;
        }
    }
    
    /* Ensure container doesn't center content on desktop */
    @media (min-width: 769px) {
        .container {
            margin: 0 !important;
            text-align: left !important;
        }
        .header-section {
            text-align: left !important;
        }
        .task-info-line, .model-info-line {
            text-align: left !important;
        }
    }

    /* Additional mobile adjustments for very small screens */
    @media (max-width: 480px) {
        .header-section {
            margin-bottom: 0.25rem !important;
        }
        
        .header-section > div:first-child {
            margin-bottom: 6px !important;
        }
        
        .task-info-line, .model-info-line {
            margin-bottom: 4px !important;
        }
        
        .info-section {
            margin-bottom: 0.25rem !important;
        }
    }

    .info-section {
        flex-wrap: wrap;
        justify-content: flex-start;
        margin-bottom: 0.5rem;
    }
    
    .info-item {
        min-width: 120px;
        flex-grow: 1;
    }

    .hide-on-mobile {
        display: flex;
    }

    /* Mobile adjustments */
    @media (max-width: 768px) {
        .hide-on-mobile {
            display: none !important;
        }
        
        /* Reduce gap between collapsible sections on mobile */
        .collapsible-sections {
            margin: 1rem 0;
        }
        
        .collapsible-section {
            margin-bottom: 0.5rem;
        }
        
        .collapsible-section summary {
            padding: 0.75rem;
            font-size: 0.9rem;
        }
        
        .collapsible-section .section-content {
            padding: 0.75rem;
        }
    }
    </style>
    <script>
        function copyCode(button, codeId) {
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                button.textContent = 'Copied!';
                button.classList.add('copied');
                setTimeout(() => {
                    button.textContent = 'Copy';
                    button.classList.remove('copied');
                }, 2000);
            });
        }

        function toggleExpand(codeBlockId) {
            const content = document.getElementById('content-' + codeBlockId);
            const overlay = document.getElementById('overlay-' + codeBlockId);
            content.classList.toggle('expanded');
            if (overlay) {
                overlay.style.display = content.classList.contains('expanded') ? 'none' : 'flex';
            }
        }
        
        document.addEventListener('DOMContentLoaded', () => {
            Prism.highlightAll();
        });
    </script>
</head>
<body>
    
        <div class="sidebar">
            <div class="sidebar-header">
                <a href="index.html#speedup-section" class="back-link">← Back to Speedup Table</a>
            </div>
            <div class="sidebar-content">
                <h3 style="color: black;">pagerank</h3>
                
            <div class="sidebar-task">
                <div class="task-runs">
                    
                <div class="sidebar-run current">
                    <a href="pagerank_Gemini_2.5_Pro.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">Gemini 2.5 Pro (30.97x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="pagerank_Claude_Opus_4.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">Claude Opus 4 (4.22x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="pagerank_o4-mini.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">o4-mini (2.61x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="pagerank_DeepSeek_R1.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">DeepSeek R1 (1.04x)</div>
                    </a>
                </div>
                </div>
            </div>
            </div>
        </div>
    
    <!-- Floating back button for mobile -->
    <a href="index.html#speedup-section" class="mobile-back-button" aria-label="Back to Speedup Table">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M19 12H5M5 12L12 19M5 12L12 5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
        </svg>
    </a>
    
    <div class="main-content">
        <div class="container">
            <div class="header-section" style="text-align: left !important; align-items: flex-start !important; justify-content: flex-start !important;">
                <div style="margin-bottom: 20px; display: flex; align-items: center; gap: 10px; justify-content: flex-start; text-align: left;">
                    <img src="assets/AlgoTunerMascot.png" alt="AlgoTune Mascot" style="height: 32px; width: auto;">
                    <span style="font-weight: 700; font-size: 1.5rem;">AlgoTuner Trajectory</span>
                </div>
                <div class="task-info-line" style="text-align: left !important; margin-bottom: 8px; display: block;">
                    <span style="color: #6c757d; font-weight: 400;">AlgoTune Task:</span>
                    <span class="task-name-display">pagerank</span>
                </div>
                <div class="model-info-line" style="text-align: left !important; display: block;">
                    <span style="color: #6c757d; font-weight: 400;">Model:</span>
                    <span class="model-name-display" style="font-weight: 500;">Gemini 2.5 Pro</span>
                </div>
            </div>
            
            <div class="info-section">
                <div class="info-item">
                    <div class="info-label">Speedup</div>
                    <div class="info-value" style="color: #006400; font-weight: 600;">30.966x</div>
                </div>
                <div class="info-item">
                    <div class="info-label">Total Budget</div>
                    <div class="info-value">$1.00</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Commands Executed</div>
                    <div class="info-value">14</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Invalid Commands</div>
                    <div class="info-value">1</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Average Reference Time (ms)</div>
                    <div class="info-value">49.7 ms</div>
                </div>
            </div>
            
            
    <div class="collapsible-sections">
        <details class="collapsible-section">
            <summary>Initial System Prompt</summary>
            <div class="section-content">
                <pre>SETTING:
You&#x27;re an autonomous programmer tasked with solving a specific problem. You are to use the commands defined below to accomplish this task. Every message you send incurs a cost—you will be informed of your usage and remaining budget by the system.
You will be evaluated based on the best-performing piece of code you produce, even if the final code doesn&#x27;t work or compile (as long as it worked at some point and achieved a score, you will be eligible).
Apart from the default Python packages, you have access to the following additional packages:
 - cryptography
 - cvxpy
 - cython
 - dace
 - dask
 - diffrax
 - ecos
 - faiss-cpu
 - hdbscan
 - highspy
 - jax
 - networkx
 - numba
 - numpy
 - ortools
 - pandas
 - pot
 - psutil
 - pulp
 - pyomo
 - python-sat
 - pythran
 - scikit-learn
 - scipy
 - sympy
 - torch

YOUR TASK:
Your objective is to define a class named `Solver` in `solver.py` with a method:
```
class Solver:
    def solve(self, problem, **kwargs) -&gt; Any:
        &quot;&quot;&quot;Your implementation goes here.&quot;&quot;&quot;
        ...
```

IMPORTANT: Compilation time of your init function will not count towards your function&#x27;s runtime.

This `solve` function will be the entrypoint called by the evaluation harness. Strive to align your class and method implementation as closely as possible with the desired performance criteria.
For each instance, your function can run for at most 10x the reference runtime for that instance. Strive to have your implementation run as fast as possible, while returning the same output as the reference function (for the same given input). Be creative and optimize your approach!

Your messages should include a short thought about what you should do, followed by a _SINGLE_ command. The command must be enclosed within ``` and ```, like so:
&lt;Reasoning behind executing the command&gt;
```
&lt;command&gt;
```

IMPORTANT: Each set of triple backticks (```) must always be on their own line, without any other words or anything else on that line.

Here are the commands available to you. Ensure you include one and only one of the following commands in each of your responses:
- `edit`: Replace a range of lines with new content in a file. This is how you can create files: if the file does not exist, it will be created. Here is an example:
  ```
  edit 
  file: &lt;file_name&gt;
  lines: &lt;start_line&gt;-&lt;end_line&gt;
  ---
  &lt;new_content&gt;
  ---
  ```

  The command will:
  1. Delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)
  2. Insert &lt;new_content&gt; starting at &lt;start_line&gt;
  3. If both &lt;start_line&gt; and &lt;end_line&gt; are 0, &lt;new_content&gt; will be prepended to the file
  
  Example:
  edit
  file: solver.py
  lines: 5-7
  ---
  def improved_function():
      print(&quot;Optimized solution&quot;)
  ---
- `ls`: List all files in the current working directory.
- `view_file &lt;file_name&gt; [start_line]`: Display 100 lines of `&lt;file_name&gt;` starting from `start_line` (defaults to line 1).
- `revert`: Revert the code to the best-performing version thus far.
- `reference &lt;string&gt;`: Query the reference solver with a problem and receive its solution. If the problem&#x27;s input is a list, this command would look like: 
  ```
  reference [1,2,3,4]
  ```
- `eval_input &lt;string&gt;`: Run your current solver implementation on the given input. This is the only command that shows stdout from your solver along with both solutions. Example: 
  ```
  eval_input [1,2,3,4]
  ```
- `eval`: Run evaluation on the current solution and report the results.
- `delete`: Delete a range of lines from a file using the format:
  ```
  delete
  file: &lt;file_name&gt;
  lines: &lt;start_line&gt;-&lt;end_line&gt;

  The command will delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)
  
  Example:
  delete
  file: solver.py
  lines: 5-10
  ```
- `profile &lt;filename.py&gt; &lt;input&gt;`: Profile your currently loaded solve method&#x27;s performance on a given input. Shows the 25 most time-consuming lines. Requires specifying a python file (e.g., `solver.py`) for validation, though profiling runs on the current in-memory code.
  Example:
  ```
  profile solver.py [1, 2, 3]
  ```

- `profile_lines &lt;filename.py&gt; &lt;line_number1, line_number2, ...&gt; &lt;input&gt;`: Profiles the chosen lines of the currently loaded code on the given input. Requires specifying a python file for validation.
  Example: 
  ```
  profile_lines solver.py 1,2,3 [1, 2, 3]
  ```

**TIPS:**
After each edit, a linter will automatically run to ensure code quality. If there are critical linter errors, your changes will not be applied, and you will receive the linter&#x27;s error message. Typically, linter errors arise from issues like improper indentation—ensure your edits maintain proper code formatting.
**Cython Compilation:** Edits creating or modifying Cython (`.pyx`) files will automatically trigger a compilation attempt (requires a `setup.py`). You will be notified if compilation succeeds or fails. If it fails, the edit to the `.pyx` file will be automatically reverted.
If the code runs successfully without errors, the in-memory &#x27;last known good code&#x27; will be updated to the new version. Following successful edits, you will receive a summary of your `solve` function&#x27;s performance compared to the reference.
If you get stuck, try reverting your code and restarting your train of thought.
Do not put an if __name__ == &quot;__main__&quot;: block in your code, as it will not be ran (only the solve function will).
Keep trying to better your code until you run out of money. Do not stop beforehand!

**GOALS:**
Your primary objective is to optimize the `solve` function to run as as fast as possible, while returning the optimal solution.
You will receive better scores the quicker your solution runs, and you will be penalized for exceeding the time limit or returning non-optimal solutions.

Below you find the description of the task you will have to solve. Read it carefully and understand what the problem is and what your solver should do.

**TASK DESCRIPTION:**
PageRank

Let G = (V, E) be a directed graph with n = |V| nodes labeled 0 through n−1 and edge set E ⊆ V × V. We represent G by its adjacency list, where adjacency_list[i] is the sorted list of nodes j for which the directed edge (i → j) exists. Define the out‑degree of node i as d_i = |adjacency_list[i]|.

PageRank models a “random surfer” who, at each time step, with probability α (the damping factor, typically 0.85) follows one of the outgoing links of the current node chosen uniformly at random, and with probability 1 − α “teleports” to any node in V chosen uniformly. Nodes with no outgoing links (d_i = 0) are treated as “dangling” and assumed to link to all nodes equally, ensuring proper redistribution of rank.

Formally, we construct the column‑stochastic transition matrix P ∈ ℝⁿ×ⁿ with entries  
    P_{ji} = 1 / d_i   if j ∈ adjacency_list[i] (d_i &gt; 0),  
    P_{ji} = 1 / n     if d_i = 0 (dangling node),  
    P_{ji} = 0         otherwise.  

The PageRank score vector r ∈ ℝⁿ is the unique solution to  
    r = α · P · r + (1 − α) · (1/n) · 1  
subject to ∑_{i=0}^{n−1} r_i = 1.  
It is computed by iterating  
    r^{(k+1)} = α · P · r^{(k)} + (1 − α) · (1/n) · 1  
until convergence (e.g., ‖r^{(k+1)} − r^{(k)}‖₁ &lt; ε for tolerance ε).


Input:
A dictionary containing a single key &quot;adjacency_list&quot;. The value associated with this key is a list of lists representing the graph&#x27;s directed adjacency structure. `adjacency_list[i]` contains a sorted list of integer indices corresponding to the nodes that node `i` points *to* (outgoing edges). Nodes are implicitly indexed from 0 to n-1, where n is the length of the outer list.

Example input:
{
  &quot;adjacency_list&quot;: [
    [1, 2],
    [2],   
    [0]
  ]
}

Output:
A dictionary containing a single key &quot;pagerank_scores&quot;. The value is a list of floating-point numbers representing the calculated PageRank score for each node, ordered by the node index (from 0 to n-1).

Example output:
(Note: Exact values depend on parameters like alpha=0.85, using networkx defaults)
{
  &quot;pagerank_scores&quot;: [
    0.3678861788617887,  // Score for node 0
    0.2926829268292683,  // Score for node 1
    0.3394308943089431   // Score for node 2
  ]
}

Category: graph

Below is the reference implementation. Your function should run much quicker.

import math
import random
from typing import Any
import networkx as nx
import numpy as np

| 01: def solve(self, problem: dict[str, list[list[int]]]) -&gt; dict[str, list[float]]:
| 02:     &quot;&quot;&quot;
| 03:     Calculates the PageRank scores for the graph using NetworkX.
| 04: 
| 05:     Args:
| 06:         problem: A dictionary containing the adjacency list of the graph.
| 07:                  {&quot;adjacency_list&quot;: adj_list}
| 08: 
| 09:     Returns:
| 10:         A dictionary containing the PageRank scores as a list, ordered by node index.
| 11:         {&quot;pagerank_scores&quot;: [score_node_0, score_node_1, ..., score_node_n-1]}
| 12:         Returns {&quot;pagerank_scores&quot;: []} for n=0.
| 13:         Returns {&quot;pagerank_scores&quot;: [1.0]} for n=1.
| 14:     
| 15: 
| 16:     NOTE: Your solution must pass validation by:
| 17:     1. Returning correctly formatted output
| 18:     2. Having no NaN or infinity values
| 19:     3. Matching expected results within numerical tolerance
| 20:     &quot;&quot;&quot;
| 21:     adj_list = problem[&quot;adjacency_list&quot;]
| 22:     n = len(adj_list)
| 23: 
| 24:     if n == 0:
| 25:         return {&quot;pagerank_scores&quot;: []}
| 26:     if n == 1:
| 27:         # Single node graph, PageRank is 1.0
| 28:         return {&quot;pagerank_scores&quot;: [1.0]}
| 29: 
| 30:     # Reconstruct the NetworkX DiGraph
| 31:     G = nx.DiGraph()
| 32:     G.add_nodes_from(range(n))
| 33:     for u, neighbors in enumerate(adj_list):
| 34:         for v in neighbors:
| 35:             # Add directed edges u -&gt; v
| 36:             G.add_edge(u, v)
| 37: 
| 38:     # Calculate PageRank using networkx defaults
| 39:     try:
| 40:         # Use specified parameters if needed, otherwise defaults are fine
| 41:         pagerank_dict = nx.pagerank(G, alpha=self.alpha, max_iter=self.max_iter, tol=self.tol)
| 42: 
| 43:         # Convert dict to list ordered by node index
| 44:         pagerank_list = [0.0] * n
| 45:         for node, score in pagerank_dict.items():
| 46:             if 0 &lt;= node &lt; n:
| 47:                 pagerank_list[node] = float(score)
| 48:             else:
| 49: 
| 50:         # Normalize just in case networkx doesn&#x27;t perfectly sum to 1 (numerical issues)
| 51:         # Though nx.pagerank is expected to return a normalized dict
| 52:         # score_sum = sum(pagerank_list)
| 53:         # if n &gt; 0 and not math.isclose(score_sum, 1.0, rel_tol=1e-5):
| 54:         #     if score_sum &gt; 0:
| 55:         #         pagerank_list = [s / score_sum for s in pagerank_list]
| 56:         #     else: # Should not happen for n&gt;0 unless graph is empty or weird issue
| 57:         #          pagerank_list = [1.0 / n] * n # Assign uniform probability if sum is zero
| 58: 
| 59:     except nx.PowerIterationFailedConvergence:
| 60:         # Return uniform distribution as a fallback? Or zeros? Let&#x27;s return zeros.
| 61:         pagerank_list = [0.0] * n
| 62:     except Exception as e:
| 63:         # Return zeros as a fallback
| 64:         pagerank_list = [0.0] * n
| 65: 
| 66:     solution = {&quot;pagerank_scores&quot;: pagerank_list}
| 67:     return solution
| 68: 

This function will be used to check if your solution is valid for a given problem. If it returns False, it means the solution is invalid:

import math
import random
from typing import Any
import networkx as nx
import numpy as np

| 001: def is_solution(
| 002:     self,
| 003:     problem: dict[str, list[list[int]]],
| 004:     solution: dict[str, Any],  # Use Any and validate internally
| 005: ) -&gt; bool:
| 006:     &quot;&quot;&quot;
| 007:     Check if the provided PageRank scores solution is valid.
| 008: 
| 009:     Checks structure, type, list length, score validity (non-negative, finite),
| 010:     sum close to 1.0 (if n &gt; 0), and numerical closeness to the reference
| 011:     networkx.pagerank output.
| 012: 
| 013:     Args:
| 014:         problem: The problem definition dictionary.
| 015:         solution: The proposed solution dictionary.
| 016: 
| 017:     Returns:
| 018:         True if the solution is valid and numerically close to reference, False otherwise.
| 019:     &quot;&quot;&quot;
| 020:     if &quot;adjacency_list&quot; not in problem:
| 021:         logging.error(&quot;Problem dictionary missing &#x27;adjacency_list&#x27;.&quot;)
| 022:         return False
| 023:     adj_list = problem[&quot;adjacency_list&quot;]
| 024:     n = len(adj_list)
| 025: 
| 026:     # --- Structural and Type Checks ---
| 027:     if not isinstance(solution, dict) or &quot;pagerank_scores&quot; not in solution:
| 028:         logging.error(&quot;Solution format invalid: not a dict or missing &#x27;pagerank_scores&#x27; key.&quot;)
| 029:         return False
| 030: 
| 031:     proposed_scores = solution[&quot;pagerank_scores&quot;]
| 032: 
| 033:     if not isinstance(proposed_scores, list):
| 034:         logging.error(
| 035:             f&quot;Proposed &#x27;pagerank_scores&#x27; is not a list (type: {type(proposed_scores)}).&quot;
| 036:         )
| 037:         return False
| 038: 
| 039:     if len(proposed_scores) != n:
| 040:         logging.error(
| 041:             f&quot;Proposed scores list length {len(proposed_scores)} does not match number of nodes {n}.&quot;
| 042:         )
| 043:         return False
| 044: 
| 045:     # Check individual score validity (type, non-negative, finite)
| 046:     for i, score in enumerate(proposed_scores):
| 047:         if not isinstance(score, float | int):
| 048:             logging.error(
| 049:                 f&quot;Score at index {i} is not a float or int (value: {score}, type: {type(score)}).&quot;
| 050:             )
| 051:             return False
| 052:         try:
| 053:             float_score = float(score)
| 054:             if not math.isfinite(float_score):
| 055:                 logging.error(f&quot;Score at index {i} is not finite (value: {float_score}).&quot;)
| 056:                 return False
| 057:             if float_score &lt; 0.0:
| 058:                 logging.error(f&quot;Score at index {i} is negative (value: {float_score}).&quot;)
| 059:                 return False
| 060:         except (ValueError, TypeError):
| 061:             logging.error(f&quot;Score at index {i} &#x27;{score}&#x27; cannot be converted to a valid float.&quot;)
| 062:             return False
| 063: 
| 064:     # Convert proposed solution to numpy array for easier comparison
| 065:     try:
| 066:         proposed_scores_np = np.array(proposed_scores, dtype=float)
| 067:     except Exception as e:
| 068:         logging.error(f&quot;Could not convert proposed scores to numpy float array: {e}&quot;)
| 069:         return False  # Should have been caught above, but as a safeguard
| 070: 
| 071:     # --- Handle Edge Cases ---
| 072:     if n == 0:
| 073:         if not proposed_scores_np.shape == (0,):  # Check it&#x27;s an empty array/list
| 074:             logging.error(&quot;Solution for n=0 should be an empty list/array.&quot;)
| 075:             return False
| 076:         logging.debug(&quot;Solution verification successful for n=0.&quot;)
| 077:         return True
| 078:     if n == 1:
| 079:         expected_scores_np = np.array([1.0])
| 080:         if np.allclose(proposed_scores_np, expected_scores_np, rtol=RTOL, atol=ATOL):
| 081:             logging.debug(&quot;Solution verification successful for n=1.&quot;)
| 082:             return True
| 083:         else:
| 084:             logging.error(
| 085:                 f&quot;Proposed score {proposed_scores_np} != expected {expected_scores_np} for n=1.&quot;
| 086:             )
| 087:             return False
| 088: 
| 089:     # --- Sum Check ---
| 090:     # PageRank scores must sum to 1.0 (within tolerance) for n &gt; 0
| 091:     proposed_sum = np.sum(proposed_scores_np)
| 092:     if not math.isclose(proposed_sum, 1.0, rel_tol=RTOL, abs_tol=ATOL):
| 093:         logging.error(
| 094:             f&quot;Proposed PageRank scores do not sum to 1.0 (Sum={proposed_sum}, Tolerance=atol={ATOL}, rtol={RTOL}).&quot;
| 095:         )
| 096:         return False
| 097: 
| 098:     # --- Numerical Comparison with Reference ---
| 099:     try:
| 100:         reference_solution = solve(problem)  # Re-compute reference
| 101:         ref_scores = reference_solution.get(&quot;pagerank_scores&quot;)
| 102:         if ref_scores is None:  # Should not happen based on solve() logic
| 103:             logging.error(&quot;Reference solution computation did not return &#x27;pagerank_scores&#x27;.&quot;)
| 104:             return False
| 105:         ref_scores_np = np.array(ref_scores, dtype=float)
| 106: 
| 107:         # Check if reference generation failed (e.g., convergence error in solve)
| 108:         # If reference is all zeros (our fallback), we cannot reliably compare.
| 109:         # However, if proposed is also all zeros, maybe accept? Or always fail if ref failed?
| 110:         # Let&#x27;s decide to fail if reference computation failed (indicated by zeros here).
| 111:         # Note: A graph could legitimately have all zero pagerank if it has no nodes, handled above.
| 112:         # For n&gt;1, if ref_scores are all zero, it indicates a failure in solve().
| 113:         if n &gt; 0 and np.allclose(ref_scores_np, 0.0, atol=ATOL):
| 114:             logging.error(
| 115:                 &quot;Reference solution computation failed (returned all zeros), cannot verify.&quot;
| 116:             )
| 117:             # Or potentially return True if proposed is *also* all zeros? No, stick to failing.
| 118:             return False
| 119: 
| 120:     except Exception as e:
| 121:         logging.error(f&quot;Error computing reference solution during verification: {e}&quot;)
| 122:         return False  # Cannot verify if reference fails
| 123: 
| 124:     # Compare values using numpy.allclose
| 125:     if not np.allclose(proposed_scores_np, ref_scores_np, rtol=RTOL, atol=ATOL):
| 126:         logging.error(
| 127:             f&quot;Solution verification failed: PageRank scores mismatch. &quot;
| 128:             f&quot;Proposed sum={proposed_sum}, Reference sum={np.sum(ref_scores_np)}. &quot;
| 129:             f&quot;(rtol={RTOL}, atol={ATOL})&quot;
| 130:             # Optional: Log first few differing values if helpful
| 131:             # diff_indices = np.where(~np.isclose(proposed_scores_np, ref_scores_np, rtol=RTOL, atol=ATOL))[0]
| 132:             # logging.error(f&quot;First few differing indices: {diff_indices[:5]}&quot;)
| 133:             # logging.error(f&quot;Proposed values: {proposed_scores_np[diff_indices[:5]]}&quot;)
| 134:             # logging.error(f&quot;Reference values: {ref_scores_np[diff_indices[:5]]}&quot;)
| 135:         )
| 136:         return False
| 137: 
| 138:     logging.debug(&quot;Solution verification successful.&quot;)
| 139:     return True
| 140:</pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>AlgoTune Task Description</summary>
            <div class="section-content">
                <pre>PageRank

Let G = (V, E) be a directed graph with n = |V| nodes labeled 0 through n−1 and edge set E ⊆ V × V. We represent G by its adjacency list, where adjacency_list[i] is the sorted list of nodes j for which the directed edge (i → j) exists. Define the out‑degree of node i as d_i = |adjacency_list[i]|.

PageRank models a “random surfer” who, at each time step, with probability α (the damping factor, typically 0.85) follows one of the outgoing links of the current node chosen uniformly at random, and with probability 1 − α “teleports” to any node in V chosen uniformly. Nodes with no outgoing links (d_i = 0) are treated as “dangling” and assumed to link to all nodes equally, ensuring proper redistribution of rank.

Formally, we construct the column‑stochastic transition matrix P ∈ ℝⁿ×ⁿ with entries  
    P_{ji} = 1 / d_i   if j ∈ adjacency_list[i] (d_i &gt; 0),  
    P_{ji} = 1 / n     if d_i = 0 (dangling node),  
    P_{ji} = 0         otherwise.  

The PageRank score vector r ∈ ℝⁿ is the unique solution to  
    r = α · P · r + (1 − α) · (1/n) · 1  
subject to ∑_{i=0}^{n−1} r_i = 1.  
It is computed by iterating  
    r^{(k+1)} = α · P · r^{(k)} + (1 − α) · (1/n) · 1  
until convergence (e.g., ‖r^{(k+1)} − r^{(k)}‖₁ &lt; ε for tolerance ε).


Input:
A dictionary containing a single key &quot;adjacency_list&quot;. The value associated with this key is a list of lists representing the graph&#x27;s directed adjacency structure. `adjacency_list[i]` contains a sorted list of integer indices corresponding to the nodes that node `i` points *to* (outgoing edges). Nodes are implicitly indexed from 0 to n-1, where n is the length of the outer list.

Example input:
{
  &quot;adjacency_list&quot;: [
    [1, 2],
    [2],   
    [0]
  ]
}

Output:
A dictionary containing a single key &quot;pagerank_scores&quot;. The value is a list of floating-point numbers representing the calculated PageRank score for each node, ordered by the node index (from 0 to n-1).

Example output:
(Note: Exact values depend on parameters like alpha=0.85, using networkx defaults)
{
  &quot;pagerank_scores&quot;: [
    0.3678861788617887,  // Score for node 0
    0.2926829268292683,  // Score for node 1
    0.3394308943089431   // Score for node 2
  ]
}

Category: graph</pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Reference Implementation</summary>
            <div class="section-content">
                <pre class="reference-code"><code class="language-python">import math
import random
from typing import Any
import networkx as nx
import numpy as np

 def solve(self, problem: dict[str, list[list[int]]]) -&gt; dict[str, list[float]]:
     &quot;&quot;&quot;
     Calculates the PageRank scores for the graph using NetworkX.
 
     Args:
         problem: A dictionary containing the adjacency list of the graph.
                  {&quot;adjacency_list&quot;: adj_list}
 
     Returns:
         A dictionary containing the PageRank scores as a list, ordered by node index.
         {&quot;pagerank_scores&quot;: [score_node_0, score_node_1, ..., score_node_n-1]}
         Returns {&quot;pagerank_scores&quot;: []} for n=0.
         Returns {&quot;pagerank_scores&quot;: [1.0]} for n=1.
     
 
     NOTE: Your solution must pass validation by:
     1. Returning correctly formatted output
     2. Having no NaN or infinity values
     3. Matching expected results within numerical tolerance
     &quot;&quot;&quot;
     adj_list = problem[&quot;adjacency_list&quot;]
     n = len(adj_list)
 
     if n == 0:
         return {&quot;pagerank_scores&quot;: []}
     if n == 1:
         # Single node graph, PageRank is 1.0
         return {&quot;pagerank_scores&quot;: [1.0]}
 
     # Reconstruct the NetworkX DiGraph
     G = nx.DiGraph()
     G.add_nodes_from(range(n))
     for u, neighbors in enumerate(adj_list):
         for v in neighbors:
             # Add directed edges u -&gt; v
             G.add_edge(u, v)
 
     # Calculate PageRank using networkx defaults
     try:
         # Use specified parameters if needed, otherwise defaults are fine
         pagerank_dict = nx.pagerank(G, alpha=self.alpha, max_iter=self.max_iter, tol=self.tol)
 
         # Convert dict to list ordered by node index
         pagerank_list = [0.0] * n
         for node, score in pagerank_dict.items():
             if 0 &lt;= node &lt; n:
                 pagerank_list[node] = float(score)
             else:
 
         # Normalize just in case networkx doesn&#x27;t perfectly sum to 1 (numerical issues)
         # Though nx.pagerank is expected to return a normalized dict
         # score_sum = sum(pagerank_list)
         # if n &gt; 0 and not math.isclose(score_sum, 1.0, rel_tol=1e-5):
         #     if score_sum &gt; 0:
         #         pagerank_list = [s / score_sum for s in pagerank_list]
         #     else: # Should not happen for n&gt;0 unless graph is empty or weird issue
         #          pagerank_list = [1.0 / n] * n # Assign uniform probability if sum is zero
 
     except nx.PowerIterationFailedConvergence:
         # Return uniform distribution as a fallback? Or zeros? Let&#x27;s return zeros.
         pagerank_list = [0.0] * n
     except Exception as e:
         # Return zeros as a fallback
         pagerank_list = [0.0] * n
 
     solution = {&quot;pagerank_scores&quot;: pagerank_list}
     return solution</code></pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Best AlgoTuner-Generated Code</summary>
            <div class="section-content">
                <div class="best-file"><div class="file-name" style="font-weight:600; margin-bottom:0.25rem;">solver.py</div><pre class="best-code"><code class="language-python">import numpy as np
import numba
from typing import Any, List
import itertools

# This helper function is JIT-compiled by Numba for performance.
# It implements the core PageRank power iteration logic.
@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    out_degree: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using power iteration, accelerated with Numba.

    Args:
        adj_list_indices: CSR-style array of neighbor indices.
        adj_list_indptr: CSR-style array of index pointers.
        out_degree: Array of out-degrees for each node.
        n: Number of nodes in the graph.
        alpha: Damping factor.
        max_iter: Maximum number of iterations.
        tol: Convergence tolerance for L1 norm.

    Returns:
        A numpy array of PageRank scores.
    &quot;&quot;&quot;
    # Initialize PageRank scores uniformly
    r = np.full(n, 1.0 / n, dtype=np.float64)

    # Identify dangling nodes (nodes with no outgoing links)
    dangling_nodes_mask = out_degree == 0
    
    # Pre-calculate the constant teleportation term
    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        r_last = r
        r_new = np.zeros(n, dtype=np.float64)

        # 1. Calculate the total rank from dangling nodes
        dangling_rank_sum = 0.0
        # This loop is faster inside numba than np.sum(r_last[dangling_nodes_mask])
        for i in range(n):
            if dangling_nodes_mask[i]:
                dangling_rank_sum += r_last[i]
        
        # Distribute dangling rank and teleportation probability to all nodes
        dangling_and_teleport = alpha * (dangling_rank_sum / n) + teleport_val
        r_new += dangling_and_teleport

        # 2. Distribute rank from non-dangling nodes
        for i in range(n):
            if not dangling_nodes_mask[i]:
                # Distribute this node&#x27;s rank to its neighbors
                start = adj_list_indptr[i]
                end = adj_list_indptr[i + 1]
                rank_to_distribute = alpha * r_last[i] / out_degree[i]
                for k in range(start, end):
                    j = adj_list_indices[k]
                    r_new[j] += rank_to_distribute
        
        r = r_new

        # 3. Check for convergence using L1 norm
        err = 0.0
        for i in range(n):
            err += abs(r[i] - r_last[i])
        
        if err &lt; n * tol: # Scale tolerance by n as in networkx
            break
            
    return r

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    power iteration method for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6 # networkx default tolerance

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}
        if n == 1:
            return {&quot;pagerank_scores&quot;: [1.0]}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Optimized conversion of adjacency list to CSR-like format
        
        # Calculate out_degrees (lengths of neighbor lists)
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)

        # Efficiently create indptr using cumsum
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])

        # Efficiently create indices by flattening the adjacency list
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, out_degree, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}</code></pre></div>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Speedup vs Budget Plot</summary>
            <div class="section-content plot-section-content">
                
    <div class="plots-container">
        <div class="plot-section plot-half">
            <h3>Speedup vs Budget</h3>
            <div class="plot-container">
                <img src="assets/plots/pagerank_gemini-2.5-pro_20250628_182330_perf.png" alt="Speedup vs Budget Plot" />
            </div>
        </div>
        <div class="plot-section plot-half">
            <h3>Action Sequence</h3>
            <div class="plot-container">
                <img src="assets/plots/pagerank_gemini-2.5-pro_20250628_182330_actions.png" alt="Action Sequence Plot" />
            </div>
        </div>
    </div>
            </div>
        </details>
    </div>
            
            <div class="conversation-section">
                <h2>Conversation Log</h2>
                
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                I&#x27;ve identified the core of the PageRank algorithm as a power iteration method, which can be significantly slow in pure Python due to its iterative nature, especially the sparse matrix-vector multiplication step. The reference implementation&#x27;s use of `networkx` likely introduces overhead from graph construction.<br><br>My strategy is to bypass `networkx` and implement the power iteration directly. To achieve high performance, I will use `numba` to Just-In-Time (JIT) compile the computationally intensive iteration loop. This involves:<br>1.  Creating a `_pagerank_numba` helper function decorated with `@numba.njit`.<br>2.  Converting the input adjacency list into a more `numba`-friendly format, specifically a CSR-like representation using NumPy arrays (`indices` and `indptr`), which is highly efficient for iterating over sparse graph edges.<br>3.  Implementing the optimized PageRank update rule within the `numba` function, handling dangling nodes and the damping factor efficiently.<br>4.  The main `solve` method will handle data preparation and call this compiled function.<br><br>This approach should be substantially faster than the reference by avoiding Python&#x27;s interpreted loop overhead and leveraging `numba`&#x27;s compilation to efficient machine code.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import numba
from typing import Any, List

# This helper function is JIT-compiled by Numba for performance.
# It implements the core PageRank power iteration logic.
@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using power iteration, accelerated with Numba.

    Args:
        adj_list_indices: CSR-style array of neighbor indices.
        adj_list_indptr: CSR-style array of index pointers.
        n: Number of nodes in the graph.
        alpha: Damping factor.
        max_iter: Maximum number of iterations.
        tol: Convergence tolerance for L1 norm.

    Returns:
        A numpy array of PageRank scores.
    &quot;&quot;&quot;
    # Calculate out-degrees from the CSR representation
    out_degree = np.zeros(n, dtype=np.int32)
    for i in range(n):
        out_degree[i] = adj_list_indptr[i + 1] - adj_list_indptr[i]

    # Initialize PageRank scores uniformly
    r = np.full(n, 1.0 / n, dtype=np.float64)

    # Identify dangling nodes (nodes with no outgoing links)
    dangling_nodes_mask = out_degree == 0
    
    # Pre-calculate the constant teleportation term
    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        r_last = r
        r_new = np.zeros(n, dtype=np.float64)

        # 1. Calculate the total rank from dangling nodes
        dangling_rank_sum = 0.0
        # This loop is faster inside numba than np.sum(r_last[dangling_nodes_mask])
        for i in range(n):
            if dangling_nodes_mask[i]:
                dangling_rank_sum += r_last[i]
        
        # Distribute dangling rank and teleportation probability to all nodes
        dangling_and_teleport = alpha * (dangling_rank_sum / n) + teleport_val
        r_new += dangling_and_teleport

        # 2. Distribute rank from non-dangling nodes
        for i in range(n):
            if not dangling_nodes_mask[i]:
                # Distribute this node&#x27;s rank to its neighbors
                start = adj_list_indptr[i]
                end = adj_list_indptr[i + 1]
                rank_to_distribute = alpha * r_last[i] / out_degree[i]
                for k in range(start, end):
                    j = adj_list_indices[k]
                    r_new[j] += rank_to_distribute
        
        r = r_new

        # 3. Check for convergence using L1 norm
        err = 0.0
        for i in range(n):
            err += abs(r[i] - r_last[i])
        
        if err &lt; n * tol: # Scale tolerance by n as in networkx
            break
            
    return r


class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    power iteration method for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6 # networkx default tolerance

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}
        if n == 1:
            return {&quot;pagerank_scores&quot;: [1.0]}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Convert adjacency list to a Numba-friendly CSR-like format.
        # This is much more efficient for Numba to process than a list of lists.
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        indices = np.empty(num_edges, dtype=np.int32)
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            num_neighbors = len(neighbors)
            end_pos = current_pos + num_neighbors
            if num_neighbors &gt; 0:
                indices[current_pos:end_pos] = neighbors
            indptr[i + 1] = end_pos
            current_pos = end_pos

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 1 messages and have used up $0.0917. You have $0.9083 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 138)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: import numpy as np
&gt; 002: import numba
&gt; 003: from typing import Any, List
&gt; 004: 
&gt; 005: # This helper function is JIT-compiled by Numba for performance.
&gt; 006: # It implements the core PageRank power iteration logic.
&gt; 007: @numba.njit(cache=True, fastmath=True)
&gt; 008: def _pagerank_numba(
&gt; 009:     adj_list_indices: np.ndarray,
&gt; 010:     adj_list_indptr: np.ndarray,
&gt; 011:     n: int,
&gt; 012:     alpha: float,
&gt; 013:     max_iter: int,
&gt; 014:     tol: float,
&gt; 015: ) -&gt; np.ndarray:
&gt; 016:     &quot;&quot;&quot;
&gt; 017:     Calculates PageRank scores using power iteration, accelerated with Numba.
&gt; 018: 
&gt; 019:     Args:
&gt; 020:         adj_list_indices: CSR-style array of neighbor indices.
&gt; 021:         adj_list_indptr: CSR-style array of index pointers.
&gt; 022:         n: Number of nodes in the graph.
&gt; 023:         alpha: Damping factor.
&gt; 024:         max_iter: Maximum number of iterations.
&gt; 025:         tol: Convergence tolerance for L1 norm.
&gt; 026: 
&gt; 027:     Returns:
&gt; 028:         A numpy array of PageRank scores.
&gt; 029:     &quot;&quot;&quot;
&gt; 030:     # Calculate out-degrees from the CSR representation
&gt; 031:     out_degree = np.zeros(n, dtype=np.int32)
&gt; 032:     for i in range(n):
&gt; 033:         out_degree[i] = adj_list_indptr[i + 1] - adj_list_indptr[i]
&gt; 034: 
&gt; 035:     # Initialize PageRank scores uniformly
&gt; 036:     r = np.full(n, 1.0 / n, dtype=np.float64)
&gt; 037: 
&gt; 038:     # Identify dangling nodes (nodes with no outgoing links)
&gt; 039:     dangling_nodes_mask = out_degree == 0
&gt; 040:     
&gt; 041:     # Pre-calculate the constant teleportation term
&gt; 042:     teleport_val = (1.0 - alpha) / n
&gt; 043: 
&gt; 044:     for _ in range(max_iter):
&gt; 045:         r_last = r
&gt; 046:         r_new = np.zeros(n, dtype=np.float64)
&gt; 047: 
&gt; 048:         # 1. Calculate the total rank from dangling nodes
&gt; 049:         dangling_rank_sum = 0.0
&gt; 050:         # This loop is faster inside numba than np.sum(r_last[dangling_nodes_mask])
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">17.35</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Snapshot saved Best speedup reached, state saved!<br>Among the 10+ LLMs we tested, your code did not rank in the top 3 for speed. Please use all available packages and tools to optimize its performance. Think outside the box!
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge other"> Invalid Command</span>
            </div>
            <div class="message-content">
                After achieving a 17.35x speedup with a Numba-based implementation, it&#x27;s clear that to gain a significant further advantage, a more fundamental change in the computational backend is required. The bottleneck in PageRank is the repeated sparse matrix-vector multiplication. While Numba is excellent, it&#x27;s hard to beat the highly optimized, often parallelized, and cache-aware C/Fortran routines (like BLAS/MKL) that are leveraged by libraries like SciPy.<br><br>My new strategy is to replace the custom Numba loop with `scipy.sparse`. This approach involves:<br><br>1.  **Matrix Construction**: Instead of iterating with Numba, I will first construct the column-stochastic transition matrix of the graph as a `scipy.sparse.csr_matrix`. This format is exceptionally efficient for matrix-vector products (`spmv`). The matrix `M` will be built such that `M_ji = 1 / out_degree(i)` for every edge from node `i` to `j`. This setup is a one-time cost.<br><br>2.  **Optimized Iteration**: The core power iteration loop will be driven by `M @ r`, which calls SciPy&#x27;s optimized `spmv` routine. This single operation replaces the main nested loop in the previous implementation.<br><br>3.  **Vectorized Operations**: All other parts of the update rule—handling dangling nodes and applying the damping factor—will be performed using fast, vectorized NumPy operations.<br><br>This method trades the zero-overhead approach of Numba for the setup cost of a sparse matrix, banking on the massively superior performance of SciPy&#x27;s `spmv` to dominate the runtime, especially for the large graphs typical in performance benchmarks.<br><br><pre><code class="language-python">import numpy as np
import scipy.sparse
from typing import Any, List

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses scipy.sparse for a highly
    optimized power iteration method.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph using scipy.sparse.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # 1. Build the transition matrix M in CSR format.
        # M_ji = 1/out_degree(i) for edges i-&gt;j.
        # This is the column-stochastic matrix for pre-multiplication (M @ r).
        
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
        
        # Identify dangling nodes (nodes with no outgoing links)
        dangling_nodes_mask = (out_degree == 0)
        
        # The sparse matrix M only contains transitions from non-dangling nodes.
        # The contribution from dangling nodes is handled separately in the iteration.
        num_edges = out_degree.sum()
        
        # Pre-allocate for COO format lists, an efficient way to build the matrix
        row_ind = np.empty(num_edges, dtype=np.int32)
        col_ind = np.empty(num_edges, dtype=np.int32)
        data = np.empty(num_edges, dtype=np.float64)
        
        # Populate COO arrays by iterating over non-dangling nodes
        current_pos = 0
        non_dangling_indices = np.where(~dangling_nodes_mask)[0]
        
        for i in non_dangling_indices:
            neighbors = adj_list[i]
            num_neighbors = len(neighbors)
            end_pos = current_pos + num_neighbors
            
            col_ind[current_pos:end_pos] = i
            row_ind[current_pos:end_pos] = neighbors
            data[current_pos:end_pos] = 1.0 / num_neighbors
            
            current_pos = end_pos

        M = scipy.sparse.csr_matrix((data, (row_ind, col_ind)), shape=(n, n))

        # 2. Power Iteration
        # Initialize PageRank scores uniformly
        r = np.full(n, 1.0 / n, dtype=np.float64)
        
        # Pre-calculate the constant teleportation term
        teleport_val = (1.0 - alpha) / n

        for _ in range(max_iter):
            r_last = r

            # Perform the sparse matrix-vector product for non-dangling contributions
            r_new = alpha * (M @ r_last)
            
            # Calculate the total rank from dangling nodes
            dangling_rank_sum = np.sum(r_last[dangling_nodes_mask])
            
            # Add dangling and teleportation contributions to all nodes
            r_new += (alpha * dangling_rank_sum / n + teleport_val)

            # Check for convergence using L1 norm (sum of absolute differences)
            # The tolerance is scaled by n, as in networkx, to be less sensitive to graph size
            err = np.abs(r_new - r_last).sum()
            r = r_new
            if err &lt; n * tol:
                break
        
        return {&quot;pagerank_scores&quot;: r.tolist()}</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 2 messages and have used up $0.1930. You have $0.8070 remaining.<br><br>Error: Command parsing failed<br>Command parsing failed. <br>Remember to include one and only one command in each message. Important: remember to include all arguments for each command.<br>Remember to sandwich your command in between ``` and ```. <br>IMPORTANT: Each set of triple backticks (```) must always be on their own line, without any other words or anything else on that line.<br><br>Here are the commands available to you. Ensure you include one and only one of the following commands in each of your responses:<br>- `edit`: Replace a range of lines with new content in a file. This is how you can create files: if the file does not exist, it will be created. Here is an example:<br>  ```<br>  edit <br>  file: &lt;file_name&gt;<br>  lines: &lt;start_line&gt;-&lt;end_line&gt;<br>  ---<br>  &lt;new_content&gt;<br>  ---<br>  ```<br><br>  The command will:<br>  1. Delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)<br>  2. Insert &lt;new_content&gt; starting at &lt;start_line&gt;<br>  3. If both &lt;start_line&gt; and &lt;end_line&gt; are 0, &lt;new_content&gt; will be prepended to the file<br>  <br>  Example:<br>  edit<br>  file: solver.py<br>  lines: 5-7<br>  ---<br>  def improved_function():<br>      print(&quot;Optimized solution&quot;)<br>  ---<br>- `ls`: List all files in the current working directory.<br>- `view_file &lt;file_name&gt; [start_line]`: Display 100 lines of `&lt;file_name&gt;` starting from `start_line` (defaults to line 1).<br>- `revert`: Revert the code to the best-performing version thus far.<br>- `reference &lt;string&gt;`: Query the reference solver with a problem and receive its solution. If the problem&#x27;s input is a list, this command would look like: <br>  ```<br>  reference [1,2,3,4]<br>  ```<br>- `eval_input &lt;string&gt;`: Run your current solver implementation on the given input. This is the only command that shows stdout from your solver along with both solutions. Example: <br>  ```<br>  eval_input [1,2,3,4]<br>  ```<br>- `eval`: Run evaluation on the current solution and report the results.<br>- `delete`: Delete a range of lines from a file using the format:<br>  ```<br>  delete<br>  file: &lt;file_name&gt;<br>  lines: &lt;start_line&gt;-&lt;end_line&gt;<br><br>  The command will delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)<br>  <br>  Example:<br>  ```<br>  delete<br>  file: solver.py<br>  lines: 5-10<br>  ```<br>- `profile &lt;filename.py&gt; &lt;input&gt;`: Profile your currently loaded solve method&#x27;s performance on a given input. Shows the 25 most time-consuming lines. Requires specifying a python file (e.g., `solver.py`) for validation, though profiling runs on the current in-memory code.<br>  Example:<br>  ```<br>  profile solver.py [1, 2, 3]<br>  ```<br>- `profile_lines &lt;filename.py&gt; &lt;line_number1, line_number2, ...&gt; &lt;input&gt;`: Profiles the chosen lines of the currently loaded code on the given input. Requires specifying a python file for validation.<br>  Example: <br>  ```<br>  profile_lines solver.py 1,2,3 [1, 2, 3]<br>  ```
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                I apologize for the formatting error. I will now resubmit my proposed solution using the correct `edit` command format.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import scipy.sparse
from typing import Any, List

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses scipy.sparse for a highly
    optimized power iteration method.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph using scipy.sparse.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # 1. Build the transition matrix M in CSR format.
        # M_ji = 1/out_degree(i) for edges i-&gt;j.
        # This is the column-stochastic matrix for pre-multiplication (M @ r).
        
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
        
        # Identify dangling nodes (nodes with no outgoing links)
        dangling_nodes_mask = (out_degree == 0)
        
        # The sparse matrix M only contains transitions from non-dangling nodes.
        # The contribution from dangling nodes is handled separately in the iteration.
        num_edges = out_degree.sum()
        
        # Pre-allocate for COO format lists, an efficient way to build the matrix
        row_ind = np.empty(num_edges, dtype=np.int32)
        col_ind = np.empty(num_edges, dtype=np.int32)
        data = np.empty(num_edges, dtype=np.float64)
        
        # Populate COO arrays by iterating over non-dangling nodes
        current_pos = 0
        non_dangling_indices = np.where(~dangling_nodes_mask)[0]
        
        for i in non_dangling_indices:
            neighbors = adj_list[i]
            num_neighbors = len(neighbors)
            end_pos = current_pos + num_neighbors
            
            col_ind[current_pos:end_pos] = i
            row_ind[current_pos:end_pos] = neighbors
            data[current_pos:end_pos] = 1.0 / num_neighbors
            
            current_pos = end_pos

        M = scipy.sparse.csr_matrix((data, (row_ind, col_ind)), shape=(n, n))

        # 2. Power Iteration
        # Initialize PageRank scores uniformly
        r = np.full(n, 1.0 / n, dtype=np.float64)
        
        # Pre-calculate the constant teleportation term
        teleport_val = (1.0 - alpha) / n

        for _ in range(max_iter):
            r_last = r

            # Perform the sparse matrix-vector product for non-dangling contributions
            r_new = alpha * (M @ r_last)
            
            # Calculate the total rank from dangling nodes
            dangling_rank_sum = np.sum(r_last[dangling_nodes_mask])
            
            # Add dangling and teleportation contributions to all nodes
            r_new += (alpha * dangling_rank_sum / n + teleport_val)

            # Check for convergence using L1 norm (sum of absolute differences)
            # The tolerance is scaled by n, as in networkx, to be less sensitive to graph size
            err = np.abs(r_new - r_last).sum()
            r = r_new
            if err &lt; n * tol:
                break
        
        return {&quot;pagerank_scores&quot;: r.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 3 messages and have used up $0.2125. You have $0.7875 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 99)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: import numpy as np
&gt; 02: import scipy.sparse
&gt; 03: from typing import Any, List
&gt; 04: 
&gt; 05: class Solver:
&gt; 06:     &quot;&quot;&quot;
&gt; 07:     A solver for the PageRank problem that uses scipy.sparse for a highly
&gt; 08:     optimized power iteration method.
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     def __init__(self):
&gt; 11:         &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
&gt; 12:         self.alpha = 0.85
&gt; 13:         self.max_iter = 100
&gt; 14:         self.tol = 1.0e-6
&gt; 15: 
&gt; 16:     def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
&gt; 17:         &quot;&quot;&quot;
&gt; 18:         Calculates PageRank scores for a given graph using scipy.sparse.
&gt; 19: 
&gt; 20:         Args:
&gt; 21:             problem: A dictionary containing the adjacency list of the graph.
&gt; 22:             **kwargs: Can be used to override default parameters like alpha, max_iter, tol.
&gt; 23: 
&gt; 24:         Returns:
&gt; 25:             A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
&gt; 26:         &quot;&quot;&quot;
&gt; 27:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 28:         n = len(adj_list)
&gt; 29: 
&gt; 30:         if n == 0:
&gt; 31:             return {&quot;pagerank_scores&quot;: []}
&gt; 32: 
&gt; 33:         # Override parameters if provided in kwargs
&gt; 34:         alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
&gt; 35:         max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
&gt; 36:         tol = kwargs.get(&quot;tol&quot;, self.tol)
&gt; 37: 
&gt; 38:         # 1. Build the transition matrix M in CSR format.
&gt; 39:         # M_ji = 1/out_degree(i) for edges i-&gt;j.
&gt; 40:         # This is the column-stochastic matrix for pre-multiplication (M @ r).
&gt; 41:         
&gt; 42:         out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
&gt; 43:         
&gt; 44:         # Identify dangling nodes (nodes with no outgoing links)
&gt; 45:         dangling_nodes_mask = (out_degree == 0)
&gt; 46:         
&gt; 47:         # The sparse matrix M only contains transitions from non-dangling nodes.
&gt; 48:         # The contribution from dangling nodes is handled separately in the iteration.
&gt; 49:         num_edges = out_degree.sum()
&gt; 50:         
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">7.98</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge revert">↩️ Revert Changes</span>
            </div>
            <div class="message-content">
                My `scipy.sparse` approach was slower than the previous `numba` implementation. This is likely due to the significant one-time cost of constructing the sparse matrix, which outweighs the benefit of its fast matrix-vector product, especially if the test cases converge in a small number of iterations. The `numba` solution had a much lower setup cost.<br><br>My analysis suggests the main remaining bottleneck in the fast `numba` solution is the pure Python code used to prepare the data structures (the CSR-like arrays) before passing them to the compiled function.<br><br>My new strategy is to optimize this data preparation step while retaining the proven `numba` core:<br>1.  **Revert**: I will revert to the high-performing `numba` implementation.<br>2.  **Optimize CSR Creation**: I will replace the Python `for` loop that builds the `indices` and `indptr` arrays with a more efficient, vectorized approach.<br>    *   The `indptr` array can be constructed efficiently using `np.array([len(..)])` followed by `np.cumsum()`.<br>    *   The `indices` array can be built by flattening the adjacency list using the highly optimized `itertools.chain.from_iterable` and then converting it to a NumPy array with `np.fromiter`, pre-specifying the size for maximum efficiency.<br>3.  **Refine Numba Function**: The `out_degree` array is a byproduct of the new `indptr` calculation. I will pass it directly to the `numba` function to avoid redundant computation inside the JIT&#x27;d code.<br><br>This hybrid approach focuses on minimizing Python-level overhead while keeping the core computation in the fast, compiled `numba` environment.<br><br>First, I will revert to the previous best solution.<br><br><pre><code class="language-python">revert</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 4 messages and have used up $0.2548. You have $0.7452 remaining.<br><br>Successfully reverted to last saved state.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My previous attempt with `scipy.sparse` was slower than the `numba` solution, likely due to the overhead of constructing the sparse matrix. The `numba` implementation&#x27;s performance is solid, but the data preparation in pure Python remains a bottleneck.<br><br>My new strategy is to optimize the creation of the CSR-like arrays (`indices` and `indptr`) that are fed into the `numba`-compiled function. I will replace the Python `for` loop used for this conversion with faster, vectorized NumPy and `itertools` operations.<br><br>1.  **Optimized `indptr` Creation**: The `indptr` array, which stores the pointers to the start of each node&#x27;s neighbors in the `indices` array, can be generated much faster. I&#x27;ll first get the length of each sublist (which is also the out-degree) and then use `numpy.cumsum` to create the pointers in a single, highly optimized operation.<br>2.  **Optimized `indices` Creation**: The `indices` array, which is a flattened list of all neighbors, will be created using `itertools.chain.from_iterable` to create a fast iterator over the adjacency list, and then `numpy.fromiter` to efficiently convert this iterator into a NumPy array. This avoids all Python-level iteration.<br>3.  **Streamline `numba` Function**: Since the out-degree of each node is calculated during the new `indptr` creation, I will pass it directly to the `_pagerank_numba` function, removing a redundant calculation from the JIT-compiled code.<br><br>This approach minimizes the time spent in the slower, interpreted Python code, allowing the program to enter the fast, compiled `numba` section as quickly as possible.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import numba
from typing import Any, List
import itertools

# This helper function is JIT-compiled by Numba for performance.
# It implements the core PageRank power iteration logic.
@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    out_degree: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using power iteration, accelerated with Numba.

    Args:
        adj_list_indices: CSR-style array of neighbor indices.
        adj_list_indptr: CSR-style array of index pointers.
        out_degree: Array of out-degrees for each node.
        n: Number of nodes in the graph.
        alpha: Damping factor.
        max_iter: Maximum number of iterations.
        tol: Convergence tolerance for L1 norm.

    Returns:
        A numpy array of PageRank scores.
    &quot;&quot;&quot;
    # Initialize PageRank scores uniformly
    r = np.full(n, 1.0 / n, dtype=np.float64)

    # Identify dangling nodes (nodes with no outgoing links)
    dangling_nodes_mask = out_degree == 0
    
    # Pre-calculate the constant teleportation term
    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        r_last = r
        r_new = np.zeros(n, dtype=np.float64)

        # 1. Calculate the total rank from dangling nodes
        dangling_rank_sum = 0.0
        # This loop is faster inside numba than np.sum(r_last[dangling_nodes_mask])
        for i in range(n):
            if dangling_nodes_mask[i]:
                dangling_rank_sum += r_last[i]
        
        # Distribute dangling rank and teleportation probability to all nodes
        dangling_and_teleport = alpha * (dangling_rank_sum / n) + teleport_val
        r_new += dangling_and_teleport

        # 2. Distribute rank from non-dangling nodes
        for i in range(n):
            if not dangling_nodes_mask[i]:
                # Distribute this node&#x27;s rank to its neighbors
                start = adj_list_indptr[i]
                end = adj_list_indptr[i + 1]
                rank_to_distribute = alpha * r_last[i] / out_degree[i]
                for k in range(start, end):
                    j = adj_list_indices[k]
                    r_new[j] += rank_to_distribute
        
        r = r_new

        # 3. Check for convergence using L1 norm
        err = 0.0
        for i in range(n):
            err += abs(r[i] - r_last[i])
        
        if err &lt; n * tol: # Scale tolerance by n as in networkx
            break
            
    return r


class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    power iteration method for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6 # networkx default tolerance

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}
        if n == 1:
            return {&quot;pagerank_scores&quot;: [1.0]}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Optimized conversion of adjacency list to CSR-like format
        
        # Calculate out_degrees (lengths of neighbor lists)
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)

        # Efficiently create indptr using cumsum
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])

        # Efficiently create indices by flattening the adjacency list
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, out_degree, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 5 messages and have used up $0.3128. You have $0.6872 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 136)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: import numpy as np
&gt; 002: import numba
&gt; 003: from typing import Any, List
&gt; 004: import itertools
&gt; 005: 
&gt; 006: # This helper function is JIT-compiled by Numba for performance.
&gt; 007: # It implements the core PageRank power iteration logic.
&gt; 008: @numba.njit(cache=True, fastmath=True)
&gt; 009: def _pagerank_numba(
&gt; 010:     adj_list_indices: np.ndarray,
&gt; 011:     adj_list_indptr: np.ndarray,
&gt; 012:     out_degree: np.ndarray,
&gt; 013:     n: int,
&gt; 014:     alpha: float,
&gt; 015:     max_iter: int,
&gt; 016:     tol: float,
&gt; 017: ) -&gt; np.ndarray:
&gt; 018:     &quot;&quot;&quot;
&gt; 019:     Calculates PageRank scores using power iteration, accelerated with Numba.
&gt; 020: 
&gt; 021:     Args:
&gt; 022:         adj_list_indices: CSR-style array of neighbor indices.
&gt; 023:         adj_list_indptr: CSR-style array of index pointers.
&gt; 024:         out_degree: Array of out-degrees for each node.
&gt; 025:         n: Number of nodes in the graph.
&gt; 026:         alpha: Damping factor.
&gt; 027:         max_iter: Maximum number of iterations.
&gt; 028:         tol: Convergence tolerance for L1 norm.
&gt; 029: 
&gt; 030:     Returns:
&gt; 031:         A numpy array of PageRank scores.
&gt; 032:     &quot;&quot;&quot;
&gt; 033:     # Initialize PageRank scores uniformly
&gt; 034:     r = np.full(n, 1.0 / n, dtype=np.float64)
&gt; 035: 
&gt; 036:     # Identify dangling nodes (nodes with no outgoing links)
&gt; 037:     dangling_nodes_mask = out_degree == 0
&gt; 038:     
&gt; 039:     # Pre-calculate the constant teleportation term
&gt; 040:     teleport_val = (1.0 - alpha) / n
&gt; 041: 
&gt; 042:     for _ in range(max_iter):
&gt; 043:         r_last = r
&gt; 044:         r_new = np.zeros(n, dtype=np.float64)
&gt; 045: 
&gt; 046:         # 1. Calculate the total rank from dangling nodes
&gt; 047:         dangling_rank_sum = 0.0
&gt; 048:         # This loop is faster inside numba than np.sum(r_last[dangling_nodes_mask])
&gt; 049:         for i in range(n):
&gt; 050:             if dangling_nodes_mask[i]:
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">31.13</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Snapshot saved Best speedup reached, state saved!<br>Among the 10+ LLMs we tested, your code did not rank in the top 3 for speed. Please use all available packages and tools to optimize its performance. Think outside the box!
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                After reaching a 31x speedup, it&#x27;s clear that while the `numba` approach is fast, it&#x27;s not the fastest possible. The feedback suggests that top solutions leverage libraries even more effectively. My previous `scipy.sparse` attempt was slow due to the Python-loop-based construction of the sparse matrix.<br><br>My new strategy is to revisit the `scipy.sparse` approach but with a completely vectorized, loop-free method for constructing the matrix. This addresses the primary bottleneck of the previous attempt.<br><br>1.  **Vectorized Matrix Construction**: I will construct the sparse transition matrix `M` without any explicit Python loops.<br>    *   The `row_ind` (destination nodes) and `col_ind` (source nodes) of the matrix in COO format will be generated using `numpy.fromiter` with `itertools.chain` and `numpy.repeat`, respectively.<br>    *   The `data` array (containing `1 / out_degree` for each edge) will also be generated using `numpy.repeat`.<br>    *   These arrays will be used to instantiate a `scipy.sparse.csr_matrix` directly, which is extremely fast.<br><br>2.  **Fully Vectorized Power Iteration**: The iteration loop will be driven by SciPy&#x27;s highly optimized sparse matrix-vector product (`M @ r`) and NumPy&#x27;s vectorized arithmetic. This offloads the entire computational workload to low-level, compiled C/Fortran code.<br><br>This approach combines the best of both worlds: NumPy&#x27;s power for data manipulation and SciPy&#x27;s specialized, high-performance routines for the core iterative calculation. By eliminating all Python-level loops from the performance-critical sections, this implementation should significantly outperform the `numba`-based solution.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import scipy.sparse
from typing import Any, List
import itertools

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a fully vectorized scipy.sparse
    and numpy implementation for maximum performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph using a vectorized
        scipy.sparse power iteration.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # 1. Vectorized construction of the transition matrix M.
        # M_ji = 1/out_degree(i) for an edge i-&gt;j.
        # This is the matrix for pre-multiplication: r_new = M @ r_old.

        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
        num_edges = out_degree.sum()
        
        dangling_nodes_mask = (out_degree == 0)

        # Create COO matrix components in a fully vectorized way.
        # `col_ind` (source nodes `i`) are repeated `out_degree[i]` times.
        col_ind = np.repeat(np.arange(n, dtype=np.int32), out_degree)
        
        # `row_ind` (destination nodes `j`) are the flattened adjacency lists.
        if num_edges &gt; 0:
            row_ind = np.fromiter(itertools.chain.from_iterable(adj_list), dtype=np.int32, count=num_edges)
        else:
            row_ind = np.array([], dtype=np.int32)

        # `data` is 1/out_degree[i] for each edge.
        # We use np.repeat on the reciprocals of the out-degrees.
        out_degree_reciprocal = np.zeros(n, dtype=np.float64)
        non_dangling_mask = ~dangling_nodes_mask
        out_degree_reciprocal[non_dangling_mask] = 1.0 / out_degree[non_dangling_mask]
        data = np.repeat(out_degree_reciprocal, out_degree)

        # Create the sparse matrix in CSR format for efficient spmv (sparse matrix-vector product).
        M = scipy.sparse.csr_matrix((data, (row_ind, col_ind)), shape=(n, n))

        # 2. Power Iteration using vectorized numpy and scipy operations.
        r = np.full(n, 1.0 / n, dtype=np.float64)
        
        for _ in range(max_iter):
            r_last = r
            
            # Calculate the total rank from dangling nodes.
            dangling_sum = r_last[dangling_nodes_mask].sum()
            
            # Calculate the constant value to add to all nodes, combining
            # dangling node redistribution and the teleportation factor.
            add_on = (alpha * dangling_sum + 1.0 - alpha) / n
            
            # The main vectorized update rule.
            r = alpha * (M @ r_last) + add_on

            # Check for convergence using L1 norm.
            if np.abs(r - r_last).sum() &lt; n * tol:
                break
        
        return {&quot;pagerank_scores&quot;: r.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 6 messages and have used up $0.3924. You have $0.6076 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 89)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: import numpy as np
&gt; 02: import scipy.sparse
&gt; 03: from typing import Any, List
&gt; 04: import itertools
&gt; 05: 
&gt; 06: class Solver:
&gt; 07:     &quot;&quot;&quot;
&gt; 08:     A solver for the PageRank problem that uses a fully vectorized scipy.sparse
&gt; 09:     and numpy implementation for maximum performance.
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     def __init__(self):
&gt; 12:         &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
&gt; 13:         self.alpha = 0.85
&gt; 14:         self.max_iter = 100
&gt; 15:         self.tol = 1.0e-6
&gt; 16: 
&gt; 17:     def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
&gt; 18:         &quot;&quot;&quot;
&gt; 19:         Calculates PageRank scores for a given graph using a vectorized
&gt; 20:         scipy.sparse power iteration.
&gt; 21: 
&gt; 22:         Args:
&gt; 23:             problem: A dictionary containing the adjacency list of the graph.
&gt; 24:             **kwargs: Can be used to override default parameters like alpha, max_iter, tol.
&gt; 25: 
&gt; 26:         Returns:
&gt; 27:             A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
&gt; 28:         &quot;&quot;&quot;
&gt; 29:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 30:         n = len(adj_list)
&gt; 31: 
&gt; 32:         if n == 0:
&gt; 33:             return {&quot;pagerank_scores&quot;: []}
&gt; 34: 
&gt; 35:         # Override parameters if provided
&gt; 36:         alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
&gt; 37:         max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
&gt; 38:         tol = kwargs.get(&quot;tol&quot;, self.tol)
&gt; 39: 
&gt; 40:         # 1. Vectorized construction of the transition matrix M.
&gt; 41:         # M_ji = 1/out_degree(i) for an edge i-&gt;j.
&gt; 42:         # This is the matrix for pre-multiplication: r_new = M @ r_old.
&gt; 43: 
&gt; 44:         out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
&gt; 45:         num_edges = out_degree.sum()
&gt; 46:         
&gt; 47:         dangling_nodes_mask = (out_degree == 0)
&gt; 48: 
&gt; 49:         # Create COO matrix components in a fully vectorized way.
&gt; 50:         # `col_ind` (source nodes `i`) are repeated `out_degree[i]` times.
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">23.37</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The current `numba` solution is fast, but the feedback indicates there&#x27;s still significant room for improvement. My analysis of the `numba`-accelerated function reveals several micro-optimizations that, when combined, should yield a substantial speedup. The current implementation loops over all nodes multiple times within the main iteration, using masks to differentiate behavior. This can be improved.<br><br>My new strategy is to refine the `_pagerank_numba` function by restructuring its internal logic to minimize redundant operations and improve data access patterns:<br><br>1.  **Pre-computation of Node Types**: Instead of checking if a node is &quot;dangling&quot; in every iteration, I will use `np.where` once, before the main loop, to create separate arrays of indices for dangling and non-dangling nodes.<br>2.  **Efficient Loop Iteration**: The main loop will iterate over these smaller, pre-computed index arrays (`dangling_nodes_indices`, `non_dangling_nodes_indices`) instead of looping from `0` to `n` and applying a conditional check. This reduces loop overhead and improves cache locality.<br>3.  **Optimized Vector Initialization**: I will replace the `r_new = np.zeros(...)` followed by a vector addition (`r_new += ...`) with a single, more efficient `r_new = np.full(...)` call. This initializes the new rank vector with the correct base value (from teleportation and dangling nodes) in one operation.<br>4.  **Vectorized Convergence Check**: I will switch from a manual summation loop for the error calculation to `np.abs(r - r_last).sum()`. Numba heavily optimizes NumPy universal functions, often leveraging SIMD instructions, which can be faster than an explicit Python-style loop.<br><br>These changes collectively reduce the work done inside the most critical loop of the algorithm, leading to a more streamlined and performant `numba` core.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import numba
from typing import Any, List
import itertools

@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    out_degree: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using a highly optimized power iteration,
    accelerated with Numba.
    &quot;&quot;&quot;
    r = np.full(n, 1.0 / n, dtype=np.float64)

    # Pre-calculate indices for dangling and non-dangling nodes to optimize loops
    dangling_nodes_indices = np.where(out_degree == 0)[0]
    non_dangling_nodes_indices = np.where(out_degree &gt; 0)[0]

    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        r_last = r

        # 1. Calculate total rank from dangling nodes by iterating over the pre-computed list
        dangling_rank_sum = 0.0
        for i in dangling_nodes_indices:
            dangling_rank_sum += r_last[i]
        
        # 2. Initialize new rank vector with a single np.full call.
        # This combines the dangling rank distribution and the teleportation value.
        base_rank = alpha * (dangling_rank_sum / n) + teleport_val
        r_new = np.full(n, base_rank, dtype=np.float64)

        # 3. Distribute rank from non-dangling nodes by iterating over the pre-computed list
        for i in non_dangling_nodes_indices:
            start = adj_list_indptr[i]
            end = adj_list_indptr[i + 1]
            rank_to_distribute = alpha * r_last[i] / out_degree[i]
            for k in range(start, end):
                j = adj_list_indices[k]
                r_new[j] += rank_to_distribute
        
        r = r_new

        # 4. Check for convergence using a vectorized L1 norm calculation
        err = np.abs(r - r_last).sum()
        if err &lt; n * tol:
            break
            
    return r


class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    power iteration method for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Optimized conversion of adjacency list to CSR-like format
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)

        # Efficiently create indptr using cumsum
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])

        # Efficiently create indices by flattening the adjacency list
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, out_degree, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 7 messages and have used up $0.4840. You have $0.5160 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 104)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: import numpy as np
&gt; 002: import numba
&gt; 003: from typing import Any, List
&gt; 004: import itertools
&gt; 005: 
&gt; 006: @numba.njit(cache=True, fastmath=True)
&gt; 007: def _pagerank_numba(
&gt; 008:     adj_list_indices: np.ndarray,
&gt; 009:     adj_list_indptr: np.ndarray,
&gt; 010:     out_degree: np.ndarray,
&gt; 011:     n: int,
&gt; 012:     alpha: float,
&gt; 013:     max_iter: int,
&gt; 014:     tol: float,
&gt; 015: ) -&gt; np.ndarray:
&gt; 016:     &quot;&quot;&quot;
&gt; 017:     Calculates PageRank scores using a highly optimized power iteration,
&gt; 018:     accelerated with Numba.
&gt; 019:     &quot;&quot;&quot;
&gt; 020:     r = np.full(n, 1.0 / n, dtype=np.float64)
&gt; 021: 
&gt; 022:     # Pre-calculate indices for dangling and non-dangling nodes to optimize loops
&gt; 023:     dangling_nodes_indices = np.where(out_degree == 0)[0]
&gt; 024:     non_dangling_nodes_indices = np.where(out_degree &gt; 0)[0]
&gt; 025: 
&gt; 026:     teleport_val = (1.0 - alpha) / n
&gt; 027: 
&gt; 028:     for _ in range(max_iter):
&gt; 029:         r_last = r
&gt; 030: 
&gt; 031:         # 1. Calculate total rank from dangling nodes by iterating over the pre-computed list
&gt; 032:         dangling_rank_sum = 0.0
&gt; 033:         for i in dangling_nodes_indices:
&gt; 034:             dangling_rank_sum += r_last[i]
&gt; 035:         
&gt; 036:         # 2. Initialize new rank vector with a single np.full call.
&gt; 037:         # This combines the dangling rank distribution and the teleportation value.
&gt; 038:         base_rank = alpha * (dangling_rank_sum / n) + teleport_val
&gt; 039:         r_new = np.full(n, base_rank, dtype=np.float64)
&gt; 040: 
&gt; 041:         # 3. Distribute rank from non-dangling nodes by iterating over the pre-computed list
&gt; 042:         for i in non_dangling_nodes_indices:
&gt; 043:             start = adj_list_indptr[i]
&gt; 044:             end = adj_list_indptr[i + 1]
&gt; 045:             rank_to_distribute = alpha * r_last[i] / out_degree[i]
&gt; 046:             for k in range(start, end):
&gt; 047:                 j = adj_list_indices[k]
&gt; 048:                 r_new[j] += rank_to_distribute
&gt; 049:         
&gt; 050:         r = r_new
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">29.58</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge revert">↩️ Revert Changes</span>
            </div>
            <div class="message-content">
                My `numba` solution has reached a performance plateau, and the feedback strongly suggests that a different approach, likely using a specialized, high-performance library, is necessary to achieve a top-tier speedup. My previous attempts to use `scipy.sparse` were hampered by the matrix construction overhead.<br><br>My new strategy is to delegate the entire PageRank calculation to the `igraph` library. `igraph` is a C library with Python bindings, renowned for its performance in graph analysis. Its built-in `pagerank` function uses the PRPACK C++ library, which is one of the fastest implementations available. This approach avoids re-implementing the algorithm in Python and instead leverages a pre-compiled, highly optimized solution.<br><br>The plan is as follows:<br>1.  **Efficient Data Conversion**: Convert the input adjacency list into a flat list of edge tuples `(source, destination)`, which is the format `igraph` prefers. I will use `numpy.repeat` and `itertools.chain.from_iterable` to perform this conversion without any slow Python loops.<br>2.  **Optimized Graph Construction**: Create an `igraph.Graph` object directly from the edge list using the `igraph.Graph(n=n, edges=edges, directed=True)` constructor. This is the most efficient way to build the graph and ensures all isolated nodes are included from the start.<br>3.  **Direct PageRank Call**: Call the `g.pagerank()` method, passing the `alpha` (damping) and `max_iter` parameters. This offloads the entire computationally intensive power iteration to `igraph`&#x27;s C backend.<br><br>This &quot;outside the box&quot; strategy of using a specialized third-party library is the most promising path to a significant performance leap beyond what is achievable with `numba` or `scipy` alone.<br><br>First, I will revert to my previous best state to ensure a clean implementation.<br><br><pre><code class="language-python">revert</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 8 messages and have used up $0.5529. You have $0.4471 remaining.<br><br>Successfully reverted to last saved state.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My `numba` solution has hit a performance ceiling. The feedback to &quot;think outside the box&quot; and &quot;use all available packages&quot; strongly implies that re-implementing the algorithm myself, even with JIT compilation, is not the optimal path. The most effective strategy is to delegate the entire computation to a specialized, high-performance graph library whose core is written in a low-level language like C or C++.<br><br>My new strategy is to use the `igraph` library, which is backed by a highly optimized C core and includes one of the fastest PageRank implementations available (PRPACK).<br><br>1.  **Delegate to `igraph`**: I will remove my custom power iteration logic entirely and replace it with a call to `igraph.Graph.pagerank()`. This function is a wrapper around a state-of-the-art C/C++ implementation, which will be significantly faster than my Python/Numba code.<br><br>2.  **Efficient Graph Construction**: The main task in my Python code is now to convert the input adjacency list into an `igraph.Graph` object as quickly as possible. I will construct an edge list (a list of `(source, destination)` tuples) using fast, vectorized operations:<br>    *   `numpy.fromiter` and `map` will be used to calculate the out-degrees.<br>    *   `numpy.repeat` will generate the `source` nodes for all edges.<br>    *   `itertools.chain.from_iterable` will flatten the adjacency list to get the `target` nodes.<br>    *   `zip` will combine these into an edge list iterator, which is then used to build the `igraph.Graph` object.<br><br>This approach minimizes Python&#x27;s role to just data marshalling, offloading the entire heavy computation to a purpose-built, pre-compiled C library, which should result in a substantial performance gain.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import igraph
from typing import Any, List
import itertools

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that leverages the high-performance
    C-based &#x27;igraph&#x27; library for maximum speed.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores by constructing an igraph.Graph object
        and calling its highly optimized built-in pagerank method.

        Args:
            problem: A dictionary containing the adjacency list of the graph.
            **kwargs: Can be used to override default parameters like alpha, max_iter, tol.

        Returns:
            A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # 1. Efficiently create the edge list for igraph.
        # This avoids slow Python loops for data preparation.

        # Use map and fromiter for a fast out_degree calculation.
        out_degrees = np.fromiter(map(len, adj_list), dtype=np.int32, count=n)
        
        # np.repeat is highly optimized for creating the source nodes array.
        sources = np.repeat(np.arange(n, dtype=np.int32), out_degrees)
        
        # itertools.chain.from_iterable is the fastest way to flatten the adjacency list.
        targets = itertools.chain.from_iterable(adj_list)
        
        # 2. Create the igraph.Graph object.
        # The constructor is optimized for an edge list.
        # `n=n` ensures isolated vertices are correctly included.
        g = igraph.Graph(n=n, edges=list(zip(sources, targets)), directed=True)

        # 3. Call the highly optimized igraph pagerank implementation (PRPACK).
        # This offloads the entire computation to the C backend.
        pagerank_scores = g.pagerank(
            damping=alpha,
            niter=max_iter,
            eps=tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 9 messages and have used up $0.6120. You have $0.3880 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 65)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: import numpy as np
&gt; 02: import igraph
&gt; 03: from typing import Any, List
&gt; 04: import itertools
&gt; 05: 
&gt; 06: class Solver:
&gt; 07:     &quot;&quot;&quot;
&gt; 08:     A solver for the PageRank problem that leverages the high-performance
&gt; 09:     C-based &#x27;igraph&#x27; library for maximum speed.
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     def __init__(self):
&gt; 12:         &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
&gt; 13:         self.alpha = 0.85
&gt; 14:         self.max_iter = 100
&gt; 15:         self.tol = 1.0e-6
&gt; 16: 
&gt; 17:     def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
&gt; 18:         &quot;&quot;&quot;
&gt; 19:         Calculates PageRank scores by constructing an igraph.Graph object
&gt; 20:         and calling its highly optimized built-in pagerank method.
&gt; 21: 
&gt; 22:         Args:
&gt; 23:             problem: A dictionary containing the adjacency list of the graph.
&gt; 24:             **kwargs: Can be used to override default parameters like alpha, max_iter, tol.
&gt; 25: 
&gt; 26:         Returns:
&gt; 27:             A dictionary with the key &quot;pagerank_scores&quot; and a list of scores.
&gt; 28:         &quot;&quot;&quot;
&gt; 29:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 30:         n = len(adj_list)
&gt; 31: 
&gt; 32:         if n == 0:
&gt; 33:             return {&quot;pagerank_scores&quot;: []}
&gt; 34: 
&gt; 35:         # Override parameters if provided
&gt; 36:         alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
&gt; 37:         max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
&gt; 38:         tol = kwargs.get(&quot;tol&quot;, self.tol)
&gt; 39: 
&gt; 40:         # 1. Efficiently create the edge list for igraph.
&gt; 41:         # This avoids slow Python loops for data preparation.
&gt; 42: 
&gt; 43:         # Use map and fromiter for a fast out_degree calculation.
&gt; 44:         out_degrees = np.fromiter(map(len, adj_list), dtype=np.int32, count=n)
&gt; 45:         
&gt; 46:         # np.repeat is highly optimized for creating the source nodes array.
&gt; 47:         sources = np.repeat(np.arange(n, dtype=np.int32), out_degrees)
&gt; 48:         
&gt; 49:         # itertools.chain.from_iterable is the fastest way to flatten the adjacency list.
&gt; 50:         targets = itertools.chain.from_iterable(adj_list)
...</code></pre><br><br>Solver validation failed: Failed to import solver.py: Module execution timed out or failed for /pfs/work9/workspace/scratch/tu_ptiig01-scratch/TEMP/task_pid_2004360_0kM2bj/solver.py
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge revert">↩️ Revert Changes</span>
            </div>
            <div class="message-content">
                The `igraph` library is not available in the evaluation environment, causing the import to fail. I must revert to a solution using the provided libraries (`numpy`, `scipy`, `numba`).<br><br>My best performing solution so far (31.13x speedup) used a `numba`-accelerated power iteration. My analysis of that solution revealed a key inefficiency: within each iteration, the code performed two separate loops over all `n` nodes—once to sum up the rank of dangling nodes, and a second time to distribute the rank from non-dangling nodes.<br><br>My new strategy is to refactor the `numba`-jitted function to perform these operations in a single pass.<br><br>1.  **Revert to Best `numba` Solution**: I will start from my previous best implementation, which has the most optimized data preparation pipeline.<br>2.  **Single-Pass Iteration Logic**: I will rewrite the core `_pagerank_numba` function.<br>    *   The new rank vector, `r_new`, will be initialized with only the base teleportation value (`(1-alpha)/n`).<br>    *   I will then iterate through all nodes **once**.<br>    *   If a node is non-dangling, its rank is distributed to its neighbors and added to `r_new`.<br>    *   If a node is dangling, its rank is accumulated into a `dangling_sum` scalar.<br>    *   After this single loop completes, the total rank from all dangling nodes (`alpha * dangling_sum / n`) is added to the `r_new` vector in one final, fast, broadcasted operation.<br><br>This new single-pass approach is mathematically equivalent to the old two-pass one but significantly more efficient. It reduces the total number of operations and improves data locality within the most critical loop of the algorithm, which should provide a substantial speedup.<br><br>First, I will revert to the previous best state.<br><pre><code class="language-python">revert</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 10 messages and have used up $0.6775. You have $0.3225 remaining.<br><br>Successfully reverted to last saved state.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                After the `igraph` library proved unavailable, I must return to optimizing the `numba`-based solution. My previous best `numba` implementation (31.13x speedup) used a two-pass approach within each iteration: one pass to sum the rank of dangling nodes, and a second pass to distribute the rank of non-dangling nodes. This is inefficient.<br><br>My new strategy is to refactor the core `numba`-jitted function to use a **single-pass loop**. This is a significant algorithmic refinement that reduces the number of passes over the data by half, which should improve performance by enhancing data locality and reducing loop overhead.<br><br>1.  **Single-Pass Logic**: I will rewrite the main loop inside `_pagerank_numba`.<br>    *   The new rank vector, `r_new`, will be initialized with the base teleportation value (`(1-alpha)/n`).<br>    *   A single loop will iterate through all nodes from `i = 0 to n-1`.<br>    *   Inside the loop, an `if/else` block will check if node `i` is dangling.<br>        *   If dangling, its rank is added to a `dangling_sum` scalar.<br>        *   If not dangling, its rank is distributed to its neighbors and added to `r_new`.<br>    *   After this single loop completes, the total accumulated `dangling_sum` is broadcast-added to the `r_new` vector in one final, vectorized step.<br><br>2.  **Optimized Data Preparation**: The `Solver` class will retain its highly optimized data preparation pipeline, which uses `numpy.cumsum` and `itertools.chain.from_iterable` to convert the adjacency list to a CSR-like format with minimal Python overhead.<br><br>This single-pass approach is mathematically identical to the previous two-pass method but is computationally superior. By processing all nodes in one go, it represents the most efficient structure for this algorithm within `numba`.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import numba
from typing import Any, List
import itertools

@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    out_degree: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using a highly optimized single-pass power iteration,
    accelerated with Numba. This version processes dangling and non-dangling nodes
    in a single loop for improved efficiency.
    &quot;&quot;&quot;
    r = np.full(n, 1.0 / n, dtype=np.float64)
    
    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        r_last = r
        
        # Initialize r_new with the base teleportation value for all nodes.
        r_new = np.full(n, teleport_val, dtype=np.float64)
        
        dangling_sum = 0.0

        # Single pass through all nodes to distribute rank from non-dangling
        # nodes and accumulate rank from dangling nodes.
        for i in range(n):
            if out_degree[i] == 0:
                # It&#x27;s a dangling node, accumulate its rank.
                dangling_sum += r_last[i]
            else:
                # It&#x27;s a non-dangling node, distribute its rank to its neighbors.
                start = adj_list_indptr[i]
                end = adj_list_indptr[i + 1]
                rank_to_distribute = alpha * r_last[i] / out_degree[i]
                for k in range(start, end):
                    j = adj_list_indices[k]
                    r_new[j] += rank_to_distribute
        
        # Add the total dangling rank contribution to all nodes in one vectorized operation.
        r_new += alpha * dangling_sum / n
        
        r = r_new

        # Check for convergence using a vectorized L1 norm.
        if np.abs(r - r_last).sum() &lt; n * tol:
            break
            
    return r


class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    single-pass power iteration method for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Optimized conversion of adjacency list to CSR-like format
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)

        # Efficiently create indptr using cumsum
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])

        # Efficiently create indices by flattening the adjacency list
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, out_degree, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 11 messages and have used up $0.7586. You have $0.2414 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 104)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: import numpy as np
&gt; 002: import numba
&gt; 003: from typing import Any, List
&gt; 004: import itertools
&gt; 005: 
&gt; 006: @numba.njit(cache=True, fastmath=True)
&gt; 007: def _pagerank_numba(
&gt; 008:     adj_list_indices: np.ndarray,
&gt; 009:     adj_list_indptr: np.ndarray,
&gt; 010:     out_degree: np.ndarray,
&gt; 011:     n: int,
&gt; 012:     alpha: float,
&gt; 013:     max_iter: int,
&gt; 014:     tol: float,
&gt; 015: ) -&gt; np.ndarray:
&gt; 016:     &quot;&quot;&quot;
&gt; 017:     Calculates PageRank scores using a highly optimized single-pass power iteration,
&gt; 018:     accelerated with Numba. This version processes dangling and non-dangling nodes
&gt; 019:     in a single loop for improved efficiency.
&gt; 020:     &quot;&quot;&quot;
&gt; 021:     r = np.full(n, 1.0 / n, dtype=np.float64)
&gt; 022:     
&gt; 023:     teleport_val = (1.0 - alpha) / n
&gt; 024: 
&gt; 025:     for _ in range(max_iter):
&gt; 026:         r_last = r
&gt; 027:         
&gt; 028:         # Initialize r_new with the base teleportation value for all nodes.
&gt; 029:         r_new = np.full(n, teleport_val, dtype=np.float64)
&gt; 030:         
&gt; 031:         dangling_sum = 0.0
&gt; 032: 
&gt; 033:         # Single pass through all nodes to distribute rank from non-dangling
&gt; 034:         # nodes and accumulate rank from dangling nodes.
&gt; 035:         for i in range(n):
&gt; 036:             if out_degree[i] == 0:
&gt; 037:                 # It&#x27;s a dangling node, accumulate its rank.
&gt; 038:                 dangling_sum += r_last[i]
&gt; 039:             else:
&gt; 040:                 # It&#x27;s a non-dangling node, distribute its rank to its neighbors.
&gt; 041:                 start = adj_list_indptr[i]
&gt; 042:                 end = adj_list_indptr[i + 1]
&gt; 043:                 rank_to_distribute = alpha * r_last[i] / out_degree[i]
&gt; 044:                 for k in range(start, end):
&gt; 045:                     j = adj_list_indices[k]
&gt; 046:                     r_new[j] += rank_to_distribute
&gt; 047:         
&gt; 048:         # Add the total dangling rank contribution to all nodes in one vectorized operation.
&gt; 049:         r_new += alpha * dangling_sum / n
&gt; 050:         
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">31.13</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My `numba` solution has plateaued at a 31.13x speedup. Despite refactoring the inner loops, I haven&#x27;t been able to improve performance further. This indicates I&#x27;ve likely reached the limit of what a JIT-compiled Python implementation of the power method can achieve on this platform.<br><br>My previous attempt to use `scipy.sparse` was flawed and slow. However, `scipy`&#x27;s sparse matrix operations, which are wrappers around highly optimized and often multi-threaded C/Fortran BLAS libraries (like MKL or OpenBLAS), represent the most promising path to a higher speedup.<br><br>My new strategy is a corrected and highly optimized `scipy`-based approach:<br><br>1.  **Efficient Matrix Construction**: I will construct the transition matrix `G` (which is the transpose of the conventional `P` matrix, `G = P.T`) directly, avoiding any in-place modifications or transpositions within the loop. The `data` array for the `csr_matrix` will be built using `numpy.repeat` on the inverse out-degrees, which is a fast, vectorized operation.<br>2.  **BLAS-Powered Iteration**: The core of the power iteration, the matrix-vector product `G @ r`, will be handled entirely by `scipy`&#x27;s optimized routine.<br>3.  **Vectorized Dangling/Teleportation Logic**: The contributions from dangling nodes and the teleportation factor will be calculated using `numpy`&#x27;s fast, vectorized operations and added to the result of the matrix-vector product. The entire update rule is performed with a few high-level operations, minimizing Python interpreter overhead.<br><br>This approach replaces the `numba`-jitted loop with a Python loop that orchestrates calls to `scipy` and `numpy`&#x27;s underlying compiled libraries. For large matrix operations, this should be significantly faster than my previous `numba` code.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
from scipy.sparse import csr_matrix
from typing import Any, List
import itertools

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses SciPy&#x27;s sparse matrix
    operations for a highly efficient power iteration.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores using a power iteration method accelerated
        by SciPy&#x27;s sparse matrix-vector multiplication.

        The method involves:
        1. Efficiently constructing a sparse matrix G (the transpose of the
           standard transition matrix P).
        2. Performing the power iteration using the fast G @ r_last operation,
           which leverages optimized BLAS libraries.
        3. Handling dangling nodes and teleportation vectorially with NumPy.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Step 1: Efficiently prepare data for CSR matrix construction.
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
        
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])
        
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Step 2: Construct the transition matrix G = P.T.
        # The value for each edge from i to j is 1/out_degree[i].
        # We use np.repeat to create the &#x27;data&#x27; array efficiently.
        non_dangling_mask = out_degree &gt; 0
        inv_out_degree = np.zeros(n, dtype=np.float64)
        inv_out_degree[non_dangling_mask] = 1.0 / out_degree[non_dangling_mask]
        data = np.repeat(inv_out_degree, out_degree)
        
        G = csr_matrix((data, indices, indptr), shape=(n, n))
        
        # Step 3: Perform the power iteration.
        r = np.full(n, 1.0 / n, dtype=np.float64)
        
        dangling_nodes_indices = np.where(out_degree == 0)[0]
        
        teleport_val = (1.0 - alpha) / n

        for _ in range(max_iter):
            r_last = r
            
            # The core matrix-vector product, handled by SciPy&#x27;s optimized code.
            r_from_links = G @ r_last
            
            # Calculate contribution from dangling nodes.
            dangling_sum = np.sum(r_last[dangling_nodes_indices])
            
            # Combine all parts of the PageRank update rule in a single vectorized expression.
            r = alpha * r_from_links + alpha * (dangling_sum / n) + teleport_val
            
            # Check for convergence using L1 norm.
            if np.linalg.norm(r - r_last, ord=1) &lt; n * tol:
                break
        
        return {&quot;pagerank_scores&quot;: r.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 12 messages and have used up $0.8429. You have $0.1571 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 85)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: import numpy as np
&gt; 02: from scipy.sparse import csr_matrix
&gt; 03: from typing import Any, List
&gt; 04: import itertools
&gt; 05: 
&gt; 06: class Solver:
&gt; 07:     &quot;&quot;&quot;
&gt; 08:     A solver for the PageRank problem that uses SciPy&#x27;s sparse matrix
&gt; 09:     operations for a highly efficient power iteration.
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     def __init__(self):
&gt; 12:         &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
&gt; 13:         self.alpha = 0.85
&gt; 14:         self.max_iter = 100
&gt; 15:         self.tol = 1.0e-6
&gt; 16: 
&gt; 17:     def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
&gt; 18:         &quot;&quot;&quot;
&gt; 19:         Calculates PageRank scores using a power iteration method accelerated
&gt; 20:         by SciPy&#x27;s sparse matrix-vector multiplication.
&gt; 21: 
&gt; 22:         The method involves:
&gt; 23:         1. Efficiently constructing a sparse matrix G (the transpose of the
&gt; 24:            standard transition matrix P).
&gt; 25:         2. Performing the power iteration using the fast G @ r_last operation,
&gt; 26:            which leverages optimized BLAS libraries.
&gt; 27:         3. Handling dangling nodes and teleportation vectorially with NumPy.
&gt; 28:         &quot;&quot;&quot;
&gt; 29:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 30:         n = len(adj_list)
&gt; 31: 
&gt; 32:         if n == 0:
&gt; 33:             return {&quot;pagerank_scores&quot;: []}
&gt; 34: 
&gt; 35:         # Override parameters if provided
&gt; 36:         alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
&gt; 37:         max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
&gt; 38:         tol = kwargs.get(&quot;tol&quot;, self.tol)
&gt; 39: 
&gt; 40:         # Step 1: Efficiently prepare data for CSR matrix construction.
&gt; 41:         out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
&gt; 42:         
&gt; 43:         indptr = np.empty(n + 1, dtype=np.int32)
&gt; 44:         indptr[0] = 0
&gt; 45:         np.cumsum(out_degree, out=indptr[1:])
&gt; 46:         
&gt; 47:         num_edges = indptr[-1]
&gt; 48:         indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
&gt; 49:                               dtype=np.int32, 
&gt; 50:                               count=num_edges)
...</code></pre><br><br>Speedup: <span style="color:#B22222; font-weight:600;">N/A</span><br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 0%<br>  Invalid Solutions: 100%<br>  Timeouts: 0%<br><br><br>Snapshot not saved - invalid solutions present<br><br>Invalid Example #1:<br><pre><code class="language-python">  181:             problem: The problem definition dictionary.
  182:             solution: The proposed solution dictionary.
  183: 
  184:         Returns:
  185:             True if the solution is valid and numerically close to reference, False otherwise.
  186:         &quot;&quot;&quot;
  187:         if &quot;adjacency_list&quot; not in problem:
  188:             logging.error(&quot;Problem dictionary missing &#x27;adjacency_list&#x27;.&quot;)
  189:             return False
  190:         adj_list = problem[&quot;adjacency_list&quot;]
  191:         n = len(adj_list)
  192: 
  193:         # --- Structural and Type Checks ---
  194:         if not isinstance(solution, dict) or &quot;pagerank_scores&quot; not in solution:
  195:             logging.error(&quot;Solution format invalid: not a dict or missing &#x27;pagerank_scores&#x27; key.&quot;)
&gt; 196:             return False</code></pre><br><br>Invalid Example #2:<br><pre><code class="language-python">  181:             problem: The problem definition dictionary.
  182:             solution: The proposed solution dictionary.
  183: 
  184:         Returns:
  185:             True if the solution is valid and numerically close to reference, False otherwise.
  186:         &quot;&quot;&quot;
  187:         if &quot;adjacency_list&quot; not in problem:
  188:             logging.error(&quot;Problem dictionary missing &#x27;adjacency_list&#x27;.&quot;)
  189:             return False
  190:         adj_list = problem[&quot;adjacency_list&quot;]
  191:         n = len(adj_list)
  192: 
  193:         # --- Structural and Type Checks ---
  194:         if not isinstance(solution, dict) or &quot;pagerank_scores&quot; not in solution:
  195:             logging.error(&quot;Solution format invalid: not a dict or missing &#x27;pagerank_scores&#x27; key.&quot;)
&gt; 196:             return False</code></pre><br><br>Invalid Example #3:<br><pre><code class="language-python">  181:             problem: The problem definition dictionary.
  182:             solution: The proposed solution dictionary.
  183: 
  184:         Returns:
  185:             True if the solution is valid and numerically close to reference, False otherwise.
  186:         &quot;&quot;&quot;
  187:         if &quot;adjacency_list&quot; not in problem:
  188:             logging.error(&quot;Problem dictionary missing &#x27;adjacency_list&#x27;.&quot;)
  189:             return False
  190:         adj_list = problem[&quot;adjacency_list&quot;]
  191:         n = len(adj_list)
  192: 
  193:         # --- Structural and Type Checks ---
  194:         if not isinstance(solution, dict) or &quot;pagerank_scores&quot; not in solution:
  195:             logging.error(&quot;Solution format invalid: not a dict or missing &#x27;pagerank_scores&#x27; key.&quot;)
&gt; 196:             return False</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The previous `scipy.sparse` attempt failed validation. The error message `Solution format invalid` indicates that my `solve` function did not return a dictionary with the key `&quot;pagerank_scores&quot;`. This typically happens if an unhandled exception occurs during execution.<br><br>Upon review, I&#x27;ve identified a critical mathematical error in my previous implementation:<br>1.  I constructed the transition matrix `P`, where `P[i, j]` represents a link from `i` to `j`.<br>2.  I then performed the matrix-vector product `P @ r`.<br>3.  The correct PageRank update requires the product with the *transpose* of `P`, which is `P.T @ r` (or equivalently, `r @ P`).<br><br>My new strategy corrects this fundamental flaw by building the `P.T` matrix directly and efficiently. This will produce the correct numerical results and resolve the exception that was likely causing the format error.<br><br>The plan is as follows:<br>1.  **Direct `P.T` Construction**: I will use the `csr_matrix((data, (row, col)))` constructor, which is ideal for this task.<br>    *   The `row` indices will be the `targets` of each edge.<br>    *   The `col` indices will be the `sources` of each edge.<br>    *   The `data` for an edge `i -&gt; j` will be `1 / out_degree[i]`.<br>    *   This directly builds `G = P.T`, where `G[j, i] = 1 / out_degree[i]`.<br>2.  **Optimized Data Preparation**: I will use `numpy.repeat` to generate the `sources` array and `itertools.chain.from_iterable` for the `targets` array. The `data` array will be created with a single vectorized operation: `1.0 / out_degree[sources]`.<br>3.  **Correct Power Iteration**: The main loop will now perform the correct matrix-vector product `G @ r`, which is `P.T @ r`. The rest of the vectorized logic for handling dangling nodes and teleportation remains the same.<br><br>This corrected `scipy`-based approach should be both numerically correct and significantly faster than my previous `numba` solutions by leveraging highly optimized BLAS libraries for the core computation.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
from scipy.sparse import csr_matrix
from typing import Any, List
import itertools

class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses SciPy&#x27;s sparse matrix
    operations for a highly efficient power iteration.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores using a power iteration method accelerated
        by SciPy&#x27;s sparse matrix-vector multiplication.

        The method involves:
        1. Efficiently constructing a sparse matrix G = P.T, where P is the
           standard transition matrix.
        2. Performing the power iteration using the fast G @ r_last operation,
           which leverages optimized BLAS libraries.
        3. Handling dangling nodes and teleportation vectorially with NumPy.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Step 1: Efficiently prepare data for sparse matrix construction.
        out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
        
        num_edges = out_degree.sum()
        
        # Create source and target arrays for all edges.
        # sources[k] -&gt; targets[k] is the k-th edge.
        sources = np.repeat(np.arange(n, dtype=np.int32), out_degree)
        targets = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Step 2: Construct the transition matrix G = P.T directly.
        # The value for an edge i-&gt;j in P is 1/out_degree[i].
        # So, the value for that edge in G (P.T) at position (j, i) is 1/out_degree[i].
        # We can safely divide as out_degree[sources] will not contain zeros.
        data = 1.0 / out_degree[sources]
        
        # G[j, i] = value, which correctly represents P.T
        G = csr_matrix((data, (targets, sources)), shape=(n, n))
        
        # Step 3: Perform the power iteration.
        r = np.full(n, 1.0 / n, dtype=np.float64)
        
        dangling_nodes_mask = (out_degree == 0)
        
        teleport_val = (1.0 - alpha) / n

        for _ in range(max_iter):
            r_last = r
            
            # The core matrix-vector product: P.T @ r_last
            # G only contains links from non-dangling nodes.
            r_from_links = G @ r_last
            
            # Calculate contribution from dangling nodes.
            dangling_sum = np.sum(r_last[dangling_nodes_mask])
            
            # Combine all parts of the PageRank update rule in a single vectorized expression.
            r = alpha * r_from_links + alpha * (dangling_sum / n) + teleport_val
            
            # Check for convergence using L1 norm.
            if np.linalg.norm(r - r_last, ord=1) &lt; n * tol:
                break
        
        return {&quot;pagerank_scores&quot;: r.tolist()}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 13 messages and have used up $0.9387. You have $0.0613 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 85)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: import numpy as np
&gt; 02: from scipy.sparse import csr_matrix
&gt; 03: from typing import Any, List
&gt; 04: import itertools
&gt; 05: 
&gt; 06: class Solver:
&gt; 07:     &quot;&quot;&quot;
&gt; 08:     A solver for the PageRank problem that uses SciPy&#x27;s sparse matrix
&gt; 09:     operations for a highly efficient power iteration.
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     def __init__(self):
&gt; 12:         &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
&gt; 13:         self.alpha = 0.85
&gt; 14:         self.max_iter = 100
&gt; 15:         self.tol = 1.0e-6
&gt; 16: 
&gt; 17:     def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
&gt; 18:         &quot;&quot;&quot;
&gt; 19:         Calculates PageRank scores using a power iteration method accelerated
&gt; 20:         by SciPy&#x27;s sparse matrix-vector multiplication.
&gt; 21: 
&gt; 22:         The method involves:
&gt; 23:         1. Efficiently constructing a sparse matrix G = P.T, where P is the
&gt; 24:            standard transition matrix.
&gt; 25:         2. Performing the power iteration using the fast G @ r_last operation,
&gt; 26:            which leverages optimized BLAS libraries.
&gt; 27:         3. Handling dangling nodes and teleportation vectorially with NumPy.
&gt; 28:         &quot;&quot;&quot;
&gt; 29:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 30:         n = len(adj_list)
&gt; 31: 
&gt; 32:         if n == 0:
&gt; 33:             return {&quot;pagerank_scores&quot;: []}
&gt; 34: 
&gt; 35:         # Override parameters if provided
&gt; 36:         alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
&gt; 37:         max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
&gt; 38:         tol = kwargs.get(&quot;tol&quot;, self.tol)
&gt; 39: 
&gt; 40:         # Step 1: Efficiently prepare data for sparse matrix construction.
&gt; 41:         out_degree = np.array([len(neighbors) for neighbors in adj_list], dtype=np.int32)
&gt; 42:         
&gt; 43:         num_edges = out_degree.sum()
&gt; 44:         
&gt; 45:         # Create source and target arrays for all edges.
&gt; 46:         # sources[k] -&gt; targets[k] is the k-th edge.
&gt; 47:         sources = np.repeat(np.arange(n, dtype=np.int32), out_degree)
&gt; 48:         targets = np.fromiter(itertools.chain.from_iterable(adj_list), 
&gt; 49:                               dtype=np.int32, 
&gt; 50:                               count=num_edges)
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">23.08</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge revert">↩️ Revert Changes</span>
            </div>
            <div class="message-content">
                My `scipy.sparse` solution was slower than my best `numba` implementation, and my attempt to refine the `numba` logic by switching to a single-pass loop did not yield any speedup. The performance has plateaued at 31.13x. To break past this, I need to apply finer micro-optimizations to the best-performing structure I&#x27;ve found: the single-pass `numba` implementation.<br><br>My analysis indicates two areas for improvement that don&#x27;t involve changing the core algorithm but rather how it&#x27;s implemented and prepared:<br><br>1.  **Data Preparation**: The `out_degree` array is currently created with a list comprehension (`[len(neighbors) for neighbors in adj_list]`). While readable, this is not the fastest way in Python for this task. Switching to `np.fromiter(map(len, adj_list), ...)` is a known pattern for speeding up the creation of a NumPy array from an iterable of numbers, reducing Python-level overhead before the `numba` function is even called.<br><br>2.  **Convergence Check**: Inside the tight `numba` loop, the convergence is checked with `np.abs(r - r_last).sum()`. This NumPy call, even when JIT-compiled, can be suboptimal as it may create intermediate temporary arrays for the result of `r - r_last` and its absolute value. A custom, manually-fused loop to calculate the L1 difference will be more efficient by avoiding these allocations and improving data locality.<br><br>My plan is to revert to my best `numba` solution and then apply these two specific micro-optimizations. This strategy focuses on refining the implementation details of the fastest approach identified so far, which is the most logical path to squeezing out additional performance.<br><br>First, I will revert to the last saved state.<br><pre><code class="language-python">revert</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 14 messages and have used up $0.9995. You have $0.0005 remaining.<br><br>Successfully reverted to last saved state.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My `numba` and `scipy` solutions have hit a performance wall at 31.13x. To surpass this, I must return to the most promising architecture—the single-pass `numba` implementation—and apply a series of targeted micro-optimizations that I had previously overlooked. The core algorithm is sound, but its implementation can be made more efficient.<br><br>My final strategy combines the algorithmically superior single-pass loop with several low-level implementation improvements:<br><br>1.  **Optimized Data Preparation**: In the `solve` method, I will replace the list comprehension for calculating `out_degree` with the faster `np.fromiter(map(len, adj_list), ...)` pattern. This reduces Python-level overhead before the main computation begins.<br>2.  **Efficient Array Handling**: Inside the `_pagerank_numba` function, I will pre-allocate the rank vectors and use `np.copyto` to manage the state between iterations. This avoids potential aliasing bugs and the overhead of re-allocating arrays within the main loop.<br>3.  **Fused Convergence Check**: I will replace the `np.abs(r - r_last).sum()` convergence check with a manual, fused loop. This is a critical optimization within `numba`, as it prevents the creation of intermediate temporary arrays and improves data locality, leading to a faster L1 norm calculation.<br><br>These changes, while small individually, collectively target the remaining inefficiencies in the fastest solution I&#x27;ve developed. By refining the implementation details of the best-performing algorithm, I expect to finally break the performance plateau.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-150
---
import numpy as np
import numba
from typing import Any, List
import itertools

@numba.njit(cache=True, fastmath=True)
def _pagerank_numba(
    adj_list_indices: np.ndarray,
    adj_list_indptr: np.ndarray,
    out_degree: np.ndarray,
    n: int,
    alpha: float,
    max_iter: int,
    tol: float,
) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Calculates PageRank scores using a highly optimized single-pass power iteration,
    accelerated with Numba. This version includes micro-optimizations for array
    handling and convergence checking.
    &quot;&quot;&quot;
    # Initialize arrays. r is the current rank vector, r_last is the previous.
    r = np.full(n, 1.0 / n, dtype=np.float64)
    r_last = np.empty(n, dtype=np.float64)
    
    teleport_val = (1.0 - alpha) / n

    for _ in range(max_iter):
        # Efficiently copy the current rank vector to r_last for the new iteration.
        np.copyto(r_last, r)
        
        # Initialize the new rank vector &#x27;r&#x27; with the base teleportation value.
        r.fill(teleport_val)
        
        dangling_sum = 0.0

        # Single pass through all nodes to distribute rank from non-dangling
        # nodes and accumulate rank from dangling nodes.
        for i in range(n):
            if out_degree[i] == 0:
                # It&#x27;s a dangling node, accumulate its rank from the previous iteration.
                dangling_sum += r_last[i]
            else:
                # It&#x27;s a non-dangling node, distribute its rank to its neighbors.
                start = adj_list_indptr[i]
                end = adj_list_indptr[i + 1]
                rank_to_distribute = alpha * r_last[i] / out_degree[i]
                for k in range(start, end):
                    j = adj_list_indices[k]
                    r[j] += rank_to_distribute
        
        # Add the total dangling rank contribution to all nodes in one vectorized operation.
        r += alpha * dangling_sum / n
        
        # Manually fused convergence check (L1 norm) to avoid temporary arrays.
        diff = 0.0
        for i in range(n):
            diff += abs(r[i] - r_last[i])
        
        if diff &lt; n * tol:
            break
            
    return r


class Solver:
    &quot;&quot;&quot;
    A solver for the PageRank problem that uses a Numba-accelerated
    single-pass power iteration method with micro-optimizations for high performance.
    &quot;&quot;&quot;
    def __init__(self):
        &quot;&quot;&quot;Initializes the solver with default PageRank parameters.&quot;&quot;&quot;
        self.alpha = 0.85
        self.max_iter = 100
        self.tol = 1.0e-6

    def solve(self, problem: dict[str, List[List[int]]], **kwargs) -&gt; dict[str, Any]:
        &quot;&quot;&quot;
        Calculates PageRank scores for a given graph.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n == 0:
            return {&quot;pagerank_scores&quot;: []}

        # Override parameters if provided in kwargs
        alpha = kwargs.get(&quot;alpha&quot;, self.alpha)
        max_iter = kwargs.get(&quot;max_iter&quot;, self.max_iter)
        tol = kwargs.get(&quot;tol&quot;, self.tol)

        # Optimized out-degree calculation using map and fromiter.
        out_degree = np.fromiter(map(len, adj_list), dtype=np.int32, count=n)

        # Efficiently create indptr using cumsum
        indptr = np.empty(n + 1, dtype=np.int32)
        indptr[0] = 0
        np.cumsum(out_degree, out=indptr[1:])

        # Efficiently create indices by flattening the adjacency list
        num_edges = indptr[-1]
        indices = np.fromiter(itertools.chain.from_iterable(adj_list), 
                              dtype=np.int32, 
                              count=num_edges)

        # Call the Numba-jitted function to perform the calculation
        pagerank_scores_np = _pagerank_numba(
            indices, indptr, out_degree, n, alpha, max_iter, tol
        )

        return {&quot;pagerank_scores&quot;: pagerank_scores_np.tolist()}
---</code></pre>
            </div>
        </div>
            </div>
        </div>
    </div>
</body>
</html>