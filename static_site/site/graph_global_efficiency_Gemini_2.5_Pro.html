<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>AlgoTuner Log – graph_global_efficiency – Gemini 2.5 Pro</title>
    <link rel="icon" type="image/png" href="assets/AlgoTunerMascot.png">
    
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7XSBWH5NQF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-7XSBWH5NQF');
    </script>
    
    <!-- Prism.js for syntax highlighting - loaded after styles.css to ensure proper precedence -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <link rel="stylesheet" href="styles.css">"
    
    <!-- Basic Styling & Layout -->
    <style>
    /* Basic Styling & Layout */
    :root {
        --primary-color: #2196F3;
        --primary-light: #E3F2FD;
        --text-color: #333;
        --border-color: #eaeaea;
        --content-bg: #ffffff;
        --error-border: #dc3545;
        --code-bg: #f6f8fa;
        --code-border: #d0d7de;
        --code-text: #24292e;
        
        /* Glass-morphism variables for light mode */
        --glass-bg: rgba(255, 255, 255, 0.12);
        --glass-border: rgba(255, 255, 255, 0.05);
        --glass-header-bg: rgba(0, 0, 0, 0.03);
        --glass-header-border: rgba(255, 255, 255, 0.08);
        --glass-btn-bg: rgba(255, 255, 255, 0.1);
        --glass-btn-border: rgba(255, 255, 255, 0.2);
        --glass-btn-hover: rgba(255, 255, 255, 0.2);
        --glass-expand-bg: linear-gradient(to top, rgba(255, 255, 255, 0.95), rgba(255, 255, 255, 0.8));
        --glass-text: rgba(0, 0, 0, 0.8);
        --glass-text-secondary: rgba(0, 0, 0, 0.6);
    }
    
    /* Dark mode support */
    @media (prefers-color-scheme: dark) {
        :root {
            --glass-bg: rgba(0, 0, 0, 0.15);
            --glass-border: rgba(255, 255, 255, 0.08);
            --glass-header-bg: rgba(255, 255, 255, 0.05);
            --glass-header-border: rgba(255, 255, 255, 0.12);
            --glass-btn-bg: rgba(255, 255, 255, 0.08);
            --glass-btn-border: rgba(255, 255, 255, 0.15);
            --glass-btn-hover: rgba(255, 255, 255, 0.15);
            --glass-expand-bg: linear-gradient(to top, rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.7));
            --glass-text: rgba(255, 255, 255, 0.9);
            --glass-text-secondary: rgba(255, 255, 255, 0.7);
        }
    }
    
    body {
        margin: 0;
        padding: 0;
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        color: var(--text-color);
        line-height: 1.5;
        background: var(--content-bg);
        display: flex;
        min-height: 100vh;
    }
    
    /* Sidebar - Desktop only, hidden by default on mobile */
    .sidebar {
        /* Slightly narrower sidebar to give more room to main content */
        width: 180px;
        background: #f8f9fa;
        border-right: 1px solid var(--border-color);
        position: fixed;
        left: 0;
        top: 0;
        height: 100vh;
        overflow-y: auto;
        z-index: 1000;
        display: none; /* Hidden by default */
    }
    
    /* Show sidebar only on large screens (1025px and up) */
    @media (min-width: 1025px) {
        .sidebar {
            display: block;
        }
    }
    
    /* --------------------------- */
    /* Sidebar header & back link  */
    /* --------------------------- */
    .sidebar-header {
        padding: 1.25rem;
        display: flex;
        justify-content: center;  /* Centre the back link horizontally */
    }

    .back-link {
        display: inline-flex;
        align-items: center;
        gap: 0.4rem;               /* Space between arrow and label */
        padding: 0.45rem 0.9rem;
        background: #2196F3;       /* Primary blue */
        border-radius: 8px;
        color: #ffffff;            /* White text */
        font-weight: 600;
        font-size: 0.9rem;
        text-decoration: none;
        transition: background 0.2s ease, box-shadow 0.2s ease;
    }

    .back-link:hover {
        background: #1976D2;       /* Darker blue on hover */
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.15);
        text-decoration: none; 
        color: #ffffff;
    }
    
    .sidebar-content {
        padding: 1rem;
    }
    
    .sidebar-content h3 {
        margin: 0 0 1rem 0;
        font-size: 1rem;
        color: var(--text-color);
        text-align: left;
        /* Allow long task names with underscores to wrap onto multiple lines */
        white-space: normal;
        word-wrap: break-word;
        overflow-wrap: anywhere;
        line-height: 1.3;
    }
    
    .sidebar-task {
        margin-bottom: 1.5rem;
    }
    
    .task-name {
        font-weight: 600;
        font-size: 0.85rem;
        color: var(--primary-color);
        margin-bottom: 0.5rem;
        padding: 0.25rem 0;
        border-bottom: 1px solid #e0e0e0;
        /* Allow very long task names to wrap instead of overflowing */
        white-space: normal;
        word-wrap: break-word;
        overflow-wrap: anywhere;
        line-height: 1.3;
    }
    
    .sidebar-run {
        margin-bottom: 8px;
    }
    
    /* Make sidebar run links occupy full width */
    .sidebar-run a {
        display: block;
        width: 100%;
        text-decoration: none;
    }
    
    .sidebar-run a:hover, .sidebar-run a:focus, .sidebar-run a:visited {
        text-decoration: none;
    }
    
    /* Ensure the coloured badge stretches the whole column */
    .run-score {
        width: 100%;
    }
    
    /* Thicker highlight for the currently selected run on desktop */
    @media (min-width: 769px) {
        .sidebar-run.current a {
            border-left: 5px solid #2196F3 !important;
        }
    }
    
    .main-content {
        flex: 1;
        margin-left: 180px;
        padding: 0;
        max-width: calc(100vw - 180px);
    }
    
    .container {
        /* Allow the main conversation area to take up the full width that is
           available once the fixed sidebar is accounted for. */
        max-width: 100%;
        margin: 0 auto;
        padding: 0 15px;
    }
    
    h1 {
        color: var(--primary-color);
        border-bottom: 2px solid var(--primary-light);
        padding-bottom: 0.5rem;
    }
    
    .info-section {
        background: var(--primary-light);
        padding: 1rem;
        border-radius: 6px;
        margin-bottom: 1.5rem;
        display: flex;
        flex-wrap: nowrap;
        gap: 1rem;
        overflow-x: auto;
        white-space: nowrap;
    }
    
    .info-item {
        display: flex;
        flex-direction: column;
        flex-shrink: 0;
        min-width: 140px;
    }
    
    .info-label {
        font-weight: 600;
        color: var(--primary-color);
        font-size: 0.85rem;
        margin-bottom: 0.25rem;
        white-space: nowrap;
    }
    
    .info-value {
        font-size: 0.9rem;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }
    
    .task-info-line, .model-info-line {
        font-size: 1.3rem;
        word-wrap: break-word;
        overflow-wrap: break-word;
        text-align: left !important;
        display: block;
    }
    
    .task-name-display {
        font-weight: 600;
        font-size: clamp(0.9rem, 4vw, 1.3rem);
        word-wrap: break-word;
        overflow-wrap: break-word;
        line-height: 1.2;
    }
    
    .plots-container {
        display: flex;
        gap: 1rem;
        margin: 1.5rem 0;
    }
    
    .plot-section {
        margin: 0;
    }
    
    .plot-half {
        flex: 1;
        width: 50%;
    }
    
    .plot-section h3 {
        margin-bottom: 0.8rem;
        color: var(--text-color);
    }
    
    .plot-container {
        text-align: center;
        background: #ffffff;
        border: 1px solid var(--border-color);
        border-radius: 6px;
        padding: 1rem;
        margin-bottom: 1rem;
    }
    
    .plot-container img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 0 auto;
    }
    
    .no-plot {
        color: #666;
        font-style: italic;
        padding: 2rem;
        text-align: center;
    }
    
    .conversation-section {
        margin: 1.5rem 0;
    }
    
    .message {
        margin-bottom: 1rem;
        padding: 1rem;
        border-radius: 18px;
        /* Give the chat bubbles more breathing room. 90 % looks good on both
           desktop and tablet while still leaving a small margin on the side. */
        max-width: 90%;
        position: relative;
    }
    
    .message.system {
        background: #e5e5ea;
        color: #000;
        margin-left: auto;
        margin-right: 0;
        border-radius: 18px 18px 4px 18px;
    }
    
    .message.assistant {
        background: #007aff;
        color: white;
        margin-left: 0;
        margin-right: auto;
        border-radius: 18px 18px 18px 4px;
    }
    
    .message-header {
        font-weight: 600;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }
    
    .command-badge {
        background: rgba(0, 0, 0, 0.2);
        color: inherit;
        padding: 0.2rem 0.5rem;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: 500;
        opacity: 0.8;
    }
    
    .message.system .command-badge {
        background: rgba(0, 0, 0, 0.15);
        color: #000;
    }
    
    .message.assistant .command-badge {
        background: rgba(255, 255, 255, 0.25);
        color: white;
    }
    
    /* Premium Glass-Morphism Code Block Container */
    .code-block {
        position: relative;
        margin: clamp(1.5rem, 2vw, 2rem) 0;
        border-radius: clamp(12px, 3vw, 20px);
        padding: 0;
        overflow: hidden;
        max-width: 100%;
        box-sizing: border-box;
        
        /* Glass-morphism backdrop effects */
        background: var(--code-bg);
        backdrop-filter: blur(16px);
        -webkit-backdrop-filter: blur(16px);
        
        /* Multi-layer shadows for depth */
        box-shadow: 
            0 8px 32px rgba(0, 0, 0, 0.12),
            0 2px 8px rgba(0, 0, 0, 0.08),
            inset 0 1px 0 rgba(255, 255, 255, 0.15),
            0 0 0 1px var(--code-border);
        
        /* Smooth animations */
        transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    }
    
    .code-block:hover {
        transform: translateY(-2px);
        box-shadow: 
            0 12px 40px rgba(0, 0, 0, 0.16),
            0 4px 12px rgba(0, 0, 0, 0.12),
            inset 0 1px 0 rgba(255, 255, 255, 0.2),
            0 0 0 1px rgba(255, 255, 255, 0.08);
    }
    
    /* Code Block Header with Copy Button */
    .code-block-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: clamp(0.75rem, 2vw, 1rem) clamp(1rem, 3vw, 1.5rem);
        background: var(--glass-header-bg);
        border-bottom: 1px solid var(--glass-header-border);
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
    }
    
    .code-language-tag {
        font-size: clamp(0.7rem, 1.5vw, 0.75rem);
        font-weight: 600;
        color: var(--glass-text-secondary);
        text-transform: uppercase;
        letter-spacing: 0.5px;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', system-ui, sans-serif;
    }
    
    .code-copy-btn {
        padding: clamp(0.4rem, 1vw, 0.5rem) clamp(0.6rem, 1.5vw, 0.8rem);
        background: var(--glass-btn-bg);
        border: 1px solid var(--glass-btn-border);
        border-radius: clamp(6px, 1.5vw, 8px);
        color: var(--glass-text-secondary);
        font-size: clamp(0.7rem, 1.5vw, 0.75rem);
        font-weight: 500;
        cursor: pointer;
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
        transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
        touch-action: manipulation;
        user-select: none;
        font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', system-ui, sans-serif;
    }
    
    .code-copy-btn:hover {
        background: var(--glass-btn-hover);
        border-color: var(--glass-btn-border);
        transform: scale(1.02);
    }
    
    .code-copy-btn:active {
        transform: scale(0.98);
    }
    
    .code-copy-btn.copied {
        background: rgba(16, 185, 129, 0.15);
        border-color: rgba(16, 185, 129, 0.3);
        color: #059669;
    }
    
    /* Code Content Container */
    .code-content {
        position: relative;
        overflow: hidden;
    }
    
    /* Code Block Content (pre/code tags) */
    .code-block pre, .code-block code {
        margin: 0;
        padding: 0;
        background: none !important;
        font-family: 'SF Mono', 'Fira Code', 'Menlo', 'Consolas', monospace;
        font-size: clamp(0.8rem, 2vw, 0.85rem);
        line-height: 1.6;
        color: var(--code-text);
        text-shadow: none;
    }
    
    .code-block pre {
        padding: clamp(1rem, 3vw, 1.5rem);
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
    }
    
    /* Expand/Collapse functionality for long code blocks */
    .code-expandable .code-content {
        max-height: 400px; /* Default collapsed height */
        transition: max-height 0.5s cubic-bezier(0.4, 0, 0.2, 1);
    }
    
    .code-expandable .code-content.expanded {
        max-height: 2000px; /* Expanded height */
    }
    
    .code-expand-overlay {
        position: absolute;
        bottom: 0;
        left: 0;
        right: 0;
        height: 100px;
        background: var(--glass-expand-bg);
        display: flex;
        justify-content: center;
        align-items: flex-end;
        padding-bottom: 1rem;
        pointer-events: none;
        opacity: 1;
        transition: opacity 0.3s;
    }
    
    .code-expandable .code-content.expanded + .code-expand-overlay {
        opacity: 0;
    }
    
    .code-expand-btn {
        padding: 0.5rem 1rem;
        background: var(--glass-btn-bg);
        border: 1px solid var(--glass-btn-border);
        border-radius: 8px;
        color: var(--glass-text-secondary);
        font-size: 0.8rem;
        font-weight: 500;
        cursor: pointer;
        pointer-events: all;
        backdrop-filter: blur(8px);
        -webkit-backdrop-filter: blur(8px);
        transition: all 0.2s;
    }
    
    .code-expand-btn:hover {
        background: var(--glass-btn-hover);
    }
    
    .code-expand-btn::after {
        content: 'Show More';
    }
    
    .code-expandable .code-content.expanded + .code-expand-overlay .code-expand-btn::after {
        content: 'Show Less';
    }
    
    /* Collapsible Sections */
    .collapsible-sections {
        margin: 2rem 0;
    }
    
    .collapsible-section {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 1rem;
        overflow: hidden;
    }
    
    .collapsible-section summary {
        padding: 1rem;
        font-weight: 600;
        cursor: pointer;
        background: #f8f9fa;
        display: flex;
        justify-content: space-between;
        align-items: center;
    }
    
    .collapsible-section summary::after {
        content: '▼';
        font-size: 0.8rem;
        transition: transform 0.2s;
    }
    
    .collapsible-section[open] summary::after {
        transform: rotate(180deg);
    }
    
    .collapsible-section .section-content {
        padding: 1rem;
        background: white;
    }
    
    /* Special styling for plot sections to avoid double frames */
    .plot-section-content {
        background: transparent !important;
        padding: 0 !important;
    }
    
    .collapsible-section pre {
        background: var(--code-bg);
        padding: 1rem;
        border-radius: 6px;
        overflow-x: auto;
    }
    
    .best-code, .reference-code {
        max-height: 500px;
        overflow-y: auto;
    }
    
    /* Desktop-specific adjustments for collapsible sections */
    @media (min-width: 769px) {
        .collapsible-section {
            margin-bottom: 0.5rem;  /* Reduced from 1rem */
        }
        
        .collapsible-section summary {
            padding: 0.75rem 1rem;  /* Reduced vertical padding */
            font-size: 0.95rem;  /* Slightly smaller font */
        }
        
        .collapsible-section .section-content {
            padding: 0.75rem 1rem;  /* Reduced padding */
        }
        
        .collapsible-section pre {
            font-size: 0.85rem;  /* Smaller font for code blocks */
            line-height: 1.4;
            padding: 0.75rem;
        }
        
        /* Larger font size for reference and best code on desktop */
        .best-code {
            font-size: 1rem !important;  /* Increase from default */
            line-height: 1.5;
        }
        
        .reference-code {
            font-size: 1rem !important;  /* Increase from default */
            line-height: 1.5;
        }
        
        .collapsible-sections {
            margin: 1.5rem 0;  /* Reduced from 2rem */
        }
    }
    
    /* Floating back button - hidden by default */
    .mobile-back-button {
        display: none;
        position: fixed;
        top: 1rem;
        left: 1rem;
        z-index: 9999;  /* ensure it stays above all content */
        background: #2196F3;
        color: white;
        text-decoration: none;
        border-radius: 8px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
    }
    
    .mobile-back-button:hover {
        background: #1976D2;
        transform: scale(1.05);
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    }
    
    .mobile-back-button:active {
        transform: scale(0.95);
    }
    
    .mobile-back-button svg {
        width: 24px;
        height: 24px;
    }
    
    /* Responsive adjustments for mobile */
    @media (max-width: 768px) {
        /* Show floating back button on mobile */
        .mobile-back-button {
            display: flex;
        }
        
        .main-content {
            margin-left: 0;
            padding: 60px 10px 0 10px;  /* Added top padding to account for floating button */
            max-width: 100vw;
        }
        
        .container {
            padding: 0 5px;
        }
        
        .plots-container {
            flex-direction: column;
        }
        
        .plot-half {
            width: 100%;
        }
        
        /* Keep plots within container on mobile */
        .plot-container {
            overflow: hidden;
            padding: 0.5rem;
        }
        
        .plot-container img {
            max-width: 100%;
            height: auto;
        }
        
        .message {
            max-width: 100%;
        }
        
        .header-section {
            margin-bottom: 0.5rem;
            text-align: left !important;
        }
        
        /* Mobile trajectory page adjustments */
        .task-info-line, .model-info-line {
            display: flex;
            flex-direction: column;
            gap: 4px;
            margin-bottom: 6px;
            text-align: left !important;
            align-items: flex-start !important;
        }
        
        .task-info-line span:first-child,
        .model-info-line span:first-child {
            font-size: 0.9rem;
            font-weight: 500;
        }
        
        .task-name-display,
        .model-name-display {
            font-size: 1.1rem !important;
            font-weight: 600;
        }
        
        .info-value {
            font-size: 1.1rem !important;
            font-weight: 600;
        }
        
        .header-section {
            margin-bottom: 0.5rem !important;
        }
        
        .header-section > div:first-child {
            margin-bottom: 8px !important;
        }
    }
    
    /* Ensure container doesn't center content on desktop */
    @media (min-width: 769px) {
        .container {
            margin: 0 !important;
            text-align: left !important;
        }
        .header-section {
            text-align: left !important;
        }
        .task-info-line, .model-info-line {
            text-align: left !important;
        }
    }

    /* Additional mobile adjustments for very small screens */
    @media (max-width: 480px) {
        .header-section {
            margin-bottom: 0.25rem !important;
        }
        
        .header-section > div:first-child {
            margin-bottom: 6px !important;
        }
        
        .task-info-line, .model-info-line {
            margin-bottom: 4px !important;
        }
        
        .info-section {
            margin-bottom: 0.25rem !important;
        }
    }

    .info-section {
        flex-wrap: wrap;
        justify-content: flex-start;
        margin-bottom: 0.5rem;
    }
    
    .info-item {
        min-width: 120px;
        flex-grow: 1;
    }

    .hide-on-mobile {
        display: flex;
    }

    /* Mobile adjustments */
    @media (max-width: 768px) {
        .hide-on-mobile {
            display: none !important;
        }
        
        /* Reduce gap between collapsible sections on mobile */
        .collapsible-sections {
            margin: 1rem 0;
        }
        
        .collapsible-section {
            margin-bottom: 0.5rem;
        }
        
        .collapsible-section summary {
            padding: 0.75rem;
            font-size: 0.9rem;
        }
        
        .collapsible-section .section-content {
            padding: 0.75rem;
        }
    }
    </style>
    <script>
        function copyCode(button, codeId) {
            const code = document.getElementById(codeId).textContent;
            navigator.clipboard.writeText(code).then(() => {
                button.textContent = 'Copied!';
                button.classList.add('copied');
                setTimeout(() => {
                    button.textContent = 'Copy';
                    button.classList.remove('copied');
                }, 2000);
            });
        }

        function toggleExpand(codeBlockId) {
            const content = document.getElementById('content-' + codeBlockId);
            const overlay = document.getElementById('overlay-' + codeBlockId);
            content.classList.toggle('expanded');
            if (overlay) {
                overlay.style.display = content.classList.contains('expanded') ? 'none' : 'flex';
            }
        }
        
        document.addEventListener('DOMContentLoaded', () => {
            Prism.highlightAll();
        });
    </script>
</head>
<body>
    
        <div class="sidebar">
            <div class="sidebar-header">
                <a href="index.html#speedup-section" class="back-link">← Back to Speedup Table</a>
            </div>
            <div class="sidebar-content">
                <h3 style="color: black;">graph_global_efficiency</h3>
                
            <div class="sidebar-task">
                <div class="task-runs">
                    
                <div class="sidebar-run current">
                    <a href="graph_global_efficiency_Gemini_2.5_Pro.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">Gemini 2.5 Pro (16.61x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="graph_global_efficiency_DeepSeek_R1.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">DeepSeek R1 (15.65x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="graph_global_efficiency_Claude_Opus_4.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">Claude Opus 4 (14.19x)</div>
                    </a>
                </div>
                <div class="sidebar-run">
                    <a href="graph_global_efficiency_o4-mini.html">
                        <div class="run-score" style="background-color: #006400; color: #ffffff; padding: 12px 16px; font-size: 0.95rem; border-radius: 8px; font-weight: 600; text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1); letter-spacing: 0.025em; min-height: 24px; width: 100%; box-sizing: border-box;">o4-mini (12.42x)</div>
                    </a>
                </div>
                </div>
            </div>
            </div>
        </div>
    
    <!-- Floating back button for mobile -->
    <a href="index.html#speedup-section" class="mobile-back-button" aria-label="Back to Speedup Table">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M19 12H5M5 12L12 19M5 12L12 5" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
        </svg>
    </a>
    
    <div class="main-content">
        <div class="container">
            <div class="header-section" style="text-align: left !important; align-items: flex-start !important; justify-content: flex-start !important;">
                <div style="margin-bottom: 20px; display: flex; align-items: center; gap: 10px; justify-content: flex-start; text-align: left;">
                    <img src="assets/AlgoTunerMascot.png" alt="AlgoTune Mascot" style="height: 32px; width: auto;">
                    <span style="font-weight: 700; font-size: 1.5rem;">AlgoTuner Trajectory</span>
                </div>
                <div class="task-info-line" style="text-align: left !important; margin-bottom: 8px; display: block;">
                    <span style="color: #6c757d; font-weight: 400;">AlgoTune Task:</span>
                    <span class="task-name-display">graph_global_efficiency</span>
                </div>
                <div class="model-info-line" style="text-align: left !important; display: block;">
                    <span style="color: #6c757d; font-weight: 400;">Model:</span>
                    <span class="model-name-display" style="font-weight: 500;">Gemini 2.5 Pro</span>
                </div>
            </div>
            
            <div class="info-section">
                <div class="info-item">
                    <div class="info-label">Speedup</div>
                    <div class="info-value" style="color: #006400; font-weight: 600;">16.608x</div>
                </div>
                <div class="info-item">
                    <div class="info-label">Total Budget</div>
                    <div class="info-value">$1.00</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Commands Executed</div>
                    <div class="info-value">19</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Invalid Commands</div>
                    <div class="info-value">1</div>
                </div>
                <div class="info-item hide-on-mobile">
                    <div class="info-label">Average Reference Time (ms)</div>
                    <div class="info-value">101.8 ms</div>
                </div>
            </div>
            
            
    <div class="collapsible-sections">
        <details class="collapsible-section">
            <summary>Initial System Prompt</summary>
            <div class="section-content">
                <pre>SETTING:
You&#x27;re an autonomous programmer tasked with solving a specific problem. You are to use the commands defined below to accomplish this task. Every message you send incurs a cost—you will be informed of your usage and remaining budget by the system.
You will be evaluated based on the best-performing piece of code you produce, even if the final code doesn&#x27;t work or compile (as long as it worked at some point and achieved a score, you will be eligible).
Apart from the default Python packages, you have access to the following additional packages:
 - cryptography
 - cvxpy
 - cython
 - dace
 - dask
 - diffrax
 - ecos
 - faiss-cpu
 - hdbscan
 - highspy
 - jax
 - networkx
 - numba
 - numpy
 - ortools
 - pandas
 - pot
 - psutil
 - pulp
 - pyomo
 - python-sat
 - pythran
 - scikit-learn
 - scipy
 - sympy
 - torch

YOUR TASK:
Your objective is to define a class named `Solver` in `solver.py` with a method:
```
class Solver:
    def solve(self, problem, **kwargs) -&gt; Any:
        &quot;&quot;&quot;Your implementation goes here.&quot;&quot;&quot;
        ...
```

IMPORTANT: Compilation time of your init function will not count towards your function&#x27;s runtime.

This `solve` function will be the entrypoint called by the evaluation harness. Strive to align your class and method implementation as closely as possible with the desired performance criteria.
For each instance, your function can run for at most 10x the reference runtime for that instance. Strive to have your implementation run as fast as possible, while returning the same output as the reference function (for the same given input). Be creative and optimize your approach!

Your messages should include a short thought about what you should do, followed by a _SINGLE_ command. The command must be enclosed within ``` and ```, like so:
&lt;Reasoning behind executing the command&gt;
```
&lt;command&gt;
```

IMPORTANT: Each set of triple backticks (```) must always be on their own line, without any other words or anything else on that line.

Here are the commands available to you. Ensure you include one and only one of the following commands in each of your responses:
- `edit`: Replace a range of lines with new content in a file. This is how you can create files: if the file does not exist, it will be created. Here is an example:
  ```
  edit 
  file: &lt;file_name&gt;
  lines: &lt;start_line&gt;-&lt;end_line&gt;
  ---
  &lt;new_content&gt;
  ---
  ```

  The command will:
  1. Delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)
  2. Insert &lt;new_content&gt; starting at &lt;start_line&gt;
  3. If both &lt;start_line&gt; and &lt;end_line&gt; are 0, &lt;new_content&gt; will be prepended to the file
  
  Example:
  edit
  file: solver.py
  lines: 5-7
  ---
  def improved_function():
      print(&quot;Optimized solution&quot;)
  ---
- `ls`: List all files in the current working directory.
- `view_file &lt;file_name&gt; [start_line]`: Display 100 lines of `&lt;file_name&gt;` starting from `start_line` (defaults to line 1).
- `revert`: Revert the code to the best-performing version thus far.
- `reference &lt;string&gt;`: Query the reference solver with a problem and receive its solution. If the problem&#x27;s input is a list, this command would look like: 
  ```
  reference [1,2,3,4]
  ```
- `eval_input &lt;string&gt;`: Run your current solver implementation on the given input. This is the only command that shows stdout from your solver along with both solutions. Example: 
  ```
  eval_input [1,2,3,4]
  ```
- `eval`: Run evaluation on the current solution and report the results.
- `delete`: Delete a range of lines from a file using the format:
  ```
  delete
  file: &lt;file_name&gt;
  lines: &lt;start_line&gt;-&lt;end_line&gt;

  The command will delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)
  
  Example:
  delete
  file: solver.py
  lines: 5-10
  ```
- `profile &lt;filename.py&gt; &lt;input&gt;`: Profile your currently loaded solve method&#x27;s performance on a given input. Shows the 25 most time-consuming lines. Requires specifying a python file (e.g., `solver.py`) for validation, though profiling runs on the current in-memory code.
  Example:
  ```
  profile solver.py [1, 2, 3]
  ```

- `profile_lines &lt;filename.py&gt; &lt;line_number1, line_number2, ...&gt; &lt;input&gt;`: Profiles the chosen lines of the currently loaded code on the given input. Requires specifying a python file for validation.
  Example: 
  ```
  profile_lines solver.py 1,2,3 [1, 2, 3]
  ```

**TIPS:**
After each edit, a linter will automatically run to ensure code quality. If there are critical linter errors, your changes will not be applied, and you will receive the linter&#x27;s error message. Typically, linter errors arise from issues like improper indentation—ensure your edits maintain proper code formatting.
**Cython Compilation:** Edits creating or modifying Cython (`.pyx`) files will automatically trigger a compilation attempt (requires a `setup.py`). You will be notified if compilation succeeds or fails. If it fails, the edit to the `.pyx` file will be automatically reverted.
If the code runs successfully without errors, the in-memory &#x27;last known good code&#x27; will be updated to the new version. Following successful edits, you will receive a summary of your `solve` function&#x27;s performance compared to the reference.
If you get stuck, try reverting your code and restarting your train of thought.
Do not put an if __name__ == &quot;__main__&quot;: block in your code, as it will not be ran (only the solve function will).
Keep trying to better your code until you run out of money. Do not stop beforehand!

**GOALS:**
Your primary objective is to optimize the `solve` function to run as as fast as possible, while returning the optimal solution.
You will receive better scores the quicker your solution runs, and you will be penalized for exceeding the time limit or returning non-optimal solutions.

Below you find the description of the task you will have to solve. Read it carefully and understand what the problem is and what your solver should do.

**TASK DESCRIPTION:**
Graph Global Efficiency

Calculate the global efficiency of a given undirected graph. Global efficiency is defined as the average inverse shortest path length over all pairs of distinct nodes. For a graph G with N nodes, it is calculated as E = (1 / (N * (N-1))) * sum(1 / d(u, v)) for all u != v, where d(u, v) is the shortest path distance between nodes u and v. If two nodes are disconnected, their distance is considered infinite, and the contribution to the sum is 0. For graphs with 0 or 1 node, the global efficiency is 0.

Input:
A dictionary containing a single key &quot;adjacency_list&quot;. The value associated with this key is a list of lists representing the graph&#x27;s adjacency structure. adjacency_list[i] contains a sorted list of integer indices corresponding to the neighbors of node i. Nodes are implicitly indexed from 0 to n-1, where n is the length of the outer list.

Example input:
{
  &quot;adjacency_list&quot;: [
    [1],
    [0, 2],
    [1]
  ]
}
    
Output:
A dictionary containing a single key &quot;global_efficiency&quot;. The value is a floating-point number representing the calculated global efficiency of the graph.

Example output:
{
  &quot;global_efficiency&quot;: 0.8333333333333334
}

Category: graph


Below is the reference implementation. Your function should run much quicker.

import math
import random
from typing import Any
import networkx as nx
import numpy as np

| 01: def solve(self, problem: dict[str, list[list[int]]]) -&gt; dict[str, float]:
| 02:     &quot;&quot;&quot;
| 03:     Calculates the global efficiency of the graph using NetworkX.
| 04: 
| 05:     Args:
| 06:         problem: A dictionary containing the adjacency list of the graph.
| 07:                  {&quot;adjacency_list&quot;: adj_list}
| 08: 
| 09:     Returns:
| 10:         A dictionary containing the global efficiency.
| 11:         {&quot;global_efficiency&quot;: efficiency_value}
| 12:     
| 13: 
| 14:     NOTE: Your solution must pass validation by:
| 15:     1. Returning correctly formatted output
| 16:     2. Having no NaN or infinity values
| 17:     3. Matching expected results within numerical tolerance
| 18:     &quot;&quot;&quot;
| 19:     adj_list = problem[&quot;adjacency_list&quot;]
| 20:     n = len(adj_list)
| 21: 
| 22:     # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
| 23:     if n &lt;= 1:
| 24:         return {&quot;global_efficiency&quot;: 0.0}
| 25: 
| 26:     # Reconstruct the NetworkX graph
| 27:     G = nx.Graph()
| 28:     G.add_nodes_from(range(n))
| 29:     for u, neighbors in enumerate(adj_list):
| 30:         for v in neighbors:
| 31:             if u &lt; v:
| 32:                 G.add_edge(u, v)
| 33: 
| 34:     # Calculate global efficiency
| 35:     try:
| 36:         efficiency = nx.global_efficiency(G)
| 37:     except Exception as e:
| 38:         # Indicate failure - perhaps return NaN or a special value?
| 39:         # For consistency, let&#x27;s return 0.0, although NaN might be more informative.
| 40:         # Check if benchmark guidelines prefer a specific failure value.
| 41:         return {&quot;global_efficiency&quot;: 0.0}  # Or potentially math.nan
| 42: 
| 43:     solution = {&quot;global_efficiency&quot;: float(efficiency)}
| 44:     return solution
| 45: 

This function will be used to check if your solution is valid for a given problem. If it returns False, it means the solution is invalid:

import math
import random
from typing import Any
import networkx as nx
import numpy as np

| 01: def is_solution(
| 02:     self,
| 03:     problem: dict[str, list[list[int]]],
| 04:     solution: dict[str, Any],  # Use Any and validate internally
| 05: ) -&gt; bool:
| 06:     &quot;&quot;&quot;
| 07:     Check if the provided global efficiency solution is valid.
| 08: 
| 09:     Checks structure, type, and numerical closeness to the reference
| 10:     networkx.global_efficiency output.
| 11: 
| 12:     Args:
| 13:         problem: The problem definition dictionary.
| 14:         solution: The proposed solution dictionary.
| 15: 
| 16:     Returns:
| 17:         True if the solution is valid, False otherwise.
| 18:     &quot;&quot;&quot;
| 19:     if &quot;adjacency_list&quot; not in problem:
| 20:         logging.error(&quot;Problem dictionary missing &#x27;adjacency_list&#x27;.&quot;)
| 21:         return False
| 22:     adj_list = problem[&quot;adjacency_list&quot;]
| 23:     n = len(adj_list)
| 24: 
| 25:     # --- Structural and Type Checks ---
| 26:     if not isinstance(solution, dict) or &quot;global_efficiency&quot; not in solution:
| 27:         logging.error(&quot;Solution format invalid: not a dict or missing &#x27;global_efficiency&#x27; key.&quot;)
| 28:         return False
| 29: 
| 30:     proposed_eff = solution[&quot;global_efficiency&quot;]
| 31: 
| 32:     try:
| 33:         # Check if value is a valid float and finite
| 34:         proposed_val = float(proposed_eff)
| 35:         if not math.isfinite(proposed_val):
| 36:             logging.error(f&quot;Proposed global_efficiency is not finite ({proposed_val}).&quot;)
| 37:             return False
| 38:     except (ValueError, TypeError):
| 39:         logging.error(f&quot;Proposed global_efficiency &#x27;{proposed_eff}&#x27; is not a valid float.&quot;)
| 40:         return False
| 41: 
| 42:     # --- Handle Edge Cases ---
| 43:     if n &lt;= 1:
| 44:         expected_eff = 0.0
| 45:         if math.isclose(proposed_val, expected_eff, rel_tol=RTOL, abs_tol=ATOL):
| 46:             logging.debug(f&quot;Solution verification successful for n={n} (expected 0.0).&quot;)
| 47:             return True
| 48:         else:
| 49:             logging.error(
| 50:                 f&quot;Proposed efficiency {proposed_val} != expected {expected_eff} for n={n}.&quot;
| 51:             )
| 52:             return False
| 53: 
| 54:     # --- Numerical Comparison ---
| 55:     try:
| 56:         reference_solution = solve(problem)  # Re-compute reference
| 57:         ref_eff = reference_solution[&quot;global_efficiency&quot;]  # This is already float
| 58: 
| 59:     except Exception as e:
| 60:         logging.error(f&quot;Error computing reference solution: {e}&quot;)
| 61:         return False  # Cannot verify if reference fails
| 62: 
| 63:     # Compare values
| 64:     if not math.isclose(proposed_val, ref_eff, rel_tol=RTOL, abs_tol=ATOL):
| 65:         logging.error(
| 66:             f&quot;Solution verification failed: Efficiency mismatch. &quot;
| 67:             f&quot;Proposed={proposed_val}, Reference={ref_eff} (rtol={RTOL}, atol={ATOL})&quot;
| 68:         )
| 69:         return False
| 70: 
| 71:     logging.debug(&quot;Solution verification successful.&quot;)
| 72:     return True
| 73:</pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>AlgoTune Task Description</summary>
            <div class="section-content">
                <pre>Graph Global Efficiency

Calculate the global efficiency of a given undirected graph. Global efficiency is defined as the average inverse shortest path length over all pairs of distinct nodes. For a graph G with N nodes, it is calculated as E = (1 / (N * (N-1))) * sum(1 / d(u, v)) for all u != v, where d(u, v) is the shortest path distance between nodes u and v. If two nodes are disconnected, their distance is considered infinite, and the contribution to the sum is 0. For graphs with 0 or 1 node, the global efficiency is 0.

Input:
A dictionary containing a single key &quot;adjacency_list&quot;. The value associated with this key is a list of lists representing the graph&#x27;s adjacency structure. adjacency_list[i] contains a sorted list of integer indices corresponding to the neighbors of node i. Nodes are implicitly indexed from 0 to n-1, where n is the length of the outer list.

Example input:
{
  &quot;adjacency_list&quot;: [
    [1],
    [0, 2],
    [1]
  ]
}
    
Output:
A dictionary containing a single key &quot;global_efficiency&quot;. The value is a floating-point number representing the calculated global efficiency of the graph.

Example output:
{
  &quot;global_efficiency&quot;: 0.8333333333333334
}

Category: graph</pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Reference Implementation</summary>
            <div class="section-content">
                <pre class="reference-code"><code class="language-python">import math
import random
from typing import Any
import networkx as nx
import numpy as np

 def solve(self, problem: dict[str, list[list[int]]]) -&gt; dict[str, float]:
     &quot;&quot;&quot;
     Calculates the global efficiency of the graph using NetworkX.
 
     Args:
         problem: A dictionary containing the adjacency list of the graph.
                  {&quot;adjacency_list&quot;: adj_list}
 
     Returns:
         A dictionary containing the global efficiency.
         {&quot;global_efficiency&quot;: efficiency_value}
     
 
     NOTE: Your solution must pass validation by:
     1. Returning correctly formatted output
     2. Having no NaN or infinity values
     3. Matching expected results within numerical tolerance
     &quot;&quot;&quot;
     adj_list = problem[&quot;adjacency_list&quot;]
     n = len(adj_list)
 
     # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
     if n &lt;= 1:
         return {&quot;global_efficiency&quot;: 0.0}
 
     # Reconstruct the NetworkX graph
     G = nx.Graph()
     G.add_nodes_from(range(n))
     for u, neighbors in enumerate(adj_list):
         for v in neighbors:
             if u &lt; v:
                 G.add_edge(u, v)
 
     # Calculate global efficiency
     try:
         efficiency = nx.global_efficiency(G)
     except Exception as e:
         # Indicate failure - perhaps return NaN or a special value?
         # For consistency, let&#x27;s return 0.0, although NaN might be more informative.
         # Check if benchmark guidelines prefer a specific failure value.
         return {&quot;global_efficiency&quot;: 0.0}  # Or potentially math.nan
 
     solution = {&quot;global_efficiency&quot;: float(efficiency)}
     return solution</code></pre>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Best AlgoTuner-Generated Code</summary>
            <div class="section-content">
                <div class="best-file"><div class="file-name" style="font-weight:600; margin-bottom:0.25rem;">solver.py</div><pre class="best-code"><code class="language-python">from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It avoids memory allocation within the main loop for maximum speed.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This optimized version pre-allocates memory for the BFS queue and distances array,
    and uses the highly optimized `fill` method to reset the distances array between
    BFS runs. This avoids the overhead of memory allocation within the main loop.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # --- Pre-allocate memory outside the hot loop ---
    # This is a key optimization to avoid repeated allocation/deallocation overhead.
    distances = np.empty(n, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    # A serial loop over all possible starting nodes.
    for i in range(n):
        # --- Efficient Reset using `fill` ---
        # `fill` is a highly optimized method for setting all elements of an array.
        # This is faster than re-allocating a new array with `np.full` in every iteration.
        distances.fill(-1)
        
        # Start BFS from the current node `i`.
        head = 0
        tail = 0

        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the efficient flattened adjacency list.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
            
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a highly optimized,
        serial Numba kernel that avoids memory allocation in its main loop.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}</code></pre></div>
            </div>
        </details>
        
        <details class="collapsible-section">
            <summary>Speedup vs Budget Plot</summary>
            <div class="section-content plot-section-content">
                
    <div class="plots-container">
        <div class="plot-section plot-half">
            <h3>Speedup vs Budget</h3>
            <div class="plot-container">
                <img src="assets/plots/graph_global_efficiency_gemini-2.5-pro_20250625_151245_perf.png" alt="Speedup vs Budget Plot" />
            </div>
        </div>
        <div class="plot-section plot-half">
            <h3>Action Sequence</h3>
            <div class="plot-container">
                <img src="assets/plots/graph_global_efficiency_gemini-2.5-pro_20250625_151245_actions.png" alt="Action Sequence Plot" />
            </div>
        </div>
    </div>
            </div>
        </details>
    </div>
            
            <div class="conversation-section">
                <h2>Conversation Log</h2>
                
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                Okay, let&#x27;s get started. The task is to calculate the global efficiency of a graph. The reference solution uses `networkx`, which is convenient but often has overhead from constructing its graph object. A direct implementation of the underlying algorithm should be faster.<br><br>The core of the problem is to compute all-pairs shortest paths (APSP). For an unweighted graph, this can be done by running a Breadth-First Search (BFS) from every node.<br><br>My plan is as follows:<br>1.  Implement a serial, pure Python version of the APSP-via-BFS algorithm. This will avoid the `networkx` overhead and serve as a correct baseline.<br>2.  Test this implementation for correctness and get a performance baseline.<br>3.  Optimize the implementation. The nested loops (iterating through all start nodes, and the BFS loop itself) are prime candidates for acceleration. I&#x27;ll use `numba` with its `@njit` decorator, as it&#x27;s excellent for speeding up numerical loops in Python with minimal code changes.<br><br>I&#x27;ll start by writing the initial pure Python implementation.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import collections

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph by running a BFS from each node.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        total_inverse_distance = 0.0
        
        # Run a BFS from each node to find all-pairs shortest paths
        for i in range(n):
            # distances from node i, -1 signifies not visited
            distances = [-1] * n
            distances[i] = 0
            
            # Use collections.deque for an efficient queue
            queue = collections.deque([i])
            
            while queue:
                u = queue.popleft()
                
                for v in adj_list[u]:
                    if distances[v] == -1:
                        dist = distances[u] + 1
                        distances[v] = dist
                        total_inverse_distance += 1.0 / dist
                        queue.append(v)
        
        # The denominator in the global efficiency formula
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 1 messages and have used up $0.0336. You have $0.9664 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-42 out of 42)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import collections
&gt; 03: 
&gt; 04: class Solver:
&gt; 05:     def solve(self, problem: dict, **kwargs) -&gt; Any:
&gt; 06:         &quot;&quot;&quot;
&gt; 07:         Calculates the global efficiency of the graph by running a BFS from each node.
&gt; 08:         &quot;&quot;&quot;
&gt; 09:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 10:         n = len(adj_list)
&gt; 11: 
&gt; 12:         # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
&gt; 13:         if n &lt;= 1:
&gt; 14:             return {&quot;global_efficiency&quot;: 0.0}
&gt; 15: 
&gt; 16:         total_inverse_distance = 0.0
&gt; 17:         
&gt; 18:         # Run a BFS from each node to find all-pairs shortest paths
&gt; 19:         for i in range(n):
&gt; 20:             # distances from node i, -1 signifies not visited
&gt; 21:             distances = [-1] * n
&gt; 22:             distances[i] = 0
&gt; 23:             
&gt; 24:             # Use collections.deque for an efficient queue
&gt; 25:             queue = collections.deque([i])
&gt; 26:             
&gt; 27:             while queue:
&gt; 28:                 u = queue.popleft()
&gt; 29:                 
&gt; 30:                 for v in adj_list[u]:
&gt; 31:                     if distances[v] == -1:
&gt; 32:                         dist = distances[u] + 1
&gt; 33:                         distances[v] = dist
&gt; 34:                         total_inverse_distance += 1.0 / dist
&gt; 35:                         queue.append(v)
&gt; 36:         
&gt; 37:         # The denominator in the global efficiency formula
&gt; 38:         denominator = n * (n - 1)
&gt; 39:         
&gt; 40:         global_efficiency = total_inverse_distance / denominator
&gt; 41:         
&gt; 42:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">1.21</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Snapshot saved Best speedup reached, state saved!<br>Among the 10+ LLMs we tested, your code did not rank in the top 3 for speed. Please use all available packages and tools to optimize its performance. Think outside the box!
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge other"> Invalid Command</span>
            </div>
            <div class="message-content">
                The initial pure Python implementation provided a modest speedup by avoiding the overhead of `networkx`. However, to achieve a top-tier performance, we need to go beyond standard Python and leverage compilation and parallelism. The core of the algorithm, running a Breadth-First Search (BFS) from every node, is computationally intensive and involves tight loops over numerical data—a perfect use case for `numba`.<br><br>My optimization strategy is as follows:<br>1.  **Just-In-Time (JIT) Compilation**: I will extract the main computational loop (the all-pairs BFS) into a standalone function and decorate it with `@numba.njit`. This compiles the Python code down to highly efficient machine code, eliminating Python interpreter overhead.<br>2.  **Parallelism**: The task of running a BFS from each starting node is &quot;embarrassingly parallel.&quot; The calculation for each starting node is completely independent of the others. I will use `numba.prange` to automatically parallelize this outer loop, allowing the workload to be distributed across all available CPU cores.<br>3.  **Optimized Data Structures**: Inside the JIT-compiled function, standard Python objects like `list` and `collections.deque` can be slow. I will replace them with `NumPy` arrays, which `numba` can optimize effectively.<br>    *   The adjacency list, which is a jagged list of lists, will be converted to a `numba.typed.List` of `NumPy` arrays.<br>    *   The BFS queue will be implemented manually using a `NumPy` array and two pointers (`head`, `tail`), a standard and highly efficient technique in compiled environments.<br><br>This approach transforms the algorithm from a sequential, interpreted process into a parallel, compiled one, which should yield a substantial performance improvement.<br><br><pre><code class="language-python">from typing import Any
import numba
import numpy as np
from numba.typed import List

# This function is JIT-compiled with Numba for performance.
# It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
@numba.njit(parallel=True, fastmath=True)
def _calculate_total_inverse_distance(n: int, adj_list_numba: List) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This is the core computation, accelerated with Numba.
    The outer loop is parallelized across all available CPU cores.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
    # can run on a separate thread. Numba automatically handles the reduction for 
    # the `total_inverse_distance` variable, ensuring thread safety and correctness.
    for i in numba.prange(n):
        # Each thread gets its own data structures for its BFS instance.
        # A distance of -1 indicates an unvisited node.
        distances = np.full(n, -1, dtype=np.int32)
        
        # A simple and fast queue implemented with a NumPy array and head/tail pointers.
        # This is much faster inside a Numba function than Python&#x27;s collections.deque.
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Explore neighbors of the current node `u`.
            for v in adj_list_numba[u]:
                if distances[v] == -1: # If neighbor `v` has not been visited yet
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    
                    # Enqueue the newly visited node.
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph.
        The main logic is offloaded to a Numba-jitted, parallel function for max performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Numba works best with its own typed lists, especially for jagged arrays.
        # We convert the Python list of lists to a Numba typed list of NumPy arrays.
        # This conversion has a small one-time cost but enables massive speedups
        # in the compiled function.
        adj_list_numba = List()
        for neighbors in adj_list:
            # Using int32 for node indices is sufficient and memory-efficient.
            adj_list_numba.append(np.array(neighbors, dtype=np.int32))

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance(n, adj_list_numba)
        
        # The denominator in the global efficiency formula for a directed graph.
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 2 messages and have used up $0.0844. You have $0.9156 remaining.<br><br>Error: Command parsing failed<br>Command parsing failed. <br>Remember to include one and only one command in each message. Important: remember to include all arguments for each command.<br>Remember to sandwich your command in between ``` and ```. <br>IMPORTANT: Each set of triple backticks (```) must always be on their own line, without any other words or anything else on that line.<br><br>Here are the commands available to you. Ensure you include one and only one of the following commands in each of your responses:<br>- `edit`: Replace a range of lines with new content in a file. This is how you can create files: if the file does not exist, it will be created. Here is an example:<br>  ```<br>  edit <br>  file: &lt;file_name&gt;<br>  lines: &lt;start_line&gt;-&lt;end_line&gt;<br>  ---<br>  &lt;new_content&gt;<br>  ---<br>  ```<br><br>  The command will:<br>  1. Delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)<br>  2. Insert &lt;new_content&gt; starting at &lt;start_line&gt;<br>  3. If both &lt;start_line&gt; and &lt;end_line&gt; are 0, &lt;new_content&gt; will be prepended to the file<br>  <br>  Example:<br>  edit<br>  file: solver.py<br>  lines: 5-7<br>  ---<br>  def improved_function():<br>      print(&quot;Optimized solution&quot;)<br>  ---<br>- `ls`: List all files in the current working directory.<br>- `view_file &lt;file_name&gt; [start_line]`: Display 100 lines of `&lt;file_name&gt;` starting from `start_line` (defaults to line 1).<br>- `revert`: Revert the code to the best-performing version thus far.<br>- `reference &lt;string&gt;`: Query the reference solver with a problem and receive its solution. If the problem&#x27;s input is a list, this command would look like: <br>  ```<br>  reference [1,2,3,4]<br>  ```<br>- `eval_input &lt;string&gt;`: Run your current solver implementation on the given input. This is the only command that shows stdout from your solver along with both solutions. Example: <br>  ```<br>  eval_input [1,2,3,4]<br>  ```<br>- `eval`: Run evaluation on the current solution and report the results.<br>- `delete`: Delete a range of lines from a file using the format:<br>  ```<br>  delete<br>  file: &lt;file_name&gt;<br>  lines: &lt;start_line&gt;-&lt;end_line&gt;<br><br>  The command will delete the lines from &lt;start_line&gt; to &lt;end_line&gt; (inclusive)<br>  <br>  Example:<br>  ```<br>  delete<br>  file: solver.py<br>  lines: 5-10<br>  ```<br>- `profile &lt;filename.py&gt; &lt;input&gt;`: Profile your currently loaded solve method&#x27;s performance on a given input. Shows the 25 most time-consuming lines. Requires specifying a python file (e.g., `solver.py`) for validation, though profiling runs on the current in-memory code.<br>  Example:<br>  ```<br>  profile solver.py [1, 2, 3]<br>  ```<br>- `profile_lines &lt;filename.py&gt; &lt;line_number1, line_number2, ...&gt; &lt;input&gt;`: Profiles the chosen lines of the currently loaded code on the given input. Requires specifying a python file for validation.<br>  Example: <br>  ```<br>  profile_lines solver.py 1,2,3 [1, 2, 3]<br>  ```
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My apologies. I failed to wrap the code in the required `edit` command block. I will correct that now.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np
from numba.typed import List

# This function is JIT-compiled with Numba for performance.
# It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
@numba.njit(parallel=True, fastmath=True)
def _calculate_total_inverse_distance(n: int, adj_list_numba: List) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This is the core computation, accelerated with Numba.
    The outer loop is parallelized across all available CPU cores.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
    # can run on a separate thread. Numba automatically handles the reduction for 
    # the `total_inverse_distance` variable, ensuring thread safety and correctness.
    for i in numba.prange(n):
        # Each thread gets its own data structures for its BFS instance.
        # A distance of -1 indicates an unvisited node.
        distances = np.full(n, -1, dtype=np.int32)
        
        # A simple and fast queue implemented with a NumPy array and head/tail pointers.
        # This is much faster inside a Numba function than Python&#x27;s collections.deque.
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Explore neighbors of the current node `u`.
            for v in adj_list_numba[u]:
                if distances[v] == -1: # If neighbor `v` has not been visited yet
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    
                    # Enqueue the newly visited node.
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph.
        The main logic is offloaded to a Numba-jitted, parallel function for max performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Numba works best with its own typed lists, especially for jagged arrays.
        # We convert the Python list of lists to a Numba typed list of NumPy arrays.
        # This conversion has a small one-time cost but enables massive speedups
        # in the compiled function.
        adj_list_numba = List()
        for neighbors in adj_list:
            # Using int32 for node indices is sufficient and memory-efficient.
            adj_list_numba.append(np.array(neighbors, dtype=np.int32))

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance(n, adj_list_numba)
        
        # The denominator in the global efficiency formula for a directed graph.
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 3 messages and have used up $0.0988. You have $0.9012 remaining.<br><br>Edit failed (and thus not applied) for solver.py: Line 4: No name &#x27;List&#x27; in module &#x27;numba.typed&#x27; (no-name-in-module)<br>Line 20: Non-iterable value numba.prange(n) is used in an iterating context (not-an-iterable)<br><br>Proposed changes - This is what you tried to apply (lines 1-50 out of 83):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: from numba.typed import List
&gt; 05: 
&gt; 06: # This function is JIT-compiled with Numba for performance.
&gt; 07: # It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
&gt; 08: @numba.njit(parallel=True, fastmath=True)
&gt; 09: def _calculate_total_inverse_distance(n: int, adj_list_numba: List) -&gt; float:
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     Calculates the sum of the inverse of all-pairs shortest path lengths.
&gt; 12:     This is the core computation, accelerated with Numba.
&gt; 13:     The outer loop is parallelized across all available CPU cores.
&gt; 14:     &quot;&quot;&quot;
&gt; 15:     total_inverse_distance = 0.0
&gt; 16: 
&gt; 17:     # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
&gt; 18:     # can run on a separate thread. Numba automatically handles the reduction for 
&gt; 19:     # the `total_inverse_distance` variable, ensuring thread safety and correctness.
&gt; 20:     for i in numba.prange(n):
&gt; 21:         # Each thread gets its own data structures for its BFS instance.
&gt; 22:         # A distance of -1 indicates an unvisited node.
&gt; 23:         distances = np.full(n, -1, dtype=np.int32)
&gt; 24:         
&gt; 25:         # A simple and fast queue implemented with a NumPy array and head/tail pointers.
&gt; 26:         # This is much faster inside a Numba function than Python&#x27;s collections.deque.
&gt; 27:         queue = np.empty(n, dtype=np.int32)
&gt; 28:         head = 0
&gt; 29:         tail = 0
&gt; 30: 
&gt; 31:         # Start BFS from the current node `i`.
&gt; 32:         distances[i] = 0
&gt; 33:         queue[tail] = i
&gt; 34:         tail += 1
&gt; 35: 
&gt; 36:         while head &lt; tail:
&gt; 37:             u = queue[head]
&gt; 38:             head += 1
&gt; 39: 
&gt; 40:             # Explore neighbors of the current node `u`.
&gt; 41:             for v in adj_list_numba[u]:
&gt; 42:                 if distances[v] == -1: # If neighbor `v` has not been visited yet
&gt; 43:                     dist = distances[u] + 1
&gt; 44:                     distances[v] = dist
&gt; 45:                     total_inverse_distance += 1.0 / dist
&gt; 46:                     
&gt; 47:                     # Enqueue the newly visited node.
&gt; 48:                     queue[tail] = v
&gt; 49:                     tail += 1
&gt; 50:                     
...</code></pre><br><br>CURRENT FILE - This is what&#x27;s actually in the file (lines 1-42 out of 42):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">| 01: from typing import Any
| 02: import collections
| 03: 
| 04: class Solver:
| 05:     def solve(self, problem: dict, **kwargs) -&gt; Any:
| 06:         &quot;&quot;&quot;
| 07:         Calculates the global efficiency of the graph by running a BFS from each node.
| 08:         &quot;&quot;&quot;
| 09:         adj_list = problem[&quot;adjacency_list&quot;]
| 10:         n = len(adj_list)
| 11: 
| 12:         # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
| 13:         if n &lt;= 1:
| 14:             return {&quot;global_efficiency&quot;: 0.0}
| 15: 
| 16:         total_inverse_distance = 0.0
| 17:         
| 18:         # Run a BFS from each node to find all-pairs shortest paths
| 19:         for i in range(n):
| 20:             # distances from node i, -1 signifies not visited
| 21:             distances = [-1] * n
| 22:             distances[i] = 0
| 23:             
| 24:             # Use collections.deque for an efficient queue
| 25:             queue = collections.deque([i])
| 26:             
| 27:             while queue:
| 28:                 u = queue.popleft()
| 29:                 
| 30:                 for v in adj_list[u]:
| 31:                     if distances[v] == -1:
| 32:                         dist = distances[u] + 1
| 33:                         distances[v] = dist
| 34:                         total_inverse_distance += 1.0 / dist
| 35:                         queue.append(v)
| 36:         
| 37:         # The denominator in the global efficiency formula
| 38:         denominator = n * (n - 1)
| 39:         
| 40:         global_efficiency = total_inverse_distance / denominator
| 41:         
| 42:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                I see the linter errors. The `numba.prange` error is a known false positive with some static analysis tools; the code is correct for Numba&#x27;s JIT compiler. The `numba.typed.List` import error is more problematic, suggesting an environment issue or an older Numba version.<br><br>To resolve this robustly and potentially improve performance further, I will switch to a more C-style, &quot;flattened&quot; representation for the adjacency list, which is highly compatible with Numba and avoids the `typed.List` dependency altogether. This involves two NumPy arrays:<br>1.  `offsets`: An array where `offsets[i]` gives the starting index for node `i`&#x27;s neighbors.<br>2.  `flat_adj`: A single, flat array containing all neighbor lists concatenated together.<br><br>This structure is extremely fast for Numba to process.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
@numba.njit(parallel=True, fastmath=True, cache=True)
def _calculate_total_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list for maximum performance with Numba.
    The outer loop is parallelized across all available CPU cores.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
    # can run on a separate thread. Numba automatically handles the reduction for 
    # the `total_inverse_distance` variable, ensuring thread safety and correctness.
    for i in numba.prange(n):
        # Each thread gets its own data structures for its BFS instance.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the flattened adjacency list and offsets array.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph.
        The main logic is offloaded to a Numba-jitted, parallel function for max performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        # `offsets` stores the starting index of each node&#x27;s neighbors.
        # `flat_adj` stores all neighbors concatenated into a single array.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            for neighbor in neighbors:
                flat_adj[current_pos] = neighbor
                current_pos += 1
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 4 messages and have used up $0.1245. You have $0.8755 remaining.<br><br>Edit failed (and thus not applied) for solver.py: Line 19: Non-iterable value numba.prange(n) is used in an iterating context (not-an-iterable)<br><br>Proposed changes - This is what you tried to apply (lines 1-50 out of 84):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
&gt; 07: @numba.njit(parallel=True, fastmath=True, cache=True)
&gt; 08: def _calculate_total_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 11:     adjacency list for maximum performance with Numba.
&gt; 12:     The outer loop is parallelized across all available CPU cores.
&gt; 13:     &quot;&quot;&quot;
&gt; 14:     total_inverse_distance = 0.0
&gt; 15: 
&gt; 16:     # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
&gt; 17:     # can run on a separate thread. Numba automatically handles the reduction for 
&gt; 18:     # the `total_inverse_distance` variable, ensuring thread safety and correctness.
&gt; 19:     for i in numba.prange(n):
&gt; 20:         # Each thread gets its own data structures for its BFS instance.
&gt; 21:         distances = np.full(n, -1, dtype=np.int32)
&gt; 22:         queue = np.empty(n, dtype=np.int32)
&gt; 23:         head = 0
&gt; 24:         tail = 0
&gt; 25: 
&gt; 26:         # Start BFS from the current node `i`.
&gt; 27:         distances[i] = 0
&gt; 28:         queue[tail] = i
&gt; 29:         tail += 1
&gt; 30: 
&gt; 31:         while head &lt; tail:
&gt; 32:             u = queue[head]
&gt; 33:             head += 1
&gt; 34: 
&gt; 35:             # Iterate through neighbors of u using the flattened adjacency list and offsets array.
&gt; 36:             start = offsets[u]
&gt; 37:             end = offsets[u+1]
&gt; 38:             for j in range(start, end):
&gt; 39:                 v = flat_adj[j]
&gt; 40:                 if distances[v] == -1:
&gt; 41:                     dist = distances[u] + 1
&gt; 42:                     distances[v] = dist
&gt; 43:                     total_inverse_distance += 1.0 / dist
&gt; 44:                     queue[tail] = v
&gt; 45:                     tail += 1
&gt; 46:                     
&gt; 47:     return total_inverse_distance
&gt; 48: 
&gt; 49: class Solver:
&gt; 50:     def solve(self, problem: dict, **kwargs) -&gt; Any:
...</code></pre><br><br>CURRENT FILE - This is what&#x27;s actually in the file (lines 1-42 out of 42):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">| 01: from typing import Any
| 02: import collections
| 03: 
| 04: class Solver:
| 05:     def solve(self, problem: dict, **kwargs) -&gt; Any:
| 06:         &quot;&quot;&quot;
| 07:         Calculates the global efficiency of the graph by running a BFS from each node.
| 08:         &quot;&quot;&quot;
| 09:         adj_list = problem[&quot;adjacency_list&quot;]
| 10:         n = len(adj_list)
| 11: 
| 12:         # Handle edge cases: efficiency is 0 for graphs with 0 or 1 node.
| 13:         if n &lt;= 1:
| 14:             return {&quot;global_efficiency&quot;: 0.0}
| 15: 
| 16:         total_inverse_distance = 0.0
| 17:         
| 18:         # Run a BFS from each node to find all-pairs shortest paths
| 19:         for i in range(n):
| 20:             # distances from node i, -1 signifies not visited
| 21:             distances = [-1] * n
| 22:             distances[i] = 0
| 23:             
| 24:             # Use collections.deque for an efficient queue
| 25:             queue = collections.deque([i])
| 26:             
| 27:             while queue:
| 28:                 u = queue.popleft()
| 29:                 
| 30:                 for v in adj_list[u]:
| 31:                     if distances[v] == -1:
| 32:                         dist = distances[u] + 1
| 33:                         distances[v] = dist
| 34:                         total_inverse_distance += 1.0 / dist
| 35:                         queue.append(v)
| 36:         
| 37:         # The denominator in the global efficiency formula
| 38:         denominator = n * (n - 1)
| 39:         
| 40:         global_efficiency = total_inverse_distance / denominator
| 41:         
| 42:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The linter is flagging `numba.prange` as a non-iterable, which is a known false positive. The linter doesn&#x27;t understand that `numba.prange` is a special directive for Numba&#x27;s JIT compiler, not a standard Python function. The code is functionally correct.<br><br>To bypass this linter check, I will add a `pylint: disable=not-an-iterable` comment on the specific line. This will instruct the linter to ignore this known issue, allowing the code to be submitted while preserving the crucial parallelization feature. The underlying algorithm using the flattened adjacency list is sound and highly optimized for this kind of problem.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
@numba.njit(parallel=True, fastmath=True, cache=True)
def _calculate_total_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list for maximum performance with Numba.
    The outer loop is parallelized across all available CPU cores.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
    # can run on a separate thread. The comment below disables a linter false positive.
    # pylint: disable=not-an-iterable
    for i in numba.prange(n):
        # Each thread gets its own data structures for its BFS instance.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the flattened adjacency list and offsets array.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph.
        The main logic is offloaded to a Numba-jitted, parallel function for max performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        # `offsets` stores the starting index of each node&#x27;s neighbors.
        # `flat_adj` stores all neighbors concatenated into a single array.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            for neighbor in neighbors:
                flat_adj[current_pos] = neighbor
                current_pos += 1
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 5 messages and have used up $0.1505. You have $0.8495 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 84)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
&gt; 07: @numba.njit(parallel=True, fastmath=True, cache=True)
&gt; 08: def _calculate_total_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 11:     adjacency list for maximum performance with Numba.
&gt; 12:     The outer loop is parallelized across all available CPU cores.
&gt; 13:     &quot;&quot;&quot;
&gt; 14:     total_inverse_distance = 0.0
&gt; 15: 
&gt; 16:     # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
&gt; 17:     # can run on a separate thread. The comment below disables a linter false positive.
&gt; 18:     # pylint: disable=not-an-iterable
&gt; 19:     for i in numba.prange(n):
&gt; 20:         # Each thread gets its own data structures for its BFS instance.
&gt; 21:         distances = np.full(n, -1, dtype=np.int32)
&gt; 22:         queue = np.empty(n, dtype=np.int32)
&gt; 23:         head = 0
&gt; 24:         tail = 0
&gt; 25: 
&gt; 26:         # Start BFS from the current node `i`.
&gt; 27:         distances[i] = 0
&gt; 28:         queue[tail] = i
&gt; 29:         tail += 1
&gt; 30: 
&gt; 31:         while head &lt; tail:
&gt; 32:             u = queue[head]
&gt; 33:             head += 1
&gt; 34: 
&gt; 35:             # Iterate through neighbors of u using the flattened adjacency list and offsets array.
&gt; 36:             start = offsets[u]
&gt; 37:             end = offsets[u+1]
&gt; 38:             for j in range(start, end):
&gt; 39:                 v = flat_adj[j]
&gt; 40:                 if distances[v] == -1:
&gt; 41:                     dist = distances[u] + 1
&gt; 42:                     distances[v] = dist
&gt; 43:                     total_inverse_distance += 1.0 / dist
&gt; 44:                     queue[tail] = v
&gt; 45:                     tail += 1
&gt; 46:                     
&gt; 47:     return total_inverse_distance
&gt; 48: 
&gt; 49: class Solver:
&gt; 50:     def solve(self, problem: dict, **kwargs) -&gt; Any:
...</code></pre><br><br>Process failed without error message. Return dict keys: []. Process may have crashed or timed out.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                A crash or timeout on a parallelized Numba solution often points to issues with thread management or memory, especially in a constrained evaluation environment. The `parallel=True` flag instructs Numba to use all available cores, which can lead to thread over-subscription if the evaluator is already running multiple processes. This can cause extreme slowdowns or crashes.<br><br>To debug this and create a more robust solution, I will take a step back from parallelism and focus on a highly optimized *serial* implementation. The performance gains from JIT compilation with `@numba.njit` are substantial on their own and often sufficient. This approach eliminates any risks associated with parallel execution in the target environment.<br><br>My plan is:<br>1.  **Remove Parallelism**: I will change `numba.prange` back to a standard `range` loop and remove the `parallel=True` argument from the `@njit` decorator. This makes the core computation serial.<br>2.  **Retain Optimizations**: I will keep the efficient flattened adjacency list (`offsets` and `flat_adj` arrays) and the manual NumPy queue, as these are key to high serial performance.<br>3.  **Optimize Data Conversion**: I&#x27;ll slightly improve the Python-side data conversion loop by using NumPy slice assignments instead of iterating element by element, which is faster.<br><br>This results in a stable, fast, serial solution that avoids the complexities and potential pitfalls of parallel execution while still being significantly faster than pure Python.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It runs a BFS from each node to calculate the sum of inverse shortest path lengths.
# This version is serial but still benefits greatly from JIT compilation.
@numba.njit(cache=True)
def _calculate_total_inverse_distance_serial(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list for maximum performance with Numba. This is a serial implementation.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # A serial loop over all possible starting nodes.
    for i in range(n):
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the flattened adjacency list and offsets array.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph.
        The main logic is offloaded to a Numba-jitted, serial function for performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                # Use faster slice assignment
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance_serial(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 6 messages and have used up $0.1988. You have $0.8012 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 81)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It runs a BFS from each node to calculate the sum of inverse shortest path lengths.
&gt; 07: # This version is serial but still benefits greatly from JIT compilation.
&gt; 08: @numba.njit(cache=True)
&gt; 09: def _calculate_total_inverse_distance_serial(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 12:     adjacency list for maximum performance with Numba. This is a serial implementation.
&gt; 13:     &quot;&quot;&quot;
&gt; 14:     total_inverse_distance = 0.0
&gt; 15: 
&gt; 16:     # A serial loop over all possible starting nodes.
&gt; 17:     for i in range(n):
&gt; 18:         distances = np.full(n, -1, dtype=np.int32)
&gt; 19:         queue = np.empty(n, dtype=np.int32)
&gt; 20:         head = 0
&gt; 21:         tail = 0
&gt; 22: 
&gt; 23:         # Start BFS from the current node `i`.
&gt; 24:         distances[i] = 0
&gt; 25:         queue[tail] = i
&gt; 26:         tail += 1
&gt; 27: 
&gt; 28:         while head &lt; tail:
&gt; 29:             u = queue[head]
&gt; 30:             head += 1
&gt; 31: 
&gt; 32:             # Iterate through neighbors of u using the flattened adjacency list and offsets array.
&gt; 33:             start = offsets[u]
&gt; 34:             end = offsets[u+1]
&gt; 35:             for j in range(start, end):
&gt; 36:                 v = flat_adj[j]
&gt; 37:                 if distances[v] == -1:
&gt; 38:                     dist = distances[u] + 1
&gt; 39:                     distances[v] = dist
&gt; 40:                     total_inverse_distance += 1.0 / dist
&gt; 41:                     queue[tail] = v
&gt; 42:                     tail += 1
&gt; 43:                     
&gt; 44:     return total_inverse_distance
&gt; 45: 
&gt; 46: class Solver:
&gt; 47:     def solve(self, problem: dict, **kwargs) -&gt; Any:
&gt; 48:         &quot;&quot;&quot;
&gt; 49:         Calculates the global efficiency of the graph.
&gt; 50:         The main logic is offloaded to a Numba-jitted, serial function for performance.
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">14.11</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Snapshot saved Best speedup reached, state saved!<br>Among the 10+ LLMs we tested, your code did not rank in the top 3 for speed. Please use all available packages and tools to optimize its performance. Think outside the box!
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The 14.11x speedup from the JIT-compiled serial solution is good, but the feedback indicates that a top-tier solution is significantly faster. This suggests that my approach of manually implementing BFS, even with Numba, is being outperformed by highly optimized, pre-compiled library functions. The prompt encourages me to &quot;use all available packages and tools&quot; and to &quot;think outside the box.&quot;<br><br>My new strategy is to completely offload the core computational task—All-Pairs Shortest Path (APSP)—to the `scipy` library, which is a cornerstone of the scientific Python ecosystem and contains routines written in C and Fortran.<br><br>The plan is as follows:<br>1.  **Graph Representation**: Instead of a Python list of lists or a custom flattened array, I will represent the graph as a `scipy.sparse.csr_matrix`. This is a standard, highly efficient format for sparse graphs and is the expected input for `scipy`&#x27;s graph algorithms.<br>2.  **APSP Calculation**: I will use the `scipy.sparse.csgraph.shortest_path` function. This single function call replaces my entire Numba-jitted BFS loop. Crucially, I will use the `unweighted=True` argument, which tells SciPy to use a fast BFS-based algorithm, perfect for this problem.<br>3.  **Result Aggregation**: The `shortest_path` function returns a dense `n x n` NumPy array containing all the path lengths. I will use efficient NumPy vectorized operations to calculate the sum of inverse distances from this matrix, handling infinities (unreachable nodes) and zeros (the diagonal) correctly.<br><br>This approach delegates the heavy lifting to a specialized, industry-standard library, minimizing Python overhead and leveraging decades of optimization work in the scientific computing community. It&#x27;s a higher-level abstraction that should yield a significant performance leap.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numpy as np
from scipy.sparse import csr_matrix
from scipy.sparse.csgraph import shortest_path

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using SciPy&#x27;s optimized routines.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # --- Step 1: Create a CSR sparse matrix representation of the graph ---
        # This is a highly efficient format for graph algorithms and the standard
        # input for scipy.sparse.csgraph functions.

        # Pre-calculate the number of edges for efficient pre-allocation of NumPy arrays.
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        
        # Pre-allocate arrays for CSR format.
        # indptr (index pointer): Stores the start index of each node&#x27;s neighbors.
        # indices: A flattened array of all neighbor indices.
        # data: The weight of each edge (1 for an unweighted graph).
        indptr = np.zeros(n + 1, dtype=np.int32)
        indices = np.empty(num_edges, dtype=np.int32)
        data = np.ones(num_edges, dtype=np.int8) # Use int8 for memory efficiency

        # Populate the CSR arrays by iterating through the adjacency list.
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            indptr[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                indices[current_pos : current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        indptr[n] = current_pos

        # Construct the CSR matrix.
        graph_csr = csr_matrix((data, indices, indptr), shape=(n, n))

        # --- Step 2: Calculate all-pairs shortest paths ---
        # Use SciPy&#x27;s highly optimized function. `unweighted=True` ensures it uses
        # a fast BFS-based algorithm. The result is a dense matrix of distances.
        dist_matrix = shortest_path(
            csgraph=graph_csr,
            directed=True,
            unweighted=True
        )

        # --- Step 3: Calculate global efficiency from the distance matrix ---
        # The formula is sum(1/d(i,j)) for all i!=j, divided by n*(n-1).
        
        # Use vectorized numpy operations for maximum speed.
        # Suppress the warning for division by zero (for the diagonal).
        with np.errstate(divide=&#x27;ignore&#x27;):
            inverse_dist_matrix = 1.0 / dist_matrix
        
        # Unreachable pairs have dist=inf, so inverse_dist=0.
        # The diagonal has dist=0, so inverse_dist=inf. We set these infs to 0.
        inverse_dist_matrix[~np.isfinite(inverse_dist_matrix)] = 0
        
        # The sum of all elements is the sum of all 1/d(i,j) for i!=j.
        total_inverse_distance = np.sum(inverse_dist_matrix)

        denominator = n * (n - 1)
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 7 messages and have used up $0.2615. You have $0.7385 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 72)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numpy as np
&gt; 03: from scipy.sparse import csr_matrix
&gt; 04: from scipy.sparse.csgraph import shortest_path
&gt; 05: 
&gt; 06: class Solver:
&gt; 07:     def solve(self, problem: dict, **kwargs) -&gt; Any:
&gt; 08:         &quot;&quot;&quot;
&gt; 09:         Calculates the global efficiency of the graph using SciPy&#x27;s optimized routines.
&gt; 10:         &quot;&quot;&quot;
&gt; 11:         adj_list = problem[&quot;adjacency_list&quot;]
&gt; 12:         n = len(adj_list)
&gt; 13: 
&gt; 14:         if n &lt;= 1:
&gt; 15:             return {&quot;global_efficiency&quot;: 0.0}
&gt; 16: 
&gt; 17:         # --- Step 1: Create a CSR sparse matrix representation of the graph ---
&gt; 18:         # This is a highly efficient format for graph algorithms and the standard
&gt; 19:         # input for scipy.sparse.csgraph functions.
&gt; 20: 
&gt; 21:         # Pre-calculate the number of edges for efficient pre-allocation of NumPy arrays.
&gt; 22:         num_edges = sum(len(neighbors) for neighbors in adj_list)
&gt; 23:         
&gt; 24:         # Pre-allocate arrays for CSR format.
&gt; 25:         # indptr (index pointer): Stores the start index of each node&#x27;s neighbors.
&gt; 26:         # indices: A flattened array of all neighbor indices.
&gt; 27:         # data: The weight of each edge (1 for an unweighted graph).
&gt; 28:         indptr = np.zeros(n + 1, dtype=np.int32)
&gt; 29:         indices = np.empty(num_edges, dtype=np.int32)
&gt; 30:         data = np.ones(num_edges, dtype=np.int8) # Use int8 for memory efficiency
&gt; 31: 
&gt; 32:         # Populate the CSR arrays by iterating through the adjacency list.
&gt; 33:         current_pos = 0
&gt; 34:         for i, neighbors in enumerate(adj_list):
&gt; 35:             indptr[i] = current_pos
&gt; 36:             num_neighbors = len(neighbors)
&gt; 37:             if num_neighbors &gt; 0:
&gt; 38:                 indices[current_pos : current_pos + num_neighbors] = neighbors
&gt; 39:             current_pos += num_neighbors
&gt; 40:         indptr[n] = current_pos
&gt; 41: 
&gt; 42:         # Construct the CSR matrix.
&gt; 43:         graph_csr = csr_matrix((data, indices, indptr), shape=(n, n))
&gt; 44: 
&gt; 45:         # --- Step 2: Calculate all-pairs shortest paths ---
&gt; 46:         # Use SciPy&#x27;s highly optimized function. `unweighted=True` ensures it uses
&gt; 47:         # a fast BFS-based algorithm. The result is a dense matrix of distances.
&gt; 48:         dist_matrix = shortest_path(
&gt; 49:             csgraph=graph_csr,
&gt; 50:             directed=True,
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">2.98</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The SciPy approach was significantly slower than my best Numba implementation. This regression suggests that the overhead of creating the `csr_matrix` in Python and allocating the large dense `n x n` distance matrix outweighs the benefits of the pre-compiled `shortest_path` function for this specific problem scale.<br><br>The serial Numba solution achieved a 14.11x speedup, indicating its core logic is highly effective. The parallel version crashed, likely due to resource contention from using all available CPU cores in the evaluation environment.<br><br>My new strategy is to reinstate the parallel Numba solution but with a crucial modification: **explicitly limiting the number of threads**. This provides the benefits of parallelism while preventing the system from being overwhelmed, which should prevent the crash and unlock performance beyond the serial version.<br><br>**The Plan:**<br>1.  **Restore Parallel Numba Code**: Revert to the fast implementation using `numba.prange` and the flattened adjacency list.<br>2.  **Limit Threads**: Before calling the parallel function, use `numba.set_num_threads()` to set a conservative number of threads (e.g., 4). This is a standard technique in high-performance computing to ensure stability and avoid resource contention.<br>3.  **Retain All Optimizations**: Keep the efficient flattened data structure and the custom NumPy-based queue, which are proven to be fast within a Numba context.<br><br>This approach directly targets the instability of my previous fastest attempt, aiming for a robust, multi-threaded solution that should significantly outperform the serial version.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
@numba.njit(parallel=True, cache=True)
def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list. The outer loop is parallelized across a controlled number of threads.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
    # can run on a separate thread. The comment below disables a linter false positive.
    # pylint: disable=not-an-iterable
    for i in numba.prange(n):
        # Each thread gets its own data structures for its BFS instance.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the flattened adjacency list and offsets array.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a parallelized Numba kernel
        with a controlled number of threads for robust performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # --- Key Optimization: Control the number of threads ---
        # In constrained environments, using all available cores can cause contention and crashes.
        # Limiting to a moderate number (e.g., 4) provides parallelism without overwhelming the system.
        # This is a more robust approach than letting Numba default to all cores.
        try:
            numba.set_num_threads(4)
        except Exception:
            # If setting threads fails for any reason, proceed with Numba&#x27;s default.
            pass

        # Convert the Python list of lists to a flattened adjacency list representation.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance parallel Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 8 messages and have used up $0.3129. You have $0.6871 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 91)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It runs the BFS from each node in parallel to calculate the sum of inverse shortest path lengths.
&gt; 07: @numba.njit(parallel=True, cache=True)
&gt; 08: def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 11:     adjacency list. The outer loop is parallelized across a controlled number of threads.
&gt; 12:     &quot;&quot;&quot;
&gt; 13:     total_inverse_distance = 0.0
&gt; 14: 
&gt; 15:     # The outer loop is parallelized using numba.prange. Each iteration (a full BFS) 
&gt; 16:     # can run on a separate thread. The comment below disables a linter false positive.
&gt; 17:     # pylint: disable=not-an-iterable
&gt; 18:     for i in numba.prange(n):
&gt; 19:         # Each thread gets its own data structures for its BFS instance.
&gt; 20:         distances = np.full(n, -1, dtype=np.int32)
&gt; 21:         queue = np.empty(n, dtype=np.int32)
&gt; 22:         head = 0
&gt; 23:         tail = 0
&gt; 24: 
&gt; 25:         # Start BFS from the current node `i`.
&gt; 26:         distances[i] = 0
&gt; 27:         queue[tail] = i
&gt; 28:         tail += 1
&gt; 29: 
&gt; 30:         while head &lt; tail:
&gt; 31:             u = queue[head]
&gt; 32:             head += 1
&gt; 33: 
&gt; 34:             # Iterate through neighbors of u using the flattened adjacency list and offsets array.
&gt; 35:             start = offsets[u]
&gt; 36:             end = offsets[u+1]
&gt; 37:             for j in range(start, end):
&gt; 38:                 v = flat_adj[j]
&gt; 39:                 if distances[v] == -1:
&gt; 40:                     dist = distances[u] + 1
&gt; 41:                     distances[v] = dist
&gt; 42:                     total_inverse_distance += 1.0 / dist
&gt; 43:                     queue[tail] = v
&gt; 44:                     tail += 1
&gt; 45:                     
&gt; 46:     return total_inverse_distance
&gt; 47: 
&gt; 48: class Solver:
&gt; 49:     def solve(self, problem: dict, **kwargs) -&gt; Any:
&gt; 50:         &quot;&quot;&quot;
...</code></pre><br><br>Process failed without error message. Return dict keys: []. Process may have crashed or timed out.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The repeated crashes indicate that the parallel Numba implementation (`prange`) is unstable in the evaluation environment, likely due to resource constraints. The SciPy approach was too slow due to overhead.<br><br>My most successful attempt was the **serial Numba solution (14.11x speedup)**. The path to a better score is to further optimize this stable foundation. The primary performance bottleneck within that serial code is memory allocation: inside the main loop, `np.full` and `np.empty` are called `n` times, which is inefficient.<br><br>My new strategy is to eliminate memory allocation from the hot loop entirely:<br>1.  **Pre-allocate Memory**: The `distances` and `queue` NumPy arrays will be allocated *once* before the main loop starts.<br>2.  **Efficient Reset**: Instead of re-creating the `distances` array for each starting node, I will reset only the values that were modified in the previous BFS run. The nodes visited in a BFS are stored in the `queue`. After each BFS completes, I will iterate through the `queue` (from index `0` to `tail`) and reset the `distances` for only those nodes.<br><br>This avoids thousands of costly memory allocations and full array initializations, replacing them with a targeted reset operation that only touches the necessary data. This is a standard technique in high-performance computing and should provide a significant speedup over the previous stable version.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It avoids memory allocation within the main loop for maximum speed.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This optimized version pre-allocates memory for the BFS queue and distances array,
    and resets only the necessary data between BFS runs, avoiding costly allocations
    and full array initializations inside the main loop.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # --- Pre-allocate memory outside the hot loop ---
    # This is a key optimization to avoid repeated allocation/deallocation overhead.
    distances = np.full(n, -1, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    # A serial loop over all possible starting nodes.
    for i in range(n):
        # Start BFS from the current node `i`.
        # The queue is managed by head/tail pointers, no re-allocation needed.
        head = 0
        tail = 0

        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        # Standard BFS implementation, operating on the pre-allocated arrays.
        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the efficient flattened adjacency list.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
        
        # --- Efficient Reset ---
        # After the BFS from node `i` is complete, we reset the `distances` array.
        # Instead of re-initializing the entire array (e.g., `distances.fill(-1)`),
        # we only reset the entries for the nodes that were actually visited.
        # The visited nodes are exactly those that were added to the queue.
        for k in range(tail):
            distances[queue[k]] = -1
            
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a highly optimized,
        serial Numba kernel that avoids memory allocation in its main loop.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 9 messages and have used up $0.3786. You have $0.6214 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 94)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It avoids memory allocation within the main loop for maximum speed.
&gt; 07: @numba.njit(cache=True, fastmath=True)
&gt; 08: def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths.
&gt; 11:     This optimized version pre-allocates memory for the BFS queue and distances array,
&gt; 12:     and resets only the necessary data between BFS runs, avoiding costly allocations
&gt; 13:     and full array initializations inside the main loop.
&gt; 14:     &quot;&quot;&quot;
&gt; 15:     total_inverse_distance = 0.0
&gt; 16: 
&gt; 17:     # --- Pre-allocate memory outside the hot loop ---
&gt; 18:     # This is a key optimization to avoid repeated allocation/deallocation overhead.
&gt; 19:     distances = np.full(n, -1, dtype=np.int32)
&gt; 20:     queue = np.empty(n, dtype=np.int32)
&gt; 21: 
&gt; 22:     # A serial loop over all possible starting nodes.
&gt; 23:     for i in range(n):
&gt; 24:         # Start BFS from the current node `i`.
&gt; 25:         # The queue is managed by head/tail pointers, no re-allocation needed.
&gt; 26:         head = 0
&gt; 27:         tail = 0
&gt; 28: 
&gt; 29:         distances[i] = 0
&gt; 30:         queue[tail] = i
&gt; 31:         tail += 1
&gt; 32:         
&gt; 33:         # Standard BFS implementation, operating on the pre-allocated arrays.
&gt; 34:         while head &lt; tail:
&gt; 35:             u = queue[head]
&gt; 36:             head += 1
&gt; 37: 
&gt; 38:             # Iterate through neighbors of u using the efficient flattened adjacency list.
&gt; 39:             start = offsets[u]
&gt; 40:             end = offsets[u+1]
&gt; 41:             for j in range(start, end):
&gt; 42:                 v = flat_adj[j]
&gt; 43:                 if distances[v] == -1:
&gt; 44:                     dist = distances[u] + 1
&gt; 45:                     distances[v] = dist
&gt; 46:                     total_inverse_distance += 1.0 / dist
&gt; 47:                     queue[tail] = v
&gt; 48:                     tail += 1
&gt; 49:         
&gt; 50:         # --- Efficient Reset ---
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">13.29</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Speedup did not improve, snapshot not saved.
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The previous attempt to optimize the serial Numba solution by pre-allocating and selectively resetting the `distances` array was slightly slower than the version that re-allocated with `np.full` in each loop. This surprising result suggests that the Numba-optimized `np.full` call is extremely fast, and my manual Python-style reset loop (`for k in range(tail): ...`) had too much overhead, even when JIT-compiled.<br><br>My best performance (14.11x) came from a simple serial Numba loop that re-allocated memory. To beat this, I need an even faster way to reset the `distances` array between BFS runs.<br><br>My new strategy is to combine pre-allocation with a different, more efficient reset mechanism: `numpy.ndarray.fill`.<br><br>**The Plan:**<br>1.  **Pre-allocate Memory**: I will allocate the `distances` and `queue` arrays once, outside the main loop, to avoid repeated allocation overhead.<br>2.  **Use `ndarray.fill()` for Reset**: Inside the loop, instead of creating a new array with `np.full` or using a manual loop, I will call `distances.fill(-1)`. This method is implemented in C and is highly optimized for setting all values of an array. It should be faster than `np.full` because it avoids the overhead of memory allocation, and faster than my previous manual reset loop due to its low-level implementation.<br><br>This approach targets the single most expensive operation inside the loop (re-initializing the `distances` array) with the most efficient tool available in the NumPy/Numba arsenal, representing a direct and logical optimization over my previous best attempt.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It avoids memory allocation within the main loop for maximum speed.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This optimized version pre-allocates memory for the BFS queue and distances array,
    and uses the highly optimized `fill` method to reset the distances array between
    BFS runs. This avoids the overhead of memory allocation within the main loop.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # --- Pre-allocate memory outside the hot loop ---
    # This is a key optimization to avoid repeated allocation/deallocation overhead.
    distances = np.empty(n, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    # A serial loop over all possible starting nodes.
    for i in range(n):
        # --- Efficient Reset using `fill` ---
        # `fill` is a highly optimized method for setting all elements of an array.
        # This is faster than re-allocating a new array with `np.full` in every iteration.
        distances.fill(-1)
        
        # Start BFS from the current node `i`.
        head = 0
        tail = 0

        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        while head &lt; tail:
            u = queue[head]
            head += 1

            # Iterate through neighbors of u using the efficient flattened adjacency list.
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
            
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a highly optimized,
        serial Numba kernel that avoids memory allocation in its main loop.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance Numba function to do the heavy lifting.
        total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 10 messages and have used up $0.4609. You have $0.5391 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 89)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This function is JIT-compiled with Numba for performance.
&gt; 06: # It avoids memory allocation within the main loop for maximum speed.
&gt; 07: @numba.njit(cache=True, fastmath=True)
&gt; 08: def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths.
&gt; 11:     This optimized version pre-allocates memory for the BFS queue and distances array,
&gt; 12:     and uses the highly optimized `fill` method to reset the distances array between
&gt; 13:     BFS runs. This avoids the overhead of memory allocation within the main loop.
&gt; 14:     &quot;&quot;&quot;
&gt; 15:     total_inverse_distance = 0.0
&gt; 16: 
&gt; 17:     # --- Pre-allocate memory outside the hot loop ---
&gt; 18:     # This is a key optimization to avoid repeated allocation/deallocation overhead.
&gt; 19:     distances = np.empty(n, dtype=np.int32)
&gt; 20:     queue = np.empty(n, dtype=np.int32)
&gt; 21: 
&gt; 22:     # A serial loop over all possible starting nodes.
&gt; 23:     for i in range(n):
&gt; 24:         # --- Efficient Reset using `fill` ---
&gt; 25:         # `fill` is a highly optimized method for setting all elements of an array.
&gt; 26:         # This is faster than re-allocating a new array with `np.full` in every iteration.
&gt; 27:         distances.fill(-1)
&gt; 28:         
&gt; 29:         # Start BFS from the current node `i`.
&gt; 30:         head = 0
&gt; 31:         tail = 0
&gt; 32: 
&gt; 33:         distances[i] = 0
&gt; 34:         queue[tail] = i
&gt; 35:         tail += 1
&gt; 36:         
&gt; 37:         while head &lt; tail:
&gt; 38:             u = queue[head]
&gt; 39:             head += 1
&gt; 40: 
&gt; 41:             # Iterate through neighbors of u using the efficient flattened adjacency list.
&gt; 42:             start = offsets[u]
&gt; 43:             end = offsets[u+1]
&gt; 44:             for j in range(start, end):
&gt; 45:                 v = flat_adj[j]
&gt; 46:                 if distances[v] == -1:
&gt; 47:                     dist = distances[u] + 1
&gt; 48:                     distances[v] = dist
&gt; 49:                     total_inverse_distance += 1.0 / dist
&gt; 50:                     queue[tail] = v
...</code></pre><br><br>Starting evaluation...<br><br>Speedup: <span style="color:#006400; font-weight:600;">15.02</span>x<br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 100%<br>  Invalid Solutions: 0%<br>  Timeouts: 0%<br><br><br>Snapshot saved Best speedup reached, state saved!<br>Among the 10+ LLMs we tested, your code did not rank in the top 3 for speed. Please use all available packages and tools to optimize its performance. Think outside the box!
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My best performance of 15.02x was achieved with a highly optimized serial Numba function. The parallel Numba attempt (`prange`) was unstable and crashed, while the SciPy approach was too slow due to data conversion and memory allocation overhead.<br><br>The feedback indicates that a much higher speedup is possible, which points towards successful parallelism. The crashes with `numba.prange` suggest that its threading model is conflicting with the evaluation environment&#x27;s resource limits.<br><br>My new strategy is to use a more robust parallelism model: Python&#x27;s built-in `multiprocessing` library. This approach uses separate processes instead of threads, which provides memory isolation and avoids issues related to Python&#x27;s Global Interpreter Lock (GIL) and thread-level resource contention.<br><br>The plan is as follows:<br>1.  **Hybrid Parallelism**: I will combine the strengths of `multiprocessing` and `numba`. The overall problem (running `n` independent BFS traversals) will be split across multiple processes using a `multiprocessing.Pool`.<br>2.  **Optimized Kernel**: Each worker process will execute my fastest serial Numba kernel on its assigned subset of nodes. This leverages the JIT-compiled speed for the core computation.<br>3.  **Efficient Data Handling**: To minimize the overhead of sending the large graph data to each worker process, I will use a `Pool` initializer function. On Linux-based systems (common in evaluation environments), this leverages `fork()` to make the large NumPy arrays available to child processes via copy-on-write memory, which is extremely efficient.<br>4.  **Dynamic Work Division**: The code will dynamically determine the number of available CPU cores and divide the `n` BFS tasks into appropriately sized chunks for each worker.<br><br>This architecture is a standard pattern for high-performance scientific computing in Python. It is robust, scalable, and should overcome the stability issues of the threaded approach while unlocking significant parallel speedup.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np
from multiprocessing import Pool, cpu_count

# This is the fast serial Numba kernel, now parameterized to work on a sub-problem.
# It will be executed in parallel by multiple processes.
@numba.njit(cache=True, fastmath=True)
def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of inverse shortest path lengths for a given range of start nodes.
    This is the core computational kernel executed by each worker process. It uses the
    fastest known serial implementation.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # Pre-allocate memory once to be reused by all BFS runs in this worker.
    distances = np.empty(n, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    for i in range(start_node, end_node):
        # Use the highly optimized `fill` method to reset the distances array.
        distances.fill(-1)
        
        head = 0
        tail = 0
        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        while head &lt; tail:
            u = queue[head]
            head += 1
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
    return total_inverse_distance

# Global dictionary to hold data for worker processes. This avoids passing large
# arrays as arguments, leveraging copy-on-write memory sharing when processes are forked.
_worker_data = {}

def _init_worker(n, offsets, flat_adj):
    &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
    _worker_data[&#x27;n&#x27;] = n
    _worker_data[&#x27;offsets&#x27;] = offsets
    _worker_data[&#x27;flat_adj&#x27;] = flat_adj

def _worker_func(task_range):
    &quot;&quot;&quot;The function each worker process executes on its assigned range of nodes.&quot;&quot;&quot;
    start_node, end_node = task_range
    return _calculate_partial_inverse_distance(
        _worker_data[&#x27;n&#x27;],
        _worker_data[&#x27;offsets&#x27;],
        _worker_data[&#x27;flat_adj&#x27;],
        start_node,
        end_node
    )

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates global efficiency using a hybrid parallel approach.
        The problem is split across multiple processes, each running a fast Numba kernel.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Prepare the flattened adjacency list for efficient processing.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Determine the number of processes and create task chunks.
        try:
            num_processes = cpu_count()
        except NotImplementedError:
            num_processes = 4  # Fallback if cpu_count is not available.
        
        num_processes = min(num_processes, n) if n &gt; 0 else 1

        chunk_size = (n + num_processes - 1) // num_processes  # Ceiling division
        tasks = []
        for i in range(num_processes):
            start = i * chunk_size
            end = min((i + 1) * chunk_size, n)
            if start &lt; end:
                tasks.append((start, end))

        # Use a multiprocessing Pool to parallelize the work.
        # The initializer passes the large numpy arrays to each worker once efficiently.
        with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
            partial_sums = pool.map(_worker_func, tasks)

        total_inverse_distance = sum(partial_sums)
        
        denominator = n * (n - 1)
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 11 messages and have used up $0.5323. You have $0.4677 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 117)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: from typing import Any
&gt; 002: import numba
&gt; 003: import numpy as np
&gt; 004: from multiprocessing import Pool, cpu_count
&gt; 005: 
&gt; 006: # This is the fast serial Numba kernel, now parameterized to work on a sub-problem.
&gt; 007: # It will be executed in parallel by multiple processes.
&gt; 008: @numba.njit(cache=True, fastmath=True)
&gt; 009: def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
&gt; 010:     &quot;&quot;&quot;
&gt; 011:     Calculates the sum of inverse shortest path lengths for a given range of start nodes.
&gt; 012:     This is the core computational kernel executed by each worker process. It uses the
&gt; 013:     fastest known serial implementation.
&gt; 014:     &quot;&quot;&quot;
&gt; 015:     total_inverse_distance = 0.0
&gt; 016:     # Pre-allocate memory once to be reused by all BFS runs in this worker.
&gt; 017:     distances = np.empty(n, dtype=np.int32)
&gt; 018:     queue = np.empty(n, dtype=np.int32)
&gt; 019: 
&gt; 020:     for i in range(start_node, end_node):
&gt; 021:         # Use the highly optimized `fill` method to reset the distances array.
&gt; 022:         distances.fill(-1)
&gt; 023:         
&gt; 024:         head = 0
&gt; 025:         tail = 0
&gt; 026:         distances[i] = 0
&gt; 027:         queue[tail] = i
&gt; 028:         tail += 1
&gt; 029:         
&gt; 030:         while head &lt; tail:
&gt; 031:             u = queue[head]
&gt; 032:             head += 1
&gt; 033:             start = offsets[u]
&gt; 034:             end = offsets[u+1]
&gt; 035:             for j in range(start, end):
&gt; 036:                 v = flat_adj[j]
&gt; 037:                 if distances[v] == -1:
&gt; 038:                     dist = distances[u] + 1
&gt; 039:                     distances[v] = dist
&gt; 040:                     total_inverse_distance += 1.0 / dist
&gt; 041:                     queue[tail] = v
&gt; 042:                     tail += 1
&gt; 043:     return total_inverse_distance
&gt; 044: 
&gt; 045: # Global dictionary to hold data for worker processes. This avoids passing large
&gt; 046: # arrays as arguments, leveraging copy-on-write memory sharing when processes are forked.
&gt; 047: _worker_data = {}
&gt; 048: 
&gt; 049: def _init_worker(n, offsets, flat_adj):
&gt; 050:     &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
...</code></pre><br><br>Speedup: <span style="color:#B22222; font-weight:600;">N/A</span><br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 0%<br>  Invalid Solutions: 0%<br>  Timeouts: 100%<br><br><br>Snapshot not saved - invalid solutions present
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The `multiprocessing` approach timed out, likely due to the high overhead of spawning new processes and the JIT compilation occurring in each worker. The `numba.prange` approach was conceptually the fastest but crashed, indicating instability in the threading model, probably from resource contention.<br><br>My best stable performance came from a serial Numba kernel (15.02x). To surpass this, I must make the parallel Numba approach work reliably. The previous crashes were likely caused by either using too many threads or contention during the reduction (summation) step.<br><br>My new strategy is to reimplement the `numba.prange` solution with two critical changes to ensure stability:<br>1.  **Fixed, Conservative Thread Count**: I will explicitly limit the number of threads to a small, safe number (e.g., 4). Instead of using `numba.set_num_threads()`, which can be fragile, I will use the `os.environ[&#x27;NUMBA_NUM_THREADS&#x27;]` method, which is a more robust way to configure Numba&#x27;s thread pool before any functions are compiled.<br>2.  **Explicit Thread-Safe Reduction**: Instead of relying on Numba&#x27;s automatic handling of the `+=` reduction on a single variable, I will implement a more robust pattern. I&#x27;ll create a NumPy array (`partial_sums`) where each thread can accumulate its results into its own private slot, identified by `numba.get_thread_id()`. This completely eliminates any possibility of race conditions or contention on the sum variable. The final result is a simple sum of this array after the parallel loop finishes.<br><br>This approach directly addresses the likely causes of the previous crashes, combining the raw speed of a JIT-compiled kernel with a stable, explicitly managed parallel execution model.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np
import os

# --- Key Optimization: Robust Parallelism Control ---
# Set a fixed, conservative number of threads via an environment variable before
# Numba compiles anything. This is more robust than calling set_num_threads().
# 4 is a safe choice that provides significant parallelism without overwhelming
# typical containerized evaluation environments.
NUM_THREADS = 4
os.environ[&#x27;NUMBA_NUM_THREADS&#x27;] = str(NUM_THREADS)

@numba.njit(parallel=True, cache=True)
def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list. The outer loop is parallelized across a controlled number of threads.
    This version uses a temporary array for reduction to ensure thread safety and stability.
    &quot;&quot;&quot;
    # Each thread will accumulate its sum into its own slot in this array. This is a
    # robust pattern for parallel reductions, avoiding contention on a single variable.
    partial_sums = np.zeros(NUM_THREADS, dtype=np.float64)

    # The outer loop is parallelized using numba.prange.
    # pylint: disable=not-an-iterable
    for i in numba.prange(n):
        # Get the unique ID for the current thread.
        tid = numba.get_thread_id()

        # Each thread gets its own private data structures for its BFS instance.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    # Add to the thread-local sum.
                    partial_sums[tid] += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    # After the parallel loop, sum the results from all threads.
    return np.sum(partial_sums)

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a robust, parallelized Numba kernel.
        The number of threads is explicitly controlled, and a safe reduction pattern is used
        to ensure stability and performance.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance parallel Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 12 messages and have used up $0.5960. You have $0.4040 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 112)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: from typing import Any
&gt; 002: import numba
&gt; 003: import numpy as np
&gt; 004: import os
&gt; 005: 
&gt; 006: # --- Key Optimization: Robust Parallelism Control ---
&gt; 007: # Set a fixed, conservative number of threads via an environment variable before
&gt; 008: # Numba compiles anything. This is more robust than calling set_num_threads().
&gt; 009: # 4 is a safe choice that provides significant parallelism without overwhelming
&gt; 010: # typical containerized evaluation environments.
&gt; 011: NUM_THREADS = 4
&gt; 012: os.environ[&#x27;NUMBA_NUM_THREADS&#x27;] = str(NUM_THREADS)
&gt; 013: 
&gt; 014: @numba.njit(parallel=True, cache=True)
&gt; 015: def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 016:     &quot;&quot;&quot;
&gt; 017:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 018:     adjacency list. The outer loop is parallelized across a controlled number of threads.
&gt; 019:     This version uses a temporary array for reduction to ensure thread safety and stability.
&gt; 020:     &quot;&quot;&quot;
&gt; 021:     # Each thread will accumulate its sum into its own slot in this array. This is a
&gt; 022:     # robust pattern for parallel reductions, avoiding contention on a single variable.
&gt; 023:     partial_sums = np.zeros(NUM_THREADS, dtype=np.float64)
&gt; 024: 
&gt; 025:     # The outer loop is parallelized using numba.prange.
&gt; 026:     # pylint: disable=not-an-iterable
&gt; 027:     for i in numba.prange(n):
&gt; 028:         # Get the unique ID for the current thread.
&gt; 029:         tid = numba.get_thread_id()
&gt; 030: 
&gt; 031:         # Each thread gets its own private data structures for its BFS instance.
&gt; 032:         distances = np.full(n, -1, dtype=np.int32)
&gt; 033:         queue = np.empty(n, dtype=np.int32)
&gt; 034:         head = 0
&gt; 035:         tail = 0
&gt; 036: 
&gt; 037:         # Start BFS from the current node `i`.
&gt; 038:         distances[i] = 0
&gt; 039:         queue[tail] = i
&gt; 040:         tail += 1
&gt; 041: 
&gt; 042:         while head &lt; tail:
&gt; 043:             u = queue[head]
&gt; 044:             head += 1
&gt; 045: 
&gt; 046:             start = offsets[u]
&gt; 047:             end = offsets[u+1]
&gt; 048:             for j in range(start, end):
&gt; 049:                 v = flat_adj[j]
&gt; 050:                 if distances[v] == -1:
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 89 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   79:         current_pos = 0
   80:         for i, neighbors in enumerate(adj_list):
   81:             offsets[i] = current_pos
   82:             num_neighbors = len(neighbors)
   83:             if num_neighbors &gt; 0:
   84:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
   85:             current_pos += num_neighbors
   86:         offsets[n] = current_pos
   87: 
   88:         # Call the high-performance parallel Numba function.
 ! 89:         total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj)
   90: 
   91:         denominator = n * (n - 1)
   92: 
   93:         global_efficiency = total_inverse_distance / denominator
   94: 
   95:         return {&quot;global_efficiency&quot;: global_efficiency}
   96:         for i in range(num_processes):
   97:             start = i * chunk_size
   98:             end = min((i + 1) * chunk_size, n)
   99:             if start &lt; end:</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The `RuntimeError` indicates a conflict in configuring the number of Numba threads. My previous code hardcoded the thread count to 4 using `os.environ`, but the evaluation environment is attempting to set it to a different value (2), likely based on the allocated CPU resources. This conflict occurs because the thread pool cannot be reconfigured after it has been initialized.<br><br>The correct approach is not to fight the environment but to adapt to it. My hardcoding of the thread count was the source of the error.<br><br>My new strategy is to:<br>1.  **Remove Hardcoded Configuration**: I will remove the `os.environ[&#x27;NUMBA_NUM_THREADS&#x27;] = &#x27;4&#x27;` line. This will allow the evaluation environment to set the thread count without conflict.<br>2.  **Dynamically Query Thread Count**: To maintain my robust, thread-safe reduction mechanism, I need to know how many threads Numba will use. I can get this information by calling `numba.get_num_threads()` from the main Python code.<br>3.  **Parameterize the JIT Function**: I will pass the dynamically obtained thread count as an argument to my parallel Numba function.<br>4.  **Correctly Size the Reduction Array**: Inside the JIT function, the `partial_sums` array will be sized using the thread count passed as a parameter, ensuring it&#x27;s always correct for the current environment.<br><br>This solution resolves the `RuntimeError` by adapting to the environment&#x27;s configuration while preserving the stable and performant parallel reduction pattern that prevents race conditions.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This function is JIT-compiled with Numba for performance.
# It is parameterized by the number of threads to adapt to any environment.
@numba.njit(parallel=True, cache=True)
def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray, num_threads: int) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
    adjacency list. The outer loop is parallelized across a number of threads determined
    by the environment. A temporary array is used for reduction to ensure thread safety.
    &quot;&quot;&quot;
    # Each thread accumulates its sum into its own slot in this array. The array size
    # is determined dynamically, making the function adaptable and robust.
    partial_sums = np.zeros(num_threads, dtype=np.float64)

    # The outer loop is parallelized using numba.prange.
    # pylint: disable=not-an-iterable
    for i in numba.prange(n):
        # Get the unique ID for the current thread.
        tid = numba.get_thread_id()

        # Each thread gets its own private data structures for its BFS instance.
        # This allocation inside the parallel loop is safe as it&#x27;s private to the thread&#x27;s iteration.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        head = 0
        tail = 0

        # Start BFS from the current node `i`.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    # Add to the thread-local sum.
                    partial_sums[tid] += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    # After the parallel loop, sum the results from all threads.
    return np.sum(partial_sums)

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a robust, parallelized Numba kernel.
        The number of threads is determined dynamically from the environment to avoid conflicts.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # --- Key Fix: Adapt to the environment&#x27;s thread count ---
        # Get the number of threads Numba is configured to use, rather than setting it ourselves.
        num_threads = numba.get_num_threads()

        # Call the high-performance parallel Numba function, passing the thread count.
        total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj, num_threads)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 13 messages and have used up $0.6473. You have $0.3527 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 104)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: from typing import Any
&gt; 002: import numba
&gt; 003: import numpy as np
&gt; 004: 
&gt; 005: # This function is JIT-compiled with Numba for performance.
&gt; 006: # It is parameterized by the number of threads to adapt to any environment.
&gt; 007: @numba.njit(parallel=True, cache=True)
&gt; 008: def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray, num_threads: int) -&gt; float:
&gt; 009:     &quot;&quot;&quot;
&gt; 010:     Calculates the sum of the inverse of all-pairs shortest path lengths using a flattened
&gt; 011:     adjacency list. The outer loop is parallelized across a number of threads determined
&gt; 012:     by the environment. A temporary array is used for reduction to ensure thread safety.
&gt; 013:     &quot;&quot;&quot;
&gt; 014:     # Each thread accumulates its sum into its own slot in this array. The array size
&gt; 015:     # is determined dynamically, making the function adaptable and robust.
&gt; 016:     partial_sums = np.zeros(num_threads, dtype=np.float64)
&gt; 017: 
&gt; 018:     # The outer loop is parallelized using numba.prange.
&gt; 019:     # pylint: disable=not-an-iterable
&gt; 020:     for i in numba.prange(n):
&gt; 021:         # Get the unique ID for the current thread.
&gt; 022:         tid = numba.get_thread_id()
&gt; 023: 
&gt; 024:         # Each thread gets its own private data structures for its BFS instance.
&gt; 025:         # This allocation inside the parallel loop is safe as it&#x27;s private to the thread&#x27;s iteration.
&gt; 026:         distances = np.full(n, -1, dtype=np.int32)
&gt; 027:         queue = np.empty(n, dtype=np.int32)
&gt; 028:         head = 0
&gt; 029:         tail = 0
&gt; 030: 
&gt; 031:         # Start BFS from the current node `i`.
&gt; 032:         distances[i] = 0
&gt; 033:         queue[tail] = i
&gt; 034:         tail += 1
&gt; 035: 
&gt; 036:         while head &lt; tail:
&gt; 037:             u = queue[head]
&gt; 038:             head += 1
&gt; 039: 
&gt; 040:             start = offsets[u]
&gt; 041:             end = offsets[u+1]
&gt; 042:             for j in range(start, end):
&gt; 043:                 v = flat_adj[j]
&gt; 044:                 if distances[v] == -1:
&gt; 045:                     dist = distances[u] + 1
&gt; 046:                     distances[v] = dist
&gt; 047:                     # Add to the thread-local sum.
&gt; 048:                     partial_sums[tid] += 1.0 / dist
&gt; 049:                     queue[tail] = v
&gt; 050:                     tail += 1
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 86 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   76:             if num_neighbors &gt; 0:
   77:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
   78:             current_pos += num_neighbors
   79:         offsets[n] = current_pos
   80: 
   81:         # --- Key Fix: Adapt to the environment&#x27;s thread count ---
   82:         # Get the number of threads Numba is configured to use, rather than setting it ourselves.
   83:         num_threads = numba.get_num_threads()
   84: 
   85:         # Call the high-performance parallel Numba function, passing the thread count.
 ! 86:         total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj, num_threads)
   87: 
   88:         denominator = n * (n - 1)
   89: 
   90:         global_efficiency = total_inverse_distance / denominator
   91: 
   92:         return {&quot;global_efficiency&quot;: global_efficiency}
   93: 
   94:         # Use a multiprocessing Pool to parallelize the work.
   95:         # The initializer passes the large numpy arrays to each worker once efficiently.
   96:         with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The `RuntimeError` is a dead giveaway: my attempts to manually control or even query the number of Numba threads are conflicting with the evaluation environment&#x27;s own resource management. The environment sets the thread count, and my code&#x27;s interaction with Numba triggers a re-configuration attempt, which fails because the thread pool is already active.<br><br>The only way forward is to completely cede control of threading to the environment.<br><br>My new strategy is to revert to the simplest possible parallel Numba implementation, removing all of my previous attempts at manual thread management that caused these conflicts.<br><br>**The Plan:**<br>1.  **No Thread Management**: I will remove all code related to `os.environ`, `numba.get_num_threads()`, and my manual `partial_sums` reduction array.<br>2.  **Trust Numba&#x27;s Automatic Reduction**: I will use a simple `parallel=True` decorator on the JIT function and a `numba.prange` loop. The reduction (`total_inverse_distance += ...`) will be done directly. Numba is designed to automatically and safely handle this for `prange` by using thread-local accumulators.<br>3.  **Private Loop Variables**: The `distances` and `queue` arrays will be allocated inside the `prange` loop. This is essential for correctness, as each parallel iteration needs its own private copies to avoid race conditions.<br><br>This approach is a direct response to the `RuntimeError`. By removing all my conflicting code, I am letting Numba operate exactly as the environment expects. This should resolve the error and finally unlock the performance of a stable, parallel solution.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This is the simplest, most robust parallel Numba implementation.
# It cedes all thread control to the environment, avoiding configuration conflicts.
@numba.njit(parallel=True, cache=True)
def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths in parallel.
    It relies on Numba&#x27;s default thread management and automatic reduction, which is
    the most robust way to achieve parallelism in a controlled environment.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # The outer loop is parallelized using numba.prange.
    # Numba automatically handles the reduction for `total_inverse_distance` in a thread-safe manner.
    # pylint: disable=not-an-iterable
    for i in numba.prange(n):
        # Each parallel iteration requires its own private data structures for the BFS.
        # Allocating inside the loop is necessary for thread safety.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        
        head = 0
        tail = 0
        distances[i] = 0
        queue[tail] = i
        tail += 1

        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    # This reduction is automatically made thread-safe by Numba for prange.
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a robust, parallelized Numba kernel.
        This implementation lets the environment control threading to ensure stability.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the high-performance parallel Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 14 messages and have used up $0.7046. You have $0.2954 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 87)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This is the simplest, most robust parallel Numba implementation.
&gt; 06: # It cedes all thread control to the environment, avoiding configuration conflicts.
&gt; 07: @numba.njit(parallel=True, cache=True)
&gt; 08: def _calculate_total_inverse_distance_parallel(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 09:     &quot;&quot;&quot;
&gt; 10:     Calculates the sum of the inverse of all-pairs shortest path lengths in parallel.
&gt; 11:     It relies on Numba&#x27;s default thread management and automatic reduction, which is
&gt; 12:     the most robust way to achieve parallelism in a controlled environment.
&gt; 13:     &quot;&quot;&quot;
&gt; 14:     total_inverse_distance = 0.0
&gt; 15: 
&gt; 16:     # The outer loop is parallelized using numba.prange.
&gt; 17:     # Numba automatically handles the reduction for `total_inverse_distance` in a thread-safe manner.
&gt; 18:     # pylint: disable=not-an-iterable
&gt; 19:     for i in numba.prange(n):
&gt; 20:         # Each parallel iteration requires its own private data structures for the BFS.
&gt; 21:         # Allocating inside the loop is necessary for thread safety.
&gt; 22:         distances = np.full(n, -1, dtype=np.int32)
&gt; 23:         queue = np.empty(n, dtype=np.int32)
&gt; 24:         
&gt; 25:         head = 0
&gt; 26:         tail = 0
&gt; 27:         distances[i] = 0
&gt; 28:         queue[tail] = i
&gt; 29:         tail += 1
&gt; 30: 
&gt; 31:         while head &lt; tail:
&gt; 32:             u = queue[head]
&gt; 33:             head += 1
&gt; 34: 
&gt; 35:             start = offsets[u]
&gt; 36:             end = offsets[u+1]
&gt; 37:             for j in range(start, end):
&gt; 38:                 v = flat_adj[j]
&gt; 39:                 if distances[v] == -1:
&gt; 40:                     dist = distances[u] + 1
&gt; 41:                     distances[v] = dist
&gt; 42:                     # This reduction is automatically made thread-safe by Numba for prange.
&gt; 43:                     total_inverse_distance += 1.0 / dist
&gt; 44:                     queue[tail] = v
&gt; 45:                     tail += 1
&gt; 46:                     
&gt; 47:     return total_inverse_distance
&gt; 48: 
&gt; 49: class Solver:
&gt; 50:     def solve(self, problem: dict, **kwargs) -&gt; Any:
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 77 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   67:         current_pos = 0
   68:         for i, neighbors in enumerate(adj_list):
   69:             offsets[i] = current_pos
   70:             num_neighbors = len(neighbors)
   71:             if num_neighbors &gt; 0:
   72:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
   73:             current_pos += num_neighbors
   74:         offsets[n] = current_pos
   75: 
   76:         # Call the high-performance parallel Numba function.
 ! 77:         total_inverse_distance = _calculate_total_inverse_distance_parallel(n, offsets, flat_adj)
   78: 
   79:         denominator = n * (n - 1)
   80: 
   81:         global_efficiency = total_inverse_distance / denominator
   82: 
   83:         return {&quot;global_efficiency&quot;: global_efficiency}
   84:         denominator = n * (n - 1)
   85:         global_efficiency = total_inverse_distance / denominator
   86: 
   87:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The recurring `RuntimeError` is a classic sign of a stateful conflict. The evaluation environment is running multiple tests within the same process but with different configurations (e.g., changing the allowed number of threads from 4 to 2). My previous attempts using Numba&#x27;s `parallel=True` failed because Numba initializes its thread pool once and cannot change it, leading to a crash on the second test run.<br><br>The `multiprocessing` approach avoids this by using separate, isolated processes, but it previously timed out. The timeout was almost certainly caused by the overhead of each new worker process having to JIT-compile the Numba function from scratch.<br><br>My new strategy combines the best of both worlds: the isolation of `multiprocessing` and the speed of a pre-compiled Numba kernel.<br><br>**The Plan:**<br>1.  **Revert to `multiprocessing`**: This is the only robust way to guarantee isolation between test runs and avoid the `RuntimeError`.<br>2.  **Pre-compile the Numba Kernel**: Before creating the `multiprocessing.Pool`, I will perform a &quot;warm-up&quot; call to the Numba-jitted function. This call will use trivial, dummy data. Its sole purpose is to trigger the expensive JIT compilation *once* in the main process.<br>3.  **Leverage `fork()`**: When the `Pool` creates worker processes (on Linux systems), they will be forked from the main process. This means they inherit the memory space, including the *already compiled* Numba function. This completely eliminates the JIT compilation overhead in the workers.<br>4.  **Class-level Flag**: I&#x27;ll use a class-level flag (`_is_jitted`) to ensure this warm-up compilation happens only once during the entire lifetime of the parent process, making it efficient across multiple calls to `solve`.<br><br>This approach directly targets the root causes of my previous failures: it uses process isolation to solve the stateful `RuntimeError` and pre-compilation to solve the `multiprocessing` timeout.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np
from multiprocessing import Pool, cpu_count

# This is the fast serial Numba kernel. It will be pre-compiled in the main
# process and then executed in parallel by multiple worker processes.
@numba.njit(cache=True, fastmath=True)
def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of inverse shortest path lengths for a given range of start nodes.
    This is the core computational kernel executed by each worker process.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # Pre-allocate memory once to be reused by all BFS runs in this worker.
    distances = np.empty(n, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    for i in range(start_node, end_node):
        distances.fill(-1)
        head = 0
        tail = 0
        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        while head &lt; tail:
            u = queue[head]
            head += 1
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
    return total_inverse_distance

# Global dictionary to hold data for worker processes, avoiding expensive serialization.
_worker_data = {}

def _init_worker(n, offsets, flat_adj):
    &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
    _worker_data[&#x27;n&#x27;] = n
    _worker_data[&#x27;offsets&#x27;] = offsets
    _worker_data[&#x27;flat_adj&#x27;] = flat_adj

def _worker_func(task_range):
    &quot;&quot;&quot;The function each worker process executes on its assigned range of nodes.&quot;&quot;&quot;
    start_node, end_node = task_range
    return _calculate_partial_inverse_distance(
        _worker_data[&#x27;n&#x27;],
        _worker_data[&#x27;offsets&#x27;],
        _worker_data[&#x27;flat_adj&#x27;],
        start_node,
        end_node
    )

class Solver:
    # Class-level flag to ensure JIT compilation happens only once per process.
    _is_jitted = False

    def solve(self, problem: dict, **kwargs) -&gt; Any:
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # --- Key Fix: Pre-compile the Numba function before forking ---
        # This &quot;warm-up&quot; call ensures the expensive JIT compilation happens only
        # once in the main process. Child processes will inherit the compiled code.
        if not Solver._is_jitted:
            dummy_offsets = np.zeros(2, dtype=np.int32)
            dummy_adj = np.zeros(1, dtype=np.int32)
            _calculate_partial_inverse_distance(1, dummy_offsets, dummy_adj, 0, 0)
            Solver._is_jitted = True

        # Prepare the flattened adjacency list.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Determine the number of processes and create task chunks.
        try:
            num_processes = cpu_count()
        except NotImplementedError:
            num_processes = 2 # A safe fallback
        
        num_processes = min(num_processes, n) if n &gt; 0 else 1

        chunk_size = (n + num_processes - 1) // num_processes
        tasks = [(i * chunk_size, min((i + 1) * chunk_size, n)) for i in range(num_processes)]
        tasks = [t for t in tasks if t[0] &lt; t[1]] # Filter out empty tasks

        # Use a multiprocessing Pool to parallelize the work.
        with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
            partial_sums = pool.map(_worker_func, tasks)

        total_inverse_distance = sum(partial_sums)
        
        denominator = n * (n - 1)
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 15 messages and have used up $0.7643. You have $0.2357 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 116)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: from typing import Any
&gt; 002: import numba
&gt; 003: import numpy as np
&gt; 004: from multiprocessing import Pool, cpu_count
&gt; 005: 
&gt; 006: # This is the fast serial Numba kernel. It will be pre-compiled in the main
&gt; 007: # process and then executed in parallel by multiple worker processes.
&gt; 008: @numba.njit(cache=True, fastmath=True)
&gt; 009: def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
&gt; 010:     &quot;&quot;&quot;
&gt; 011:     Calculates the sum of inverse shortest path lengths for a given range of start nodes.
&gt; 012:     This is the core computational kernel executed by each worker process.
&gt; 013:     &quot;&quot;&quot;
&gt; 014:     total_inverse_distance = 0.0
&gt; 015:     # Pre-allocate memory once to be reused by all BFS runs in this worker.
&gt; 016:     distances = np.empty(n, dtype=np.int32)
&gt; 017:     queue = np.empty(n, dtype=np.int32)
&gt; 018: 
&gt; 019:     for i in range(start_node, end_node):
&gt; 020:         distances.fill(-1)
&gt; 021:         head = 0
&gt; 022:         tail = 0
&gt; 023:         distances[i] = 0
&gt; 024:         queue[tail] = i
&gt; 025:         tail += 1
&gt; 026:         
&gt; 027:         while head &lt; tail:
&gt; 028:             u = queue[head]
&gt; 029:             head += 1
&gt; 030:             start = offsets[u]
&gt; 031:             end = offsets[u+1]
&gt; 032:             for j in range(start, end):
&gt; 033:                 v = flat_adj[j]
&gt; 034:                 if distances[v] == -1:
&gt; 035:                     dist = distances[u] + 1
&gt; 036:                     distances[v] = dist
&gt; 037:                     total_inverse_distance += 1.0 / dist
&gt; 038:                     queue[tail] = v
&gt; 039:                     tail += 1
&gt; 040:     return total_inverse_distance
&gt; 041: 
&gt; 042: # Global dictionary to hold data for worker processes, avoiding expensive serialization.
&gt; 043: _worker_data = {}
&gt; 044: 
&gt; 045: def _init_worker(n, offsets, flat_adj):
&gt; 046:     &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
&gt; 047:     _worker_data[&#x27;n&#x27;] = n
&gt; 048:     _worker_data[&#x27;offsets&#x27;] = offsets
&gt; 049:     _worker_data[&#x27;flat_adj&#x27;] = flat_adj
&gt; 050: 
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 79 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   69: 
   70:         if n &lt;= 1:
   71:             return {&quot;global_efficiency&quot;: 0.0}
   72: 
   73:         # --- Key Fix: Pre-compile the Numba function before forking ---
   74:         # This &quot;warm-up&quot; call ensures the expensive JIT compilation happens only
   75:         # once in the main process. Child processes will inherit the compiled code.
   76:         if not Solver._is_jitted:
   77:             dummy_offsets = np.zeros(2, dtype=np.int32)
   78:             dummy_adj = np.zeros(1, dtype=np.int32)
 ! 79:             _calculate_partial_inverse_distance(1, dummy_offsets, dummy_adj, 0, 0)
   80:             Solver._is_jitted = True
   81: 
   82:         # Prepare the flattened adjacency list.
   83:         offsets = np.zeros(n + 1, dtype=np.int32)
   84:         num_edges = sum(len(neighbors) for neighbors in adj_list)
   85:         flat_adj = np.empty(num_edges, dtype=np.int32)
   86:         current_pos = 0
   87:         for i, neighbors in enumerate(adj_list):
   88:             offsets[i] = current_pos
   89:             num_neighbors = len(neighbors)</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The `RuntimeError` is exceptionally stubborn. It indicates a stateful conflict where the evaluation environment runs multiple tests within the same parent process but tries to change Numba&#x27;s thread configuration between tests. My attempts to use Numba&#x27;s `parallel=True` failed because Numba initializes its thread pool once and cannot change it.<br><br>My last attempt with `multiprocessing` also failed with the same error, even though it was designed to isolate processes. The error occurred during my &quot;warm-up&quot; JIT compilation call in the main process. This proves that *any* JIT compilation, even for a serial function, initializes parts of Numba&#x27;s backend in the parent process, which then conflicts with subsequent tests.<br><br>The only truly robust solution is to use `multiprocessing` to guarantee process isolation and to **not perform any JIT compilation in the parent process**.<br><br>**The Corrected Strategy:**<br>1.  **Use `multiprocessing`**: This is non-negotiable. It&#x27;s the only way to get a clean state for each test run and avoid the `RuntimeError`.<br>2.  **Remove All JIT from the Parent Process**: I will remove my faulty &quot;warm-up&quot; call. The parent process will not import or call any Numba-jitted functions.<br>3.  **JIT Compilation in Workers**: The JIT compilation of the kernel function will occur independently inside each worker process when it&#x27;s first called. While this adds a one-time overhead *per worker*, it is a clean, isolated operation that cannot cause state conflicts. The previous timeouts were likely caused by my flawed attempts to fix a non-existent problem, which ironically introduced the fatal error.<br>4.  **Efficient Data Transfer**: The `initializer` pattern with `fork()` remains the most efficient way to share the large graph data with workers, minimizing serialization overhead.<br><br>This is the pure, correct implementation of the `multiprocessing` pattern for this problem. It directly addresses the `RuntimeError` by ensuring the conflicting code never runs in the shared parent process.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np
from multiprocessing import Pool, cpu_count

# This is the fast serial Numba kernel. It will be JIT-compiled inside each
# worker process, avoiding stateful conflicts in the main process.
@numba.njit(cache=True, fastmath=True)
def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of inverse shortest path lengths for a given range of start nodes.
    This is the core computational kernel executed by each worker process.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # Pre-allocate memory once to be reused by all BFS runs in this worker.
    distances = np.empty(n, dtype=np.int32)
    queue = np.empty(n, dtype=np.int32)

    for i in range(start_node, end_node):
        distances.fill(-1)
        head = 0
        tail = 0
        distances[i] = 0
        queue[tail] = i
        tail += 1
        
        while head &lt; tail:
            u = queue[head]
            head += 1
            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
    return total_inverse_distance

# Global dictionary to hold data for worker processes, avoiding expensive serialization.
_worker_data = {}

def _init_worker(n, offsets, flat_adj):
    &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
    _worker_data[&#x27;n&#x27;] = n
    _worker_data[&#x27;offsets&#x27;] = offsets
    _worker_data[&#x27;flat_adj&#x27;] = flat_adj

def _worker_func(task_range):
    &quot;&quot;&quot;The function each worker process executes on its assigned range of nodes.&quot;&quot;&quot;
    start_node, end_node = task_range
    # The JIT compilation of _calculate_partial_inverse_distance happens here,
    # safely inside the isolated worker process.
    return _calculate_partial_inverse_distance(
        _worker_data[&#x27;n&#x27;],
        _worker_data[&#x27;offsets&#x27;],
        _worker_data[&#x27;flat_adj&#x27;],
        start_node,
        end_node
    )

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates global efficiency using a robust multiprocessing approach.
        This isolates each test run, avoiding Numba&#x27;s thread configuration errors,
        while parallelizing the computation across multiple CPU cores.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Prepare the flattened adjacency list for efficient processing.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Determine the number of processes and create task chunks.
        try:
            num_processes = cpu_count()
        except NotImplementedError:
            num_processes = 2 # A safe fallback.
        
        num_processes = min(num_processes, n) if n &gt; 0 else 1

        chunk_size = (n + num_processes - 1) // num_processes
        tasks = [(i * chunk_size, min((i + 1) * chunk_size, n)) for i in range(num_processes)]
        tasks = [t for t in tasks if t[0] &lt; t[1]] # Filter out empty tasks

        # Use a multiprocessing Pool to parallelize the work.
        with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
            partial_sums = pool.map(_worker_func, tasks)

        total_inverse_distance = sum(partial_sums)
        
        denominator = n * (n - 1)
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 16 messages and have used up $0.8289. You have $0.1711 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 127)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 001: from typing import Any
&gt; 002: import numba
&gt; 003: import numpy as np
&gt; 004: from multiprocessing import Pool, cpu_count
&gt; 005: 
&gt; 006: # This is the fast serial Numba kernel. It will be JIT-compiled inside each
&gt; 007: # worker process, avoiding stateful conflicts in the main process.
&gt; 008: @numba.njit(cache=True, fastmath=True)
&gt; 009: def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
&gt; 010:     &quot;&quot;&quot;
&gt; 011:     Calculates the sum of inverse shortest path lengths for a given range of start nodes.
&gt; 012:     This is the core computational kernel executed by each worker process.
&gt; 013:     &quot;&quot;&quot;
&gt; 014:     total_inverse_distance = 0.0
&gt; 015:     # Pre-allocate memory once to be reused by all BFS runs in this worker.
&gt; 016:     distances = np.empty(n, dtype=np.int32)
&gt; 017:     queue = np.empty(n, dtype=np.int32)
&gt; 018: 
&gt; 019:     for i in range(start_node, end_node):
&gt; 020:         distances.fill(-1)
&gt; 021:         head = 0
&gt; 022:         tail = 0
&gt; 023:         distances[i] = 0
&gt; 024:         queue[tail] = i
&gt; 025:         tail += 1
&gt; 026:         
&gt; 027:         while head &lt; tail:
&gt; 028:             u = queue[head]
&gt; 029:             head += 1
&gt; 030:             start = offsets[u]
&gt; 031:             end = offsets[u+1]
&gt; 032:             for j in range(start, end):
&gt; 033:                 v = flat_adj[j]
&gt; 034:                 if distances[v] == -1:
&gt; 035:                     dist = distances[u] + 1
&gt; 036:                     distances[v] = dist
&gt; 037:                     total_inverse_distance += 1.0 / dist
&gt; 038:                     queue[tail] = v
&gt; 039:                     tail += 1
&gt; 040:     return total_inverse_distance
&gt; 041: 
&gt; 042: # Global dictionary to hold data for worker processes, avoiding expensive serialization.
&gt; 043: _worker_data = {}
&gt; 044: 
&gt; 045: def _init_worker(n, offsets, flat_adj):
&gt; 046:     &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
&gt; 047:     _worker_data[&#x27;n&#x27;] = n
&gt; 048:     _worker_data[&#x27;offsets&#x27;] = offsets
&gt; 049:     _worker_data[&#x27;flat_adj&#x27;] = flat_adj
&gt; 050: 
...</code></pre><br><br>Speedup: <span style="color:#B22222; font-weight:600;">N/A</span><br>  (Speedup = Baseline Time / Your Time; Higher is better)<br><br>  Valid Solutions: 0%<br>  Invalid Solutions: 0%<br>  Timeouts: 100%<br><br><br>Snapshot not saved - invalid solutions present
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                The repeated timeouts and `RuntimeError`s indicate a fundamental mismatch between my parallelization strategies and the constraints of the evaluation environment. The `RuntimeError` points to a stateful conflict with Numba&#x27;s threading layer across multiple test runs. The timeouts with `multiprocessing` suggest that the overhead of process creation and per-worker JIT compilation is too high.<br><br>My most successful attempt was a simple, serial Numba kernel which achieved a 15x speedup without errors. This is the most stable foundation. The path to success is not through more complex parallelization, but by making this serial kernel significantly faster.<br><br>The primary bottleneck in the previous serial kernel was the O(n) re-initialization of the `distances` array inside a loop that runs `n` times, leading to an overall O(n^2) cost for initialization alone.<br><br>My final strategy is to eliminate this bottleneck with a well-known algorithmic optimization for repeated BFS:<br><br>1.  **Single, Serial Numba Kernel**: I will use a single, fast, serial Numba function. This completely avoids all threading and multiprocessing issues that have caused failures.<br>2.  **Pre-allocation**: The `distances` array will be allocated *once* in the main `solve` method and filled with `-1`.<br>3.  **Optimized &quot;Sparse&quot; Reset**: The kernel will reuse this single `distances` array. After each BFS completes, instead of re-initializing the entire array, it will iterate only through the nodes that were just visited (which are conveniently stored in the BFS queue) and reset their distances back to `-1`. This changes the reset cost from O(n) to O(|V_visited|), which is a substantial improvement for graphs that are not fully connected.<br><br>This approach is robust, avoids all previous sources of error, and targets the single largest remaining performance bottleneck in the stable serial solution.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-100
---
from typing import Any
import numba
import numpy as np

# This is the final, highly optimized serial Numba kernel.
# It avoids the O(n^2) cost of re-initializing the distances array on each iteration
# by only resetting the values for nodes that were visited in the previous BFS.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray, distances: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This version uses an optimized reset strategy for the distances array to avoid
    O(n) work on every outer loop iteration.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # The queue is the only major allocation inside the loop.
    queue = np.empty(n, dtype=np.int32)

    # Main loop iterates through each node as a starting point for BFS.
    for i in range(n):
        # Start of the BFS from node `i`.
        head = 0
        tail = 0
        
        # Set the distance for the starting node.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        # Standard BFS implementation.
        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                # A distance of -1 means the node has not been visited in this BFS.
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
        
        # --- Key Optimization: Sparse Reset ---
        # Instead of resetting the entire distances array (O(n)), we only reset
        # the entries for the nodes we visited in this BFS run. The list of
        # visited nodes is conveniently stored in the queue.
        for k in range(tail):
            distances[queue[k]] = -1

    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a highly optimized,
        serial Numba kernel. This approach is robust against threading errors and
        avoids the overhead of multiprocessing by focusing on algorithmic optimization
        of the core serial task.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # --- Key Optimization: Pre-allocation ---
        # The distances array is allocated once and initialized with -1.
        # It&#x27;s then passed to the kernel to be reused, avoiding repeated allocations.
        distances = np.full(n, -1, dtype=np.int32)

        # Call the high-performance optimized serial Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj, distances)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 17 messages and have used up $0.8880. You have $0.1120 remaining.<br><br>Edit failed (and thus not applied) for solver.py: Line 100: Using variable &#x27;num_processes&#x27; before assignment (used-before-assignment)<br>Line 101: Using variable &#x27;tasks&#x27; before assignment (used-before-assignment)<br><br>Proposed changes - This is what you tried to apply (lines 75-124 out of 124):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">...
&gt; 075:         
&gt; 076:         current_pos = 0
&gt; 077:         for i, neighbors in enumerate(adj_list):
&gt; 078:             offsets[i] = current_pos
&gt; 079:             num_neighbors = len(neighbors)
&gt; 080:             if num_neighbors &gt; 0:
&gt; 081:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
&gt; 082:             current_pos += num_neighbors
&gt; 083:         offsets[n] = current_pos
&gt; 084: 
&gt; 085:         # --- Key Optimization: Pre-allocation ---
&gt; 086:         # The distances array is allocated once and initialized with -1.
&gt; 087:         # It&#x27;s then passed to the kernel to be reused, avoiding repeated allocations.
&gt; 088:         distances = np.full(n, -1, dtype=np.int32)
&gt; 089: 
&gt; 090:         # Call the high-performance optimized serial Numba function.
&gt; 091:         total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj, distances)
&gt; 092:         
&gt; 093:         denominator = n * (n - 1)
&gt; 094:         
&gt; 095:         global_efficiency = total_inverse_distance / denominator
&gt; 096:         
&gt; 097:         return {&quot;global_efficiency&quot;: global_efficiency}
| 098: 
| 099:         # Use a multiprocessing Pool to parallelize the work.
| 100:         with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
| 101:             partial_sums = pool.map(_worker_func, tasks)
| 102: 
| 103:         total_inverse_distance = sum(partial_sums)
| 104:         
| 105:         denominator = n * (n - 1)
| 106:         global_efficiency = total_inverse_distance / denominator
| 107:         
| 108:         return {&quot;global_efficiency&quot;: global_efficiency}
| 109:         num_processes = min(num_processes, n) if n &gt; 0 else 1
| 110: 
| 111:         chunk_size = (n + num_processes - 1) // num_processes
| 112:         tasks = [(i * chunk_size, min((i + 1) * chunk_size, n)) for i in range(num_processes)]
| 113:         tasks = [t for t in tasks if t[0] &lt; t[1]] # Filter out empty tasks
| 114: 
| 115:         # Use a multiprocessing Pool to parallelize the work.
| 116:         with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
| 117:             partial_sums = pool.map(_worker_func, tasks)
| 118: 
| 119:         total_inverse_distance = sum(partial_sums)
| 120:         
| 121:         denominator = n * (n - 1)
| 122:         global_efficiency = total_inverse_distance / denominator
| 123:         
| 124:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre><br><br>CURRENT FILE - This is what&#x27;s actually in the file (lines 1-50 out of 127):<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">| 001: from typing import Any
| 002: import numba
| 003: import numpy as np
| 004: from multiprocessing import Pool, cpu_count
| 005: 
| 006: # This is the fast serial Numba kernel. It will be JIT-compiled inside each
| 007: # worker process, avoiding stateful conflicts in the main process.
| 008: @numba.njit(cache=True, fastmath=True)
| 009: def _calculate_partial_inverse_distance(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
| 010:     &quot;&quot;&quot;
| 011:     Calculates the sum of inverse shortest path lengths for a given range of start nodes.
| 012:     This is the core computational kernel executed by each worker process.
| 013:     &quot;&quot;&quot;
| 014:     total_inverse_distance = 0.0
| 015:     # Pre-allocate memory once to be reused by all BFS runs in this worker.
| 016:     distances = np.empty(n, dtype=np.int32)
| 017:     queue = np.empty(n, dtype=np.int32)
| 018: 
| 019:     for i in range(start_node, end_node):
| 020:         distances.fill(-1)
| 021:         head = 0
| 022:         tail = 0
| 023:         distances[i] = 0
| 024:         queue[tail] = i
| 025:         tail += 1
| 026:         
| 027:         while head &lt; tail:
| 028:             u = queue[head]
| 029:             head += 1
| 030:             start = offsets[u]
| 031:             end = offsets[u+1]
| 032:             for j in range(start, end):
| 033:                 v = flat_adj[j]
| 034:                 if distances[v] == -1:
| 035:                     dist = distances[u] + 1
| 036:                     distances[v] = dist
| 037:                     total_inverse_distance += 1.0 / dist
| 038:                     queue[tail] = v
| 039:                     tail += 1
| 040:     return total_inverse_distance
| 041: 
| 042: # Global dictionary to hold data for worker processes, avoiding expensive serialization.
| 043: _worker_data = {}
| 044: 
| 045: def _init_worker(n, offsets, flat_adj):
| 046:     &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
| 047:     _worker_data[&#x27;n&#x27;] = n
| 048:     _worker_data[&#x27;offsets&#x27;] = offsets
| 049:     _worker_data[&#x27;flat_adj&#x27;] = flat_adj
| 050: 
...</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                My apologies for the error. The linter correctly identified that my previous edit was incomplete, leaving behind orphaned code from the `multiprocessing` implementation which I intended to remove entirely. This was a simple mistake in how I constructed the edit command.<br><br>My strategy remains the same, as it is the most logical path forward: revert to a highly optimized **serial** Numba kernel. This approach is robust against the threading and process-related errors that have plagued all parallel attempts, and it targets the primary performance bottleneck (the O(n^2) initialization cost) of my previously successful serial solution.<br><br>I will now submit the corrected code, ensuring that all remnants of the `multiprocessing` implementation are removed, resulting in a clean, self-contained, and highly optimized serial solver.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-127
---
from typing import Any
import numba
import numpy as np

# This is the final, highly optimized serial Numba kernel.
# It avoids the O(n^2) cost of re-initializing the distances array on each iteration
# by only resetting the values for nodes that were visited in the previous BFS.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray, distances: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths.
    This version uses an optimized reset strategy for the distances array to avoid
    O(n) work on every outer loop iteration.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # The queue is the only major allocation inside the loop.
    queue = np.empty(n, dtype=np.int32)

    # Main loop iterates through each node as a starting point for BFS.
    for i in range(n):
        # Start of the BFS from node `i`.
        head = 0
        tail = 0
        
        # Set the distance for the starting node.
        distances[i] = 0
        queue[tail] = i
        tail += 1

        # Standard BFS implementation.
        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                # A distance of -1 means the node has not been visited in this BFS.
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
        
        # --- Key Optimization: Sparse Reset ---
        # Instead of resetting the entire distances array (O(n)), we only reset
        # the entries for the nodes we visited in this BFS run. The list of
        # visited nodes is conveniently stored in the queue.
        for k in range(tail):
            distances[queue[k]] = -1

    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a highly optimized,
        serial Numba kernel. This approach is robust against threading errors and
        avoids the overhead of multiprocessing by focusing on algorithmic optimization
        of the core serial task.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # --- Key Optimization: Pre-allocation ---
        # The distances array is allocated once and initialized with -1.
        # It&#x27;s then passed to the kernel to be reused, avoiding repeated allocations.
        distances = np.full(n, -1, dtype=np.int32)

        # Call the high-performance optimized serial Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj, distances)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 18 messages and have used up $0.9304. You have $0.0696 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 97)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This is the final, highly optimized serial Numba kernel.
&gt; 06: # It avoids the O(n^2) cost of re-initializing the distances array on each iteration
&gt; 07: # by only resetting the values for nodes that were visited in the previous BFS.
&gt; 08: @numba.njit(cache=True, fastmath=True)
&gt; 09: def _calculate_total_inverse_distance_optimized(n: int, offsets: np.ndarray, flat_adj: np.ndarray, distances: np.ndarray) -&gt; float:
&gt; 10:     &quot;&quot;&quot;
&gt; 11:     Calculates the sum of the inverse of all-pairs shortest path lengths.
&gt; 12:     This version uses an optimized reset strategy for the distances array to avoid
&gt; 13:     O(n) work on every outer loop iteration.
&gt; 14:     &quot;&quot;&quot;
&gt; 15:     total_inverse_distance = 0.0
&gt; 16:     # The queue is the only major allocation inside the loop.
&gt; 17:     queue = np.empty(n, dtype=np.int32)
&gt; 18: 
&gt; 19:     # Main loop iterates through each node as a starting point for BFS.
&gt; 20:     for i in range(n):
&gt; 21:         # Start of the BFS from node `i`.
&gt; 22:         head = 0
&gt; 23:         tail = 0
&gt; 24:         
&gt; 25:         # Set the distance for the starting node.
&gt; 26:         distances[i] = 0
&gt; 27:         queue[tail] = i
&gt; 28:         tail += 1
&gt; 29: 
&gt; 30:         # Standard BFS implementation.
&gt; 31:         while head &lt; tail:
&gt; 32:             u = queue[head]
&gt; 33:             head += 1
&gt; 34: 
&gt; 35:             start = offsets[u]
&gt; 36:             end = offsets[u+1]
&gt; 37:             for j in range(start, end):
&gt; 38:                 v = flat_adj[j]
&gt; 39:                 # A distance of -1 means the node has not been visited in this BFS.
&gt; 40:                 if distances[v] == -1:
&gt; 41:                     dist = distances[u] + 1
&gt; 42:                     distances[v] = dist
&gt; 43:                     total_inverse_distance += 1.0 / dist
&gt; 44:                     queue[tail] = v
&gt; 45:                     tail += 1
&gt; 46:         
&gt; 47:         # --- Key Optimization: Sparse Reset ---
&gt; 48:         # Instead of resetting the entire distances array (O(n)), we only reset
&gt; 49:         # the entries for the nodes we visited in this BFS run. The list of
&gt; 50:         # visited nodes is conveniently stored in the queue.
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 91 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   81:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
   82:             current_pos += num_neighbors
   83:         offsets[n] = current_pos
   84: 
   85:         # --- Key Optimization: Pre-allocation ---
   86:         # The distances array is allocated once and initialized with -1.
   87:         # It&#x27;s then passed to the kernel to be reused, avoiding repeated allocations.
   88:         distances = np.full(n, -1, dtype=np.int32)
   89: 
   90:         # Call the high-performance optimized serial Numba function.
 ! 91:         total_inverse_distance = _calculate_total_inverse_distance_optimized(n, offsets, flat_adj, distances)
   92: 
   93:         denominator = n * (n - 1)
   94: 
   95:         global_efficiency = total_inverse_distance / denominator
   96: 
   97:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                This is an exceptionally tricky problem. The recurring `RuntimeError` indicates a fundamental conflict between my code and the evaluation environment. The error message, &quot;Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched,&quot; reveals that the environment runs multiple tests within the same process while changing the thread configuration.<br><br>My attempts at parallelization with `numba.prange` and `multiprocessing` have all failed, either due to this error or timeouts.<br><br>Crucially, my most recent attempt with a supposedly &quot;optimized&quot; **serial** kernel also failed with the exact same threading error. This is the key insight. The only significant difference between my previously successful 15x speedup solution and this failing one was my optimization strategy:<br><br>*   **Successful version**: Allocated the `distances` array *inside* the Numba-jitted function&#x27;s main loop.<br>*   **Failing version**: Pre-allocated the `distances` array outside the function and passed it in as an argument to be modified in-place.<br><br>This strongly suggests that passing a NumPy array as an argument to be modified in-place triggers a bug or an unsupported code path within the specific Numba version used by the environment, causing the threading layer to lock up incorrectly.<br><br>The only logical course of action is to retreat to the last known-good configuration. I will abandon the &quot;sparse reset&quot; optimization, as it is the source of this instability. I will resubmit the simpler, robust serial Numba kernel that has proven to work in this environment. While it may be theoretically less optimal due to O(n^2) initialization costs, its stability is paramount. A 15x speedup that works is infinitely better than a faster one that crashes.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-127
---
from typing import Any
import numba
import numpy as np

# This is the robust, serial Numba kernel that is known to work in the environment.
# It avoids the specific pattern (passing arrays for in-place modification) that
# triggers the threading runtime error. Stability is prioritized over the final
# micro-optimization.
@numba.njit(cache=True, fastmath=True)
def _calculate_total_inverse_distance_serial(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of the inverse of all-pairs shortest path lengths using
    a stable, serial Numba implementation.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0

    # Main loop iterates through each node as a starting point for BFS.
    for i in range(n):
        # Allocating arrays inside the loop is less optimal but avoids the
        # runtime error seen in the environment when passing arrays as arguments.
        distances = np.full(n, -1, dtype=np.int32)
        queue = np.empty(n, dtype=np.int32)
        
        head = 0
        tail = 0
        distances[i] = 0
        queue[tail] = i
        tail += 1

        # Standard BFS implementation.
        while head &lt; tail:
            u = queue[head]
            head += 1

            start = offsets[u]
            end = offsets[u+1]
            for j in range(start, end):
                v = flat_adj[j]
                if distances[v] == -1:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue[tail] = v
                    tail += 1
                    
    return total_inverse_distance

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates the global efficiency of the graph using a robust and stable
        serial Numba kernel. This approach has been proven to work within the
        constraints of the evaluation environment.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Convert the Python list of lists to a flattened adjacency list representation.
        # This is a highly efficient format for Numba.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Call the stable, high-performance serial Numba function.
        total_inverse_distance = _calculate_total_inverse_distance_serial(n, offsets, flat_adj)
        
        denominator = n * (n - 1)
        
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
        <div class="message system">
            <div class="message-header">
                System 
            </div>
            <div class="message-content">
                You have sent 19 messages and have used up $0.9982. You have $0.0018 remaining.<br><br>Edit successful for solver.py.<br><br>Contents of solver.py (lines 1-50 out of 83)<br>(| = existing code, &gt; = modified code)<br><br><pre><code class="language-python">&gt; 01: from typing import Any
&gt; 02: import numba
&gt; 03: import numpy as np
&gt; 04: 
&gt; 05: # This is the robust, serial Numba kernel that is known to work in the environment.
&gt; 06: # It avoids the specific pattern (passing arrays for in-place modification) that
&gt; 07: # triggers the threading runtime error. Stability is prioritized over the final
&gt; 08: # micro-optimization.
&gt; 09: @numba.njit(cache=True, fastmath=True)
&gt; 10: def _calculate_total_inverse_distance_serial(n: int, offsets: np.ndarray, flat_adj: np.ndarray) -&gt; float:
&gt; 11:     &quot;&quot;&quot;
&gt; 12:     Calculates the sum of the inverse of all-pairs shortest path lengths using
&gt; 13:     a stable, serial Numba implementation.
&gt; 14:     &quot;&quot;&quot;
&gt; 15:     total_inverse_distance = 0.0
&gt; 16: 
&gt; 17:     # Main loop iterates through each node as a starting point for BFS.
&gt; 18:     for i in range(n):
&gt; 19:         # Allocating arrays inside the loop is less optimal but avoids the
&gt; 20:         # runtime error seen in the environment when passing arrays as arguments.
&gt; 21:         distances = np.full(n, -1, dtype=np.int32)
&gt; 22:         queue = np.empty(n, dtype=np.int32)
&gt; 23:         
&gt; 24:         head = 0
&gt; 25:         tail = 0
&gt; 26:         distances[i] = 0
&gt; 27:         queue[tail] = i
&gt; 28:         tail += 1
&gt; 29: 
&gt; 30:         # Standard BFS implementation.
&gt; 31:         while head &lt; tail:
&gt; 32:             u = queue[head]
&gt; 33:             head += 1
&gt; 34: 
&gt; 35:             start = offsets[u]
&gt; 36:             end = offsets[u+1]
&gt; 37:             for j in range(start, end):
&gt; 38:                 v = flat_adj[j]
&gt; 39:                 if distances[v] == -1:
&gt; 40:                     dist = distances[u] + 1
&gt; 41:                     distances[v] = dist
&gt; 42:                     total_inverse_distance += 1.0 / dist
&gt; 43:                     queue[tail] = v
&gt; 44:                     tail += 1
&gt; 45:                     
&gt; 46:     return total_inverse_distance
&gt; 47: 
&gt; 48: class Solver:
&gt; 49:     def solve(self, problem: dict, **kwargs) -&gt; Any:
&gt; 50:         &quot;&quot;&quot;
...</code></pre><br><br>RuntimeError: Cannot set NUMBA_NUM_THREADS to a different value once the threads have been launched (currently have 4, trying to set 2) in function &#x27;solve&#x27; at line 77 in solver.py<br><br>Code Context:<br><pre><code class="language-python">   67:         current_pos = 0
   68:         for i, neighbors in enumerate(adj_list):
   69:             offsets[i] = current_pos
   70:             num_neighbors = len(neighbors)
   71:             if num_neighbors &gt; 0:
   72:                 flat_adj[current_pos:current_pos + num_neighbors] = neighbors
   73:             current_pos += num_neighbors
   74:         offsets[n] = current_pos
   75: 
   76:         # Call the stable, high-performance serial Numba function.
 ! 77:         total_inverse_distance = _calculate_total_inverse_distance_serial(n, offsets, flat_adj)
   78: 
   79:         denominator = n * (n - 1)
   80: 
   81:         global_efficiency = total_inverse_distance / denominator
   82: 
   83:         return {&quot;global_efficiency&quot;: global_efficiency}</code></pre>
            </div>
        </div>
        <div class="message assistant">
            <div class="message-header">
                Language Model <span class="command-badge edit">✏️ Code Edit</span>
            </div>
            <div class="message-content">
                This is a very difficult situation. The `RuntimeError` is now occurring even on my most stable, serial Numba solution, which had previously passed with a 15x speedup. This indicates that the evaluation environment is fundamentally incompatible with any use of Numba, as even the simplest JIT compilation triggers a stateful conflict in its test runner.<br><br>All paths using Numba are blocked. Parallelization with `numba.prange` failed. Algorithmic optimizations on the Numba kernel failed. Even the most basic serial Numba kernel now fails.<br><br>The only remaining strategy is to abandon Numba entirely and seek performance through other means. The only tool left is parallelization. I will revert to a `multiprocessing` strategy, but this time, the computational kernel will be pure Python. This completely sidesteps the Numba-related `RuntimeError`.<br><br>**Final Strategy:**<br>1.  **Eliminate Numba**: Remove all `numba` imports and decorators. This is the only way to avoid the `RuntimeError`.<br>2.  **Pure Python Kernel**: The core BFS logic will be implemented in a standard Python function. I will use `collections.deque` for the queue, which is efficient for this task.<br>3.  **Parallelize with `multiprocessing`**: I will use a `multiprocessing.Pool` to distribute the BFS tasks (one for each starting node) across all available CPU cores.<br>4.  **Efficient Data Sharing**: I will continue to use a flattened adjacency list and the `initializer` pattern for the `Pool` to efficiently share the graph data with worker processes, minimizing serialization overhead.<br><br>This approach is a calculated trade-off. While a pure Python BFS is much slower than a Numba-jitted one, running many of them in parallel may provide enough of a speedup to pass the time limit. Given the consistent failures of all Numba-based solutions, this is the only viable path forward.<br><br><pre><code class="language-python">edit
file: solver.py
lines: 1-127
---
from typing import Any
import numpy as np
from multiprocessing import Pool, cpu_count
from collections import deque

# This is a pure Python BFS kernel. It avoids Numba entirely to prevent the
# runtime errors observed in the evaluation environment.
def _calculate_partial_inverse_distance_python(n: int, offsets: np.ndarray, flat_adj: np.ndarray, start_node: int, end_node: int) -&gt; float:
    &quot;&quot;&quot;
    Calculates the sum of inverse shortest path lengths for a given range of start nodes.
    This is the pure Python computational kernel executed by each worker process.
    &quot;&quot;&quot;
    total_inverse_distance = 0.0
    # Iterate through the assigned range of starting nodes for BFS.
    for i in range(start_node, end_node):
        # A dictionary is used for distances for efficient &#x27;in&#x27; checks.
        distances = {i: 0}
        # collections.deque is the standard, efficient queue for BFS in Python.
        queue = deque([i])
        
        while queue:
            u = queue.popleft()
            
            start = offsets[u]
            end = offsets[u+1]
            # Iterate through neighbors using the flattened adjacency list.
            for j in range(start, end):
                v = flat_adj[j]
                if v not in distances:
                    dist = distances[u] + 1
                    distances[v] = dist
                    total_inverse_distance += 1.0 / dist
                    queue.append(v)
    return total_inverse_distance

# Global dictionary to hold data for worker processes, avoiding expensive serialization.
_worker_data = {}

def _init_worker(n, offsets, flat_adj):
    &quot;&quot;&quot;Initializer for each worker process in the multiprocessing Pool.&quot;&quot;&quot;
    _worker_data[&#x27;n&#x27;] = n
    _worker_data[&#x27;offsets&#x27;] = offsets
    _worker_data[&#x27;flat_adj&#x27;] = flat_adj

def _worker_func(task_range):
    &quot;&quot;&quot;The function each worker process executes on its assigned range of nodes.&quot;&quot;&quot;
    start_node, end_node = task_range
    return _calculate_partial_inverse_distance_python(
        _worker_data[&#x27;n&#x27;],
        _worker_data[&#x27;offsets&#x27;],
        _worker_data[&#x27;flat_adj&#x27;],
        start_node,
        end_node
    )

class Solver:
    def solve(self, problem: dict, **kwargs) -&gt; Any:
        &quot;&quot;&quot;
        Calculates global efficiency using a pure Python kernel parallelized with
        multiprocessing. This approach is designed to be robust against the
        Numba-specific threading errors encountered previously.
        &quot;&quot;&quot;
        adj_list = problem[&quot;adjacency_list&quot;]
        n = len(adj_list)

        if n &lt;= 1:
            return {&quot;global_efficiency&quot;: 0.0}

        # Prepare the flattened adjacency list for efficient processing.
        offsets = np.zeros(n + 1, dtype=np.int32)
        num_edges = sum(len(neighbors) for neighbors in adj_list)
        flat_adj = np.empty(num_edges, dtype=np.int32)
        current_pos = 0
        for i, neighbors in enumerate(adj_list):
            offsets[i] = current_pos
            num_neighbors = len(neighbors)
            if num_neighbors &gt; 0:
                flat_adj[current_pos:current_pos + num_neighbors] = neighbors
            current_pos += num_neighbors
        offsets[n] = current_pos

        # Determine the number of processes and create task chunks.
        try:
            num_processes = cpu_count()
        except NotImplementedError:
            num_processes = 2 # A safe fallback.
        
        num_processes = min(num_processes, n) if n &gt; 0 else 1

        chunk_size = (n + num_processes - 1) // num_processes
        tasks = [(i * chunk_size, min((i + 1) * chunk_size, n)) for i in range(num_processes)]
        tasks = [t for t in tasks if t[0] &lt; t[1]] # Filter out empty tasks

        # Use a multiprocessing Pool to parallelize the work.
        with Pool(processes=num_processes, initializer=_init_worker, initargs=(n, offsets, flat_adj)) as pool:
            partial_sums = pool.map(_worker_func, tasks)

        total_inverse_distance = sum(partial_sums)
        
        denominator = n * (n - 1)
        global_efficiency = total_inverse_distance / denominator
        
        return {&quot;global_efficiency&quot;: global_efficiency}
---</code></pre>
            </div>
        </div>
            </div>
        </div>
    </div>
</body>
</html>