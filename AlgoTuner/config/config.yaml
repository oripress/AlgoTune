global:
  spend_limit: 1.0 # in USD
  total_messages: 9999
  max_messages_in_history: 5

benchmark:
  dev_runs: 2               
  eval_runs: 10
  baseline_timeout: 60000 # in milliseconds
  manager_refresh_interval: 10 # Refresh multiprocessing.Manager after this many subprocess spawns to prevent resource exhaustion              
  validation_pool:       
    num_workers: 1       # Max number of parallel workers (1 = sequential, null = os.cpu_count())
    maxtasksperchild: null # Disable worker recycling for debugging (was 10)
    memory_limit_gb_per_worker: 14 # Increased for baseline algorithms (was 6GB)
    disable_rlimit_as: true # Disable RLIMIT_AS to prevent virtual memory allocation failures
  tempdir_cleanup:
    retries: 3          # Number of cleanup attempts
    delays: [0.5, 1.0, 2.0]  # Delays between attempts in seconds
  cache:
    max_entries: 100      # Maximum cached arrays
    max_memory_mb: 2048   # Maximum cache memory in MB
    ttl_seconds: 900      # Time-to-live in seconds (15 min)

dataset:
  train_size: 100
  test_size: 100

models:
  o4-mini:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    # OpenAI o4-mini with maximum reasoning effort and context
    reasoning_effort: "high"
    max_tokens: 100000
    drop_params: true
  o3:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    # OpenAI o4-mini with maximum reasoning effort and context
    reasoning_effort: "high"
    max_tokens: 100000
    drop_params: true
  claude-opus-4-20250514:
    api_key_env: "CLAUDE_API_KEY"
    temperature: 1.0
    top_p: 0.95
    modify_params: true
    # Claude Opus 4 with maximum thinking within API limits (thinking + output â‰¤ 32K total)
    max_tokens: 32000
    thinking:
      type: "enabled"
      budget_tokens: 24000
  deepseek/deepseek-reasoner:
    api_key_env: "DEEPSEEK_API_KEY"
    max_tokens: 64000
    temperature: 0.0
    # DeepSeek R1 with maximum reasoning/thinking enabled
    enable_reasoning: true
    thinking_budget: 32768  # Maximum thinking tokens for DeepSeek R1
  gemini/gemini-2.5-pro:
    api_key_env: "GEMINI_API_KEY"
    temperature: 0.0
    top_p: 0.95
    modify_params: true
    # Gemini 2.5 Pro with MAXIMUM thinking and context (Google AI Studio)
    max_tokens: 64000
    thinking_budget: 32768   # MAXIMUM thinking budget for Gemini 2.5 Pro (128-32,768 range)
    include_thoughts: true   # Include full reasoning process in response
  deepseek-ai/DeepSeek-R1:
    api_key_env: "TOGETHER_API_KEY"
    temperature: 0.0
    top_p: 0.95
    # DeepSeek-R1-0528 via Together API with maximum context
    max_tokens: 32000
    model_provider: "together"
  # --- MoonshotAI / Kimi K2 (no explicit thinking mode) ---
  openrouter/moonshotai/kimi-k2:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    # OpenRouter lists 32,768 context; use a safe max output
    max_tokens: 32768
    drop_params: true

  # --- Qwen / Qwen3 Coder 480B (thinking enabled) ---
  openrouter/qwen/qwen3-coder:
    api_key_env: "OPENROUTER_API_KEY"
    # Qwen3 thinking works best with some sampling
    temperature: 0.6
    top_p: 0.95
    modify_params: true
    # Request maximum thinking via OpenRouter's unified interface
    reasoning:
      enabled: true
      effort: "high"
    # Also pass Qwen's native flag for compatibility
    enable_thinking: true
    # Long outputs are possible; cap to a practical value
    max_tokens: 65536

  # --- Z.AI / GLM-4.5 (thinking enabled) ---
  openrouter/z-ai/glm-4.5:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    modify_params: true
    reasoning:
      enabled: true
      effort: "high"
    # GLM-4.5 page indicates up to 96K output; use that ceiling
    max_tokens: 96000
    usage:
      include: true


