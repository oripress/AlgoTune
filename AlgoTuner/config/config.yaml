global:
  spend_limit: 1.0 # in USD
  total_messages: 9999
  max_messages_in_history: 5

benchmark:
  dev_runs: 2               
  eval_runs: 10
  baseline_timeout: 60000 # in milliseconds
  manager_refresh_interval: 100
  validation_pool:       
    num_workers: 1       # Max number of parallel workers (1 = sequential, null = os.cpu_count())
    maxtasksperchild: null # Disable worker recycling for debugging (was 10)
    memory_limit_gb_per_worker: 14 # Increased for baseline algorithms (was 6GB)
    disable_rlimit_as: false # Re-enabled for proper OOM detection and memory safety
  tempdir_cleanup:
    retries: 3          # Number of cleanup attempts
    delays: [0.5, 1.0, 2.0]  # Delays between attempts in seconds
  cache:
    max_entries: 100      # Maximum cached arrays
    max_memory_mb: 2048   # Maximum cache memory in MB
    ttl_seconds: 900      # Time-to-live in seconds (15 min)

dataset:
  train_size: 100
  test_size: 100

models:
  o4-mini:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    reasoning_effort: "high"
    max_completion_tokens: 100000
    drop_params: true
  o3:
    api_key_env: "OPENAI_API_KEY"
    temperature: 0.0
    # OpenAI o4-mini with maximum reasoning effort and context
    reasoning_effort: "high"
    max_tokens: 100000
    drop_params: true
  claude-opus-4-20250514:
    api_key_env: "CLAUDE_API_KEY"
    temperature: 1.0
    top_p: 0.95
    modify_params: true
    # Claude Opus 4 with maximum thinking within API limits (thinking + output ≤ 32K total)
    max_tokens: 32000
    thinking:
      type: "enabled"
      budget_tokens: 24000
  deepseek/deepseek-reasoner:
    api_key_env: "DEEPSEEK_API_KEY"
    max_tokens: 64000
    temperature: 0.0
    # DeepSeek R1 with maximum reasoning/thinking enabled
    enable_reasoning: true
    thinking_budget: 32768  # Maximum thinking tokens for DeepSeek R1

  claude-sonnet-4-5-20250929:
    api_key_env: "CLAUDE_API_KEY"
    temperature: 1.0
    modify_params: true
    # Sonnet 4.5 supports long outputs; set a large cap if you need big dumps.
    max_tokens: 64000
    # Extended thinking (summarized thinking in Claude 4.x). Counts toward max_tokens.
    thinking:
      type: "enabled"
      # Aggressive budget; dial down if you hit latency/timeouts.
      budget_tokens: 60000

  gemini/gemini-2.5-pro:
    api_key_env: "GEMINI_API_KEY"
    temperature: 0.0
    top_p: 0.95
    modify_params: true
    # Gemini 2.5 Pro with MAXIMUM thinking and context (Google AI Studio)
    max_tokens: 64000
    thinking_budget: 32768   # MAXIMUM thinking budget for Gemini 2.5 Pro (128-32,768 range)
    include_thoughts: true   # Include full reasoning process in response
  deepseek-ai/DeepSeek-R1:
    api_key_env: "TOGETHER_API_KEY"
    temperature: 0.0
    top_p: 0.95
    # DeepSeek-R1-0528 via Together API with maximum context
    max_tokens: 32000
    model_provider: "together"
  # --- MoonshotAI / Kimi K2 (no explicit thinking mode) ---
  openrouter/moonshotai/kimi-k2:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    max_tokens: 8192
    drop_params: true
    extra_body:
      usage:
        include: true

  # --- Qwen / Qwen3 Coder 480B (thinking enabled) ---
  openrouter/qwen/qwen3-coder:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    top_p: 0.95
    modify_params: true
    max_tokens: 65536
    usage:
      include: true

  openrouter/z-ai/glm-4.5:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    modify_params: true
    max_tokens: 96000
    reasoning:
      enabled: true       # GLM-4.5: on/off only
      exclude: true     # think but don’t return the chain
    usage:
      include: true

  anthropic/claude-opus-4-1-20250805:
    api_key_env: "CLAUDE_API_KEY"
    temperature: 1.0  # MUST be 1.0 when thinking is enabled
    modify_params: true
    max_tokens: 32768
    thinking:
      type: "enabled"
      budget_tokens: 24000   # leaves ~8.7k tokens for answer text

  openrouter/openai/gpt-oss-120b:
    api_key_env: "OPENROUTER_API_KEY"
    temperature: 0.0
    modify_params: true
    drop_params: true
    max_tokens: 100000
    reasoning:
      effort: "high"     # max thinking (OpenAI-style effort control)
      exclude: true      # hide chain-of-thought in the response (set to false to see it)
    usage:
      include: true      # return token and cost accounting in response

  gpt-5:
    api_key_env: "OPENAI_API_KEY"
    max_completion_tokens: 128000
    reasoning_effort: "high"

  gpt-5-mini:
    api_key_env: "OPENAI_API_KEY"
    max_completion_tokens: 128000
    reasoning_effort: "high" 

  gpt-5-pro:
    api_key_env: "OPENAI_API_KEY"
    modify_params: true
    drop_params: false
    max_tokens: 128000       # or as high as your quota / backend allows
    reasoning_effort: "medium"
    usage:
      include: true
